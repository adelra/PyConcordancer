[MUSIC]. 
Hello, and a warm welcome to every one of 
you from around the world. 
This is Week 1 of Computation 
Neuroscience and I'm your instructor, 
Rajesh Rao. 
Let's begin our computation adventures 
with a picture. 
You've probably seen a picture like this 
before. 
Physicists tell us that this is the 
universe that we live in, but I think 
they're mistaken. 
This is the universe that we really live 
in. 
This 3-pound mass of tissue inside our 
skull is what allows us to perceive the 
world and indeed the universe. 
This amazing machine is what enables us 
to think, feel, act, and be human. 
This is what is enabling me to speak 
these words right now and allowing you to 
listen. 
And when the lecture gets boring, which 
hopefully, won't happen too often. 
You can thank the same 3-pound organ for 
enabling you to skip forward a few slides 
or maybe doze off in that chair that 
you're sitting in. 
Understanding how the brain does all of 
these things is one of the most profound 
scientific mysteries of the 21st century. 
In this course, we'll try to unravel some 
of this mystery and understand the brain 
using computational models. 
In this course, we'll cover three types 
of Computational Models. 
The first kind are Descriptive Models. 
So in this case, we're interested in 
quantifying how neurons respond to 
external stimuli. 
And what we get here is something called 
a neural encoding model, which 
quantitatively describes a particular 
neuron responds to external stimuli. 
The counterpart to encoding is decoding. 
So in this case, we're interested in 
extracting information from neurons that 
have been recorded from the brain. 
And then, using this information for 
controlling something like a prosthetic 
hand for example. 
So this problem of decoding is extremely 
important in the field of brain-computer 
interfacing and neuroprosthetics. 
The second type of model that we look at 
are called Mechanistic Models. 
So in this case, we are interested in 
simulating the behavior of a single 
neuron or a network of neurons on a 
computer. 
So you might have heard about the Human 
Brain Project that is being led by Henry 
Markram in Europe. 
And, that project is an example of a 
computer simulation of an extremely large 
network of neurons in the extreme case, 
perhaps, the entire brain on a computer. 
The last type of models that we look at 
are called Interpretive or Normative 
Models. 
So in this case, we are interested in 
understanding why brain circuits operate 
in the way that they do. 
In other words, we're interested in 
extracting some computational principles 
that underlie the function of a 
particular brain circuit. 
So we'll look at examples of all these 
three types of models in the coming 
weeks. 
Now, moving on to some course information 
and logistics. 
The length of the course is eight weeks, 
and each week, we'll give you one video 
lecture, which will be about 1 hour long. 
And we'll split this up into 10 minute 
chunks and we'll also give you one 
homework quiz each week. 
Now, these lectures and homeworks will be 
released on Fridays and the homeworks 
will be due the second Monday from the 
release date. 
So this way, you'll be able to use two 
weekends to work on each homework and the 
complete syllabus and schedule is on the 
course web page. 
Here are the two recommended textbooks 
for this course. 
They are not required, but they might be 
useful if you need additional information 
besides what's covered in the lecture 
videos and lecture slides. 
The first one is Theoretical 
Neuroscience, this is a standard textbook 
in the field and its a book written by 
Peter Dayan and Larry Abbot, two leading 
researchers in computational 
neuroscience. 
The other textbook is called Tutorial on 
Neural Systems Modelling and it's by 
another researcher in the field, Thomas 
Anastasio. 
And this book also comes with the Matlab 
code that you might find useful as you're 
exploring concepts and computational 
neuroscience. 
Now, on to homeworks and gradings. 
So the course grade will be based on six 
weekly homeworks, most of these will be 
multiple choice questions. 
Some of them will be based on the result 
of programming and Matlab or Octave. 
And, we'll allow up to submissions for 
each homework and we'll take the maximum 
score out of the three that you've 
submitted. 
The good news is there's no exams and 
there will be a certificate of completion 
if your total course grade is greater 
than 60%. 
I mentioned earlier that some of the 
homework questions will be based on 
results that you get after you've 
executed your program in Matlab. 
So what if you don't have Matlab? 
Well, no worries. 
You can use Octave which is quite similar 
to Matlab, but it's free. 
And you can download Octave from the 
website given on the slide. 
And Octave should suffice for all the 
homework questions that we'll have in 
this course. 
But what if you have no programming 
experience to begin with? 
Well, no problem. 
Perhaps you can use this opportunity to 
learn programming. 
there are several Matlab tutorials on the 
course website that you can try out, and 
also, the first homework assignment is 
actually a Matlab practice homework. 
So the submission is optional, but it 
might be really useful for those of you 
who are programming for the first time in 
Matlab to try out the homework and submit 
it and see how you did. 
So let's end with some of the goals of 
the course. 
In other words, what can we expect to 
learn in this course? 
Well, at the end of the course, you 
should be able to, first of all, 
quantitatively describe what a biological 
neuron, a network of neurons is doing 
given some experimental data that perhaps 
you got from your neuroscientist friend. 
Secondly, you would like to be able to 
simulate on a computer the behavior of 
neuron or other networks of neurons. 
And finally, you should be able to at the 
end of the course, formulate competition 
of principles that would help explain the 
operation of certain neurons or networks 
in the brain. 
So, are you ready? 
Let's begin. 

 >> Welcome back. 
Let's begin by asking what is 
computational neuroscience? 
According to Terry Sejnowski, who was one 
of the founding fathers of the field and 
was also my postdoctoral advisers, the 
goal of computation neuroscience is to 
explain in computational terms how brains 
generate behaviors. 
Now, let's dissect this definition a 
little bit. 
This leads us to a definition proposed by 
Peter Dayan and Larry Abbott. 
According to them, computational 
neuroscience is the field that provides 
us with the tools and methods for doing 
three different things. 
One is characterizing what nervous 
systems do. 
The second is Determining how they 
function. 
And finally, the third is, understanding 
why they operate in particular ways. 
Now, this actually corresponds quite 
nicely to the three types of 
computational models that we looked at 
earlier in the, in a previous lecture. 
This corresponds to descriptive models, 
mechanistic models and interpretive 
models. 
Now to understand these three types of 
models in a little bit more detail with 
an example, let's look at the concept of 
receptor fields. 
In order to understand the concept of 
receptive fields it is useful to go back 
to some early experiments performed by 
Hubel and Wiesel back in the 1960s. 
Now, they were interested in trying to 
understand the visual system of the cat. 
In order to do so, they implanted tiny 
electrodes, or tiny wires. 
Into the visual area of the cat's brain. 
So this is this an area that's in the 
very rear of the cat's brain and by using 
these electrodes, they were able to 
record some electrical signals from 
particular brain cells. 
So these electrical signals that they 
record are due to the output of the brain 
cells and these outputs are in the form 
of Tiny digital pulses. 
which are also called spikes or action 
potentials. 
We'll learn more about them in a later 
lecture. 
And in order to get these cells to 
respond, they show different types of 
stimuli to the animal. 
And on the right hand side, you see one 
of these, stimuli. 
So this is a bar of light that's oriented 
at approximately 45 degrees, and I'm 
going to show you a movie that will have 
this bar of light moving in a particular 
direction. 
And what you're going to hear are the 
responses of one particular brain cell in 
a cat's brain. 
And you're going to hear the responses 
because Hubel and Wiesel have converted 
the electrical signals that they're 
recording into sound signals. 
So are you ready? 
Here we go. 
[SOUND] So the crackling sound that 
you're hearing are the responses of a 
brain cell, a visual cortical brain cell. 
[SOUND] In the cat's brain. 
And you'll notice that this particular 
brain cell likes bars of light that are 
oriented at this 45 degree angle. 
But it doesn't like broad field 
illumination, like this big square of 
light that they are showing. 
So it doesn't really respond when this 
big square of light is being turned on. 
On or off, but it does respond to the 
edge of that square when it's oriented at 
this 45 degree angle. 
Okay, so what did we see in the previous 
slide? 
We saw that when the bar of light was 
horizontal, there was not much of a 
response from the cells so this is a way 
of representing There being not much of a 
response. 
Each of these vertical lines is a spike. 
So you were hearing the crackling noise 
that responded to one little pop for each 
of these vertical lines. 
And that's the spike from the recorded 
neuron. 
And we found that in the particular movie 
that we say in the previous slide The 
cat's cell, the one that we were 
recording from, responded the most when 
the bar of light was at a 45-degree 
angle. 
So we got a very robust response. 
So here's where the light, the bar of 
light, was in a particular location at 
that particular orientation. 
And so we got a very robust response as 
shown by. 
These vertical lines which correspond to 
the output of neuron, also called a 
spike. 
And similarly when the bar of light was 
at a different angle, you would not 
expect much of a response. 
That the response is lesser than it is 
for the 45 degree angle. 
So what this leads us to is a notion 
called A receptive field. 
So here is the definition. 
So the receptive field is defined by 
neuroscientists as comprising of all 
these specific properties of a sensory 
stimulus that generates a very robust or 
a strong response from any cell that 
you're recording from. 
So examples could be that, for example, 
you're, recording from a cell. 
In the retina, and you might find that 
the cell responds really robustly to 
spots of light that are turned on at a 
particular location. 
Similarly, as we saw in the Hubel and 
Wiesel experiments, a bar of light that 
is at a particular orientation and at a 
particular location on the retina. 
Might cause a robust response in a visual 
cortex cell in the cat's brain. 
So what we'll do now is we'll look at the 
three different types of computational 
models that we mentioned earlier, 
descriptive, mechanistic and interpretive 
models. 
And we're going to build these models for 
The concept of Receptive Fields. 
So first, let's look at Descriptive 
Models. 
So how do you build a Descriptive Model 
of a Receptive Field? 
Well let's take the case of the Retina. 
So the Retina is the layer of tissue 
that's at the back of your eyes. 
And when you for example are looking at a 
particular object, let's say this pencil, 
the inverted image is projected to the 
back of your eyes and on the retina. 
And if you're recording from a particular 
group of cells called the retinal 
ganglion cells, you will find that it is 
conveying information about the image. 
To the other areas of the brain. 
Particularly, this area called the 
lateral geniculate nucleus. 
And so you can do an experiment to try to 
understand the receptive fields of cells 
in the retina. 
So how would you do that? 
Well, you could try to flash spots of 
light that are circular, as shown by the 
yellow circle here. 
different locations on the retina. 
And what you'll find is that for any 
particular cell that you're recording 
from, let's say, this particular cell 
over here. 
You might find that the cell only 
responds when you turn on a spot of 
light. 
So when you turn on a spot of light in 
this particular location. 
And that generates a robust response, as 
shown by these spikes over here. 
And interestingly enough, when you turn 
on the spot of light in the surrounding 
area. 
In this annulus around the center. 
You might find that the cell stops 
responding. 
So it does not generate these spikes. 
This allows us to define the concept of 
center surround receptive fields in the 
retina. 
So as we saw in the previous slide, when 
you turn on a spot of light in the 
central region, you get an increase in 
the activity of the cell in the retina. 
And when you turn off a spot of light in 
the surrounding region, you also get an 
increase in the activity of the retinal 
cell. 
So this leads us to the concept of its 
On-Center, Off-Surround, Receptive Field. 
And this basically means that the cell 
responds when you turn on a spot of light 
in the center or when you turn off a spot 
of light in the surrounding region. 
Now, the counterpart to this type of a 
receptive field is the off-center. 
On-surround type receptive field. 
And so as you might expect, in this case 
the cell likes it when you turn off a 
spot of light in the center region. 
And also it will respond with increased 
activity when you turn on a spot of light 
in the surrounding region. 
So the plus indicates on, and the minus 
indicates off. 
Now the information from the retina is 
passed on to a nucleus. 
As I mentioned earlier, called the 
Lateral Geniculate Nucleus or LGN for 
short. 
And, this in turn passes information, to 
the back of your brain, to an area called 
the Primary Visual Cortex, and so one 
might ask What happens if you record from 
cells in the primary visual cortex? 
What kind of receptive fields do you 
observe in the primary visual cortex? 
Well, remember what happened in the Hubel 
and Wiesel movie that I showed you 
earlier? 
In that movie we saw that a particular 
cell responded robustly, to an oriented 
bar of light that was oriented in a 45 
degree angle. 
So this gives rise to, receptive fields, 
that look like this in the visual cortex. 
So these are called oriented receptive 
fields, because they're oriented at 
different angles. 
And the, neurons are cells in the visual 
cortex, in this particular case, the 
primary visual cortex. 
Tend to respond the best to bars, such as 
this one, bright bar in a dark 
background. 
So the black or the gray over here 
represents a dark background. 
And so we have a bar that's bright that 
is oriented at 45 degrees, and that is 
what the particular cell that we saw in 
the movie responded to best. 
And so this corresponds to a descriptive 
model. 
Of the oriented receptive field of the 
neuron, in that case in the primary 
visual cortex of a cat. 
Now, obviously these are not the only 
types of receptive fields that are found 
not just at one orientation. 
What you'll find if you recall from a 
whole bunch of cells in the primary 
visual cortex is that the receptive 
fields vary in their orientation such as 
shown over here. 
Some might be vertically oriented or at 
90 degrees, some might be horizontal, 
some might be at a different angle, such 
as the one shown here. 
But you would also find that there are 
cells that respond to dark bars such as 
one shown over here, or the one shown 
over here, it's oriented at a 45 degree 
angle but it likes dark bars. 
As opposed to this one over here, which 
likes the bright bar, oriented at 45 
degrees. 
So we'll later learn in the course how to 
quantitatively estimate these types of 
receptive fields using a technique called 
reverse correlation. 
Now, the second question we can ask is. 
We know that there are center surround 
receptive fields in the retina. 
And also, it turns out, in the LGN, or or 
lateral geniculate nucleus. 
But when you come up to the cortex, you 
find this oriented receptive fields. 
So, how do we get from center-surround 
type of receptive fields to oriented 
receptive fields? 
And that leads us to a Mechanistic Model 
of Receptive Fields. 
And that'll be covered in the next 
lecture. 
So, see you then. 

Hello and welcome back. 
In the previous lecture Hubel and Wiesel 
entertained us with their Star Wars Jedi 
Knight lightsaber-like experiment and 
introduced us to the concept of receptive 
fields. 
We learned about a descriptor model of 
receptive fields, and in particular, we 
looked at two different kinds of receptor 
fields, center-surround receptor fields 
and oriented receptive fields. 
So here are some examples of these 
oriented receptive fields. 
And, the question that we asked was, how 
are these oriented receptive fields 
constructed from the center-surround type 
receptive fields? 
In other words, what we wanted was a 
mechanistic model of how receptive fields 
are constructed using the neural 
circuitry of the visual cortex. 
So, in order to answer this question, we 
need to look at the neuroanatomy of the 
visual system. 
Here's what happens to the visual 
information after it's processed by the 
retina. 
It flows through the optic nerve to this 
nucleus, called the lateral geniculate 
nucleus. 
And from there, it flows to the primary 
visual cortex or V1. 
And as we know, in V1, we have receptor 
fields which are elongated that look 
something like this. 
Whereas the lateral geniculate nucleus, 
LGN, we have receptive fields that are 
basically centered-surround, similar to 
what you find in the retina. 
And the question that we're asking is how 
do we go from the center-surround 
receptor fields to these elongated 
receptor fields? 
And the clue to this conundrum comes from 
the anatomy. 
So if you look at the anatomy, you'll see 
that a number of LGN cells converge to 
single V1 cells. 
And so, what this means is that a single 
V1 cell receives inputs from a large 
number of LGN cells. 
And so the question then becomes, how do 
the inputs which have receptive fields 
such as this, give rise to a receptor 
field in the output in V1 that looks 
something like this? 
So I'll give you a couple of moments to 
figure out the answer. 
So are you ready? 
Let's see. 
Here is the answer. 
It's actually quite obvious when you 
think about it. 
In order to form a receptive field that 
is oriented and looks like this, all you 
have to do is arrange the inputs to have 
receptive fields that are aligned in this 
particular manner. 
And, given that, there is a feed forward 
connection or a convergence of these 
inputs onto one particular V1 cell. 
This particular cell is going to behave 
as if it has this type of a receptive 
field. 
In other words, this particular cell in 
V1, our primary visual cortex is going to 
behave just like the cell that we saw in 
the Hubel and Wiesel movie. 
It's going to respond to any bar that is 
oriented in this particular way and is 
bright and has this particular 45 degree 
orientation. 
So this particular model is a mechanistic 
model of how receptive fields in V1, of 
this particular kind are constructed. 
And this was suggested by Hubel and 
Wiesel in the 1960s. 
This model is actually quite 
controversial, in the sense that, it does 
not take into account other inputs that 
this V1 cell is receiving. 
So if you look at the neuroanatomy, in V1 
you'll find that there are a lot of 
recurrent connections from one V1 cell to 
other V1 cells. 
So each V1 cell receives inputs from its 
counterparts within V1, in addition to 
the feed forward inputs from the LGN. 
And the Hubel and Wiesel model does not 
take into account these recurrent inputs 
to V1. 
And it turns out that these frequent 
inputs also contribute to the responses 
of the V1 cell. 
We will look at both feed forward, as 
well as recurrent networks a little bit 
later in the course when we discuss 
network level models. 
Okay, great. 
Now, let's move on to the last type of 
models. 
These are called interpretive models. 
So we're going to look at an interpretive 
model of receptive fields, and the 
question that we're asking is, why are 
receptive fields in V1 shaped in this 
particular way? 
In other words, why do they look like 
this? 
Why do they have this orientation and why 
are they selective for bright or dark 
bars? 
Another way of asking the same question 
is, what are the computational advantages 
of such receptive fields? 
Now, this is the kind of question that 
perhaps an engineer or a computer 
scientist would ask. 
So why do we need to use these types of 
receptive fields? 
Do they confer any advantages? 
So, let's look at one particular 
interpretive model of receptive fields. 
The interpretive model that we look at is 
based on the efficient coding hypothesis, 
so what does this hypothesis state. 
Well, it states that the goal of the 
brain, through evolution for example, is 
to represent images as faithfully and as 
efficiently as possible using the neurons 
that it has. 
Now, these neurons have receptive fields 
RF1, RF2, and so on. 
So here are some receptive fields of 
neurons. 
And the question we're asking is, are 
these the best way, or the most efficient 
way, of representing images. 
And so, how do we represent images using 
these types of receptive fields? 
Well, as an example, I can take these 
receptive feels are of three and four so 
as take these two, and I can just add 
them. 
So imagine that they are images, so here 
is a bright region in the image and a 
dark region in the image bright region in 
the image and dark in the image so if i 
think of these as just two image patches 
so these two receptive feels I can add 
the two receptive fields. 
And what kind of an image can I 
reconstruct? 
Well, if you add these two together, 
you're going to get some linear 
combination of these regions. 
So, you have that particular shape. 
So, it's going to be a really bright 
region up here. 
Maybe a slightly bright region here and 
here and here and here. 
Because you're adding the plus is here 
with a little bit of the minuses over 
there, and you're going to get some dark 
regions over here. 
And, so, what you have is an image that 
looks something like that. 
So, given that you are adding these two 
receptive fields, you have a image that 
you reconstructed that looks something 
like, let's say, a plus sign. 
Now, if you're given a whole bunch of 
these receptive fields, you can literally 
combine them in this particular way. 
So this is just a summation sign over all 
the receptive fields, RF1, RF2 and so on 
and each of these are weighted by some 
number. 
So, these are the neural response, so a 
linear combination of them is going to 
give you some particular image. 
Now, what is the goal here? 
The goal is to find out what are the 
receptive fields, RF1, 2, and 3, and so 
on that minimize the total squared 
pixel-wise errors between a given set of 
images. 
So one of these images, perhaps the brain 
is trying to optimize its representation 
for natural images. 
And so we can look at the squared 
pixelwise errors between natural images 
and the reconstruction of those natural 
images I had. 
And we've also add an additional 
constraint, so you want them to be 
efficient and so we want these responses 
for example to be as independent as 
possible we don't want all the neurons to 
be firing at the same time for example 
and so we can add the constraint that. 
We want these coefficients or these 
responses r sub i to be as independent as 
possible. 
So given that we have now this 
optimization criteria and this particular 
idea of minimizing the total square pixel 
wise errors. 
And also keeping these responses as 
independent as possible. 
What are the receptive fields that 
achieve this objective? 
So, here's what we could do. 
We could start out with a set of random 
receptive fields. 
So, we are assuming that we don't know 
what the optimal receptive fields are. 
And we could run our efficient coding 
algorithm, which tries to minimize the 
reconstruction error. 
The error of the square pixel-wise errors 
between the reconstructed images and the 
actual images, and we can run it on 
natural image patches. 
So, why natural image patches? 
Well, we can assume that the brain has 
evolved to process natural images. 
In that case, if the brain is indeed 
trying to perform efficient coding, then. 
Perhaps it's trying to be efficient on 
these types of images, natural images of 
plants and trees et cetera. 
And so we can take these kinds of images 
and run our efficient coding algorithm on 
tiny patches. 
So here's an example of a tiny patch. 
So we can randomly sample from different 
locations on these natural images and 
then run the efficient coding algorithm. 
So what is the efficient coding 
algorithm? 
Well, there's several different kinds. 
So one is called sparse coding and this 
was suggested by Olshausen and Field. 
Another one is independent component 
analysis and this is suggested by Bell 
and Sejnowski. 
And then another algorithm called 
predictive coding. 
Was suggested by myself and Dana Ballad 
this was back when I was a graduate 
student I think I'm giving away my age 
now, but that's okay. 
And basically, these three types of 
efficient coding algorithms give rise to 
a set of receptive fields that I have 
been optimized as a less. 
Reconstruct these particular tiny image 
patches within these natural images as 
faithfully as possible. 
So, let's look at what these receptor 
fields that started out as random, but, 
then they were tuned by the efficient 
coding algorithm, what they look like 
after they've converged. 
Aha, so here they are. 
What you see here is that each of these 
is one particular receptor feel that has 
been learnt from natural images by the 
efficient coding algorithm. 
And you'll see that each of them seems to 
have remarkably structured that's quite 
similar to what we saw before in terms of 
these types of receptive feels in V1. 
In other words, they are receptive fields 
that are oriented, so there's an 
orientation here and they have both the 
white and dark regions. 
So in other words, the plus and the minus 
that we saw in the V1 receptive fields, 
you see them here also. 
And you can see that they're localized to 
different locations, so they're very 
specific to location, which is another 
characteristic of receptive fields. 
And they're oriented and they have 
different orientations that span the set 
of orientations that you might get in 
natural images. 
And so what is the conclusion that we can 
draw? 
The conclusion is that the brain maybe 
trying to find faithful and efficient 
representations of an animals, natural 
environment. 
So people have applied this principle 
also to other kinds of inputs, such as 
sounds for example. 
And they find that the auditory cortex 
representation is also quite explainable 
by this type of principle, the principle 
of efficient coding. 
Okay, great. 
So we'll explore a variety of these types 
of Descriptive, Mechanistic, and 
Interpretive models throughout the 
course. 
But before we do that, we have to do one 
thing and you might guess what that is? 
No, it's not homeworks. 
It's being introduced to neurobiology. 
So a lot of you might not have a 
background in neurobiology. 
So for those of you have, who have never 
taken a neurobiology course before, the 
next set of lectures will introduce you 
to neurons, synapses, and also brain 
regions. 
So until then, adios, amigos, and amigas. 

[MUSIC]. 
Hello and welcome to Neurobiology 101. 
In this lecture and in the next two 
lectures, we'll introduce you to neurons, 
synapses, and brain regions. 
Now you might have already taken a course 
or two in neuroscience or neurobiology in 
your careers. 
And if so, you might consider skipping 
this lecture and the next two ones. 
But on the other hand, if you do so, you 
might miss on some of the entertainment 
we've planned in these lectures. 
Okay, let's get started. 
If the brain is a stage, the star of the 
show are the Jackie Chan, Julia Roberts, 
Shah Ruck Khan and Tom Cruise are rolled 
into one. 
Would undoubtedly be the neuron. 
Now here is an example of the neuron, 
this is a neuron from the visual cortex 
and you can see that the neuron is 
extremely tiny. 
The size of the cell body is only around 
25 microns. 
That's one micron is one millionth of a 
meter. 
And you can also see that the cell has 
different branches, so the neuron has all 
these branches which are called 
dendrites. 
And there's also one slender branch that 
goes down like this and that's called the 
axon. 
And as we'll see later, that conveys the 
output of the neuron. 
Now this is not the only type of neuron 
that exists in the brain. 
Let's look at some other types. 
There is in fact, a veritable zoo of 
neurons, as shown by these wonderful 
drawings by Ramon Y Cajal from the turn 
of the last century. 
Here are some cells from the visual 
cortex. 
And you'll notice that some of them, such 
as this one and this one, have a 
triangular shaped cell body. 
These are called pyramidal neurons, 
partly because of the shape of their cell 
bodies. 
But also because their axons form what is 
known as the pyramidal track in the motor 
system. 
Now here we have some cells from the 
cerebellum. 
So these two cells over here are called 
purkinje cells. 
And you'll notice they have a very 
interesting branching structure in their 
dendrites. 
And then finally here we have some cells 
from the Optic Tectum. 
And once again we notice a wide variety 
of cells. 
And you'll notice that they have 
different branching structures depending 
on where they are in terms of the depth 
of their location. 
Now when Ramon Y Cajal made these 
drawings back in the early 1900's, there 
were some controversy in the field 
regarding the structure of the brain. 
Now the two competing hypothesis were 
that, one, that the brain is a continuous 
network and this was called the vaticular 
hypotheses. 
And the other hypotheses was that the 
brain consists of discrete cells. 
And when Ramon Y Cajal made these 
drawings and he had very clear 
observations that the cells were indeed 
discrete. 
This led to what is known as the neuron 
doctrine. 
So what is the neuron doctrine? 
The neuron doctrine states that the 
neuron is the fundamental structural and 
functional unit of the brain. 
And moreover, the neurons are discrete, 
and thereby not continuous with other 
cells. 
So there's actually some exceptions to 
that rule, but it holds true for a 
majority of the brain regions. 
And finally, the third part of the neuron 
doctrine states that, information flows 
from dendrites via the cell body to the 
axon. 
And once again, there are some exceptions 
to this rule, but it holds true for a 
majority of neurons. 
So, this leads us to what we might call 
an idealized model of a neuron. 
So let's look at what an idealized model 
of a neuron looks like. 
Here's the idealized model of a neuron. 
You can see that it has these dendrites, 
as well as a cell body and then the axon 
that conveys the output of the neuron to 
other cells. 
Now the inputs to the neuron might come 
from axons, from other neurons and how do 
these inputs look like inside the neuron? 
They look something like this. 
So each time you have an input from a 
neuron that is then translated into some 
activity inside this neuron. 
It's going to give rise to what is known 
as an EPSP or Excitatory Post-Synaptic 
Potential. 
That's one kind of input that the cell 
might receive. 
And if there's a whole bunch of them, so 
here's one, two, three, four. 
And these are all EPSP's that arrive 
let's say almost simultaneously, then 
what you get at the cell body or indeed 
in these regions. 
If the, if these arrive in neighboring 
regions, it's a summation of the EPSP's, 
and if there's enough of them, then the 
summation might reach a threshold. 
And what you will get out of that is 
something called an action potential or 
spike. 
So here is a depiction of an action 
potential. 
And so what we have here is basically a 
really simple model of inputs coming in 
from other neurons being summated in the 
cell body. 
Followed by an output, which is called a 
spike or action potential. 
And that happens if some particular 
threshold is reached. 
When you have enough inputs coming into 
the neuron. 
So that is a really simple depiction of 
what happens in a neuron. 
Now let's go into a little bit more 
detail. 
Let's start by asking what is a neuron. 
Well a neuron is nothing but a leaky bag 
of charged liquid. 
Now, I know that doesn't sound too 
attractive, but that's basically where it 
is. 
It's a leaky bag of charged liquid. 
Now, why is it a bag? 
Well, it's a bag because the contents of 
the neuron are enclosed within a cell 
membrane. 
What does the cell membrane consist of? 
It's actually a lipid bi layer. 
So, here's a depiction of that. 
The bi layer. 
And what do we mean by lipid? 
So do you know what a lipid is? 
Well, lipid's are nothing but fat. 
And so what we have here is basically a 
fatty bi layer. 
That is enclosing the contents of the 
neuron. 
Now the bi layer. 
The cell membrane is impermeable to 
charged ions such as sodium chloride and 
potassium which are dissolved on the 
outside. 
And are also present on the inside of the 
cell. 
Now, if that was all that there was in 
our brains, just a cell membrane with 
things on the inside and things on the 
outside. 
Nothing much would be happening and 
things would be pretty boring. 
So, things really get interesting because 
embedded within this, this particular sub 
membrane are what are known as ionic 
channels. 
Now these ionic channels allow ions to 
pass from the outside to the inside or 
from the inside to the outside and that's 
where all the action happens. 
And so we'll be looking at the actions of 
these ion channels in a little bit more 
detail in the next few slides. 
Now you can see that the neuron has an 
electrical personality because it 
maintains a potential difference between 
the outside and the inside. 
So the inside of the cell, the inside of 
a neuron is approximately minus 70 
millivolts compared to the outside. 
So if the outside is zero millivots, the 
inside is minus 70 millivolts. 
And this is basically called the resting 
membrane potential because this is when 
there is no spike or action potential. 
And this particular difference between 
the outside and the inside is. 
Because there is a higher concentration 
of certain ions on the outside compared 
to the ions on the inside. 
So, in particular, there's more of sodium 
and chloride on the outside, compared to 
on the inside. 
Now that's pretty interesting. 
So why should there be more sodium and 
chloride? 
And water, on the outside. 
Compared to the inside. 
Well, if you want to speak on it a little 
bit. 
Maybe it's because it's, reflect to where 
we came from as single cellular organism. 
So recall that life. 
Perhaps began in the ocean. 
And what does the ocean have? 
Well it's NACL. 
Salt, so salty water. 
So perhaps, we're still living. 
At least in our brains, in something like 
the ocean. 
Anyway, that's just a speculation. 
Now, it's not sufficient to just have a 
higher concentration of one set of ions 
and another set of ions on the inside. 
What you really need to maintain this 
difference between the outside and the 
inside is something called an Ionic pump. 
Which actively expels sodium from the 
inside to the outside and then it also 
allows potassium ions from the outside to 
the inside. 
So this takes energy. 
So a lot of the energy that we consume is 
actually devoted to maintaining this 
particular difference between the outside 
and the inside of neurons. 
Well, if it's always minus 70 millivolts 
on the inside of the neuron, then things 
are pretty boring. 
There's not much happening in the neuron. 
So how do we influence a neuron's 
electrical personality? 
Well how do we influence anyone's 
personality, whether it be electric or 
not? 
Well, we need to provide them with some 
inputs, so in the case of neurons what 
this amounts to is asking. 
How can the electrical potential be 
changed in local regions of a neuron? 
And the answer to this question, again 
comes down to, our good old friends, the 
ionic channels. 
So the ionic channels, which I earlier 
mentioned, are embedded in the cell 
membrane. 
Are nothing but proteins, and they're 
selective in that they only allow 
specific ions to pass through. 
So for example this particular ionic 
channel might only allow sodiums. 
So perhaps these blue circles depict 
sodium ions, and so only sodium ions are 
allowed to pass through. 
Whereas the potassium or chloride ions 
which might be these yellow and red 
circles are not allowed to pass through. 
Now to make things a little more 
interesting, the ion, the ionic channels 
are gated. 
So what do we mean by that? 
Well, they're gated in the sense that 
they might change their property of being 
open or closed to ions. 
According to conditions that are existing 
in the neighborhood of the ionic 
channels. 
So for example, there's three different 
types of ionic channels. 
One is voltage-gated, so in this case, 
the probability that the ionic channel is 
open depends on the local membrane 
voltage. 
So if the membrane voltage is high, for 
example, then the channel might open and 
then it might allow sodium ions to come 
through. 
And if the membrane voltage in this local 
region is low then perhaps it will close 
and then no longer will sodium ions be 
able to come inside the cell. 
Now similarly another set of ionic 
channels are chemically gated. 
So in this case the opening of the 
channels is determined by the presence of 
certain chemicals. 
And these chemicals bind to the ionic 
channel, and the binding causes the ionic 
channel to open and let inside certain 
ions. 
Or perhaps let ions from the inside go 
outside. 
We'll actually come to these chemically 
gated ionic channels later when we 
discuss synapses. 
Synapses are examples of chemically-gated 
ionic channels. 
And finally, there's another class of 
ionic channels which are called 
mechanically-gated, because they are 
sensitive to pressure or stretch. 
And so they open or close depending on if 
there is pressure or if there's a stretch 
in the, the neighborhood. 
These gated channels are what allow 
neuronal signaling and communication 
between neurons in the brain. 
So here's an example, suppose you have 
spikes that are arriving from another 
neuron, and the spikes arrive and 
terminate at these locations. 
On this neuron number two, so these 
junctions between the neurons are called 
synapses, and let's see what happens in 
this particular region here. 
So when you have the inputs from other 
neurons, in this case the input coming 
from this neuron's axon. 
You have an opening of the chemically 
gated channels at these locations. 
So, these chemically gated channels are 
at these synapses, one, two, and three, 
and when these chemically gated channels 
open. 
They are going to cause changes in the 
local membrane potential. 
So for example here, if there are some 
sodium channels they might start opening, 
and that in turn is going to cause sodium 
ions from the outside to come inside. 
So do you know why they are coming 
inside? 
Well, remember that there's more sodium 
ions on the outside, there's a higher 
concentration. 
Which means that there's going to be a 
diffusion of these sodium ions into the 
inside. 
And since sodium ions are positively 
charged, they're going to cause an 
increase in the local membrane potential. 
So what happens when you have an increase 
in the local membrane potential? 
Well, this in turn is going to cause 
opening or closing of voltage-gated 
channels. 
So remember there's a second type of 
channels called voltage-gated. 
So when the local membrane potential 
changes, is going to cause these voltage 
gated channels to also start opening and 
closing. 
And this in turn is going to result in 
either a depolarization which is, that's 
a big word. 
But what it really means is a positive 
change in the local voltage of the cell. 
And it might also, in the other case, it 
might cause a hyperpolarization. 
So that's when for example, what will be 
open here are some potassium channels, so 
remember that there is more potassium on 
the inside compared to the outside. 
And so when these potassium channels 
open, then you are going to have some of 
the positive charge. 
So potassium is positively charged, you 
are going to have those potassium ions 
diffusing to the outside. 
And that is going to result in a negative 
change in voltage. 
So in either case, you're going to have 
some changes in the local voltage of the 
different parts of the cell. 
And now if there's a strong enough 
depolarization, which is, there's a 
strong enough excitation. 
And then the excitation reaches that 
particular threshold that we talked about 
then you're going to get a spike or an 
action potential. 
So let's look at the action potential in 
a little bit more details as to how it's 
generated as a result of these voltage 
gated channels. 
So here's the action potential, and. 
The reason why we have this particular 
shape of the action potential is because 
of voltage-gated channels. 
So let's see how an action potential is 
generated. 
So when there's a strong depolarization 
or excitation in the cell then we have an 
opening of potassium channels. 
And that's because these potassium 
channels are voltage gated. 
And so when there's an increase in the 
membrane voltage you're going to have 
these potassium channels opening, and 
that in turn causes a rapid influx. 
Of these sodium ions from the outside of 
the cell into inside the cell. 
And this causes even more channels to 
open, because remember these are 
voltage-gated channels. 
And so when the membrane potential 
increases, then even more channels are 
going to open. 
And so here's a depiction of what 
happens. 
So initially we have these sodium 
channels opening, and that's given by the 
initial part of this trajectory of the 
action potential. 
And as the membrane potential increases 
you have more sodium channels opening and 
that's basically you can look at it as a 
positive feedback loop. 
And so the more these sodium channels 
open, the more the local membrane 
potential increases. 
And the more these sodium channels open 
until you get to this very top and that's 
when you have these channels 
inactivating. 
So, when these channels inactivate at the 
same time, or approximately at the same 
time, you also have the potassium 
channels opening. 
So we have the sodium channel closing, 
and the potassium channels opening. 
And as the potassium channels open, you 
know what happens? 
Well, since there's more of the potassium 
ions on the inside, you're going to have 
an outflux of the potassium ions. 
And that causes a decrease in the 
membrane potential. 
And that's responsible for this downward 
slope of the action potential. 
And finally. 
You get to the point where, at the very 
end here of this cian block you're going 
to have the potassium channel also 
closing and that completes the action 
potential. 
So you have this very stereotypical shape 
of the action potential, which is given 
by the sodium and the potassium channels 
opening and closing. 
And this particular shape is, as we 
mentioned, very stereotypical. 
So it means that there really isn't any 
information communicated between neurons 
in the shape of the action potential. 
So it's not that one neuron has a short 
action potential and another one has a 
long, tall action potential or that. 
And one neuron has a very broad action 
potential. 
Another one has a very narrow action 
potential. 
They're all going to look the same 
because you have the property that the 
action potential is given by the dynamics 
of the sodium channels opening and then 
closing along with the potassium channels 
opening and closing. 
So what happens to the action potential 
after it's generated at the initial 
segment of the axon? 
Well, it's propagated along the axon as 
depicted by this animation. 
So at each location on the axon you have 
these sodium channels opening first and 
then that causes the rising edge, the red 
part of the action potential. 
And then this is followed by those sodium 
channels closing and the potassium 
channels opening as we saw in the 
previous slide. 
And that causes the fall of the action 
potential, so essentially you have the 
action potential propagating along. 
The axon, as shown in this manner due to 
the opening and closing of the potassium 
and sodium channels along the axon. 
If you thought that was a neat trick, 
wait til you hear about this other trick 
that neurons have up their sleeve. 
It's called myelination. 
So there are cells called 
oligodendrocytes that grow these sheaths 
called myelin around the axon of cells. 
So these sheaths are insulating sheaths, 
so they don't allow charge to pass 
through. 
But they do leave open certain areas. 
These are uncovered regions of the axon. 
These are called, nodes of Ranvier. 
And so, here in this picture, the green 
are the Myelin sheaths, and the red, 
uncovered parts are called the Nodes of 
Ranvier. 
Now, why should the brain go to the 
trouble of covering parts of the axon, 
and not covering other parts of the axon. 
Well, it turns out that there is a very 
important function that the myelin 
serves. 
And in particular the myelinated axon 
allows fast long-range communicational 
spikes. 
So for example, if you want to 
communicate spikes from the brain to 
other areas of the nervous system such as 
distant parts of the spinal cord then the 
myelinated axon does the job for you. 
How does it do its job? 
Well, here's what it does. 
It allows the action potential that's 
generated near the cell body to 
essentially hop from one non-myelinated 
region which is one of the nodes of 
Ranvier to the next. 
And this is something that's called 
saltatory conduction. 
So, let's see how this works in this 
case. 
So if you have an action potential that's 
generated in this particular node of 
Ranvier then through the cytoplasm. 
There's a propagation of the charge and 
there's a buildup and another. 
Action potential that's generated at each 
node of Ranvier, and so on until it 
reaches the termination point. 
And so what you have is a fast and a 
robust method of communicating spikes 
along the axon to its destination. 
So we can call this a form of active 
wiring. 
And this allows loss less signal 
propagation. 
A very useful property to have. 
And you might have heard of a disease 
called multiple sclerosis. 
And this disease happens because there is 
an auto immune response where you 
essentially loose a lot of the myelin. 
On your axons and so in that case 
unfortunately the cell loses it's 
capacity to send these action potentials 
or spikes in a fast and robust manner. 
So it will manifest itself in various 
forms and different symptoms all caused 
by the fact that the axons are no longer 
myelemated. 
Okay, so let's summarize what we've 
learned in this lecture. 
We learned about neurons, and in 
particular, we learned that a neuron has 
a cell body. 
It has these branches, that are called 
dendrites. 
And a neuron also has a long, slender, 
fiber, called an axon. 
And the axon carries the output of a 
neuron. 
These outputs are essentially electrical 
impulse called spikes or action 
potentials. 
And these action potentials are delivered 
at these junctions between neurons and 
these junctions are called synapses. 
So what happens at the synapse. 
So what happens to the action potential, 
once it reaches the synapse. 
That is something we going to learn in 
the next lecture. 
So until then, ciao and[UNKNOWN] 

Hello again and Namaste to all of you.
The last lecture we talked about ion
channels and
how they give rise to action potentials or
spikes.
In a later lecture we will describe this
mathematically, using
a model first proposed by Hodgkin and
Huxley in 1952.
Hodgkin and Huxley got a nobel prize for
their computational model.
Now what does that mean for you?
It means that you too can could get a
Nobel prize someday by solving problems
and computational neuroscience.
But before you start making plans for that
trip to Sweden, we first need to
learn about what happens to a spike when
it reaches the end of an axon.
This is where we meet the synapse.
So what is a synapse?
A synapse is a connection or junction,
between two neurons.
There are two different kinds of synapses.
The first kind are called electrical
synapses.
They use something called, gap junctions.
The second kind of synapses are chemical
synapses, and they use chemicals known as
neurotransmitters.
Let's first look at electrical synapses.
Now here's an example of an electrical
synapse
between two neurons, Neuron A and Neuron
B.
Now, electrical synapses function in a
manner very
similar to the way that the connections
between
components in your cell phone in your
computer function.
They allow the activity from one side, in
this case neuron A,
to be directly propagated to the other
side, in this case neuron B.
So they change the voltage on one side,
given
some activity or voltage changes on the
other side.
And the way that neuron A communicates
with neuron B, or vice-versa,
is through what is known as a gap
junction, here.
And the gap junction is depicted here in
terms of these.
So these are essentially ionic channels.
So here's one, here's another and so on.
And you'll notice that these ionic
channels span the membrane, of both,
neuron A an the membrane of neuron B.
And the result of this particular
arrangement is that if you have,
excitation on one side, due to for
example, an action potential.
And maybe you have, sodium
ions on this side that are, in higher
concentration than on the other side, then
these channels allow these ions to migrate
to the other
side.
And the result of this movement of
ions from one side to the other is that
you're going to have a change
in the membrane potential of Neuron B as a
result of some activity in
Neuron A.
So these types of electrical
synapses are really useful when you want
to have fast connections between two
neurons.
And typically they're found in the case
where you need to synchronize, which is
you want to make neurons fire
simultaneously together.
It turns out to be a useful mechanism when
you want to synchronize sets of neurons.
The other case where you would like
to have these types of fast electrical
synapses
or fast connections between neurons is
when you
have to implement something like an escape
reflex.
And that's something that's found for
example in the crayfish.
So that's an example of an electrical
synapse.
Well what about chemical synapses?
So here's a depiction of a chemical
synapse.
Suppose we have a neuron A and a neuron
B, and here's the action potential, a
spike coming in.
Now, what we have on one side, on the
side of neuron A, are these bags, which
are known
as vesicles.
And these are bags of neurotransmitter
molecules.
So these are chemicals that are stored in
these bags.
And so when an action potential spike
comes in along the axon of Neuron A, it
causes these bags, these bags called
vesicles, to fuse with the membrane.
And in doing so, these bags release the
neurotransmitter molecules into the gap,
between the two neurons.
Now this gap is called a, the synaptic
cleft and so when these neurotransmitter
molecules then fuse with the receptors on
the other side.
So these receptors are nothing but the
chemically gated ionic channels that we
talked about in the previous lecture, then
these chemically gated channels start to
open.
And so you know what happens when these
channels start to open.
Well they are going to allow some ions to
either come inside, or go
outside, according to the concentration of
the ions on the inside or the outside.
So suppose that these are channels that
allow sodium ions to come in.
Then you're going to have sodium ions
coming in into this neuron.
And as a result, you're going to have an
increase in the membrane
potential of neuron B.
So you can see how a spike from neuron A
causes this cascade of chemical events.
And then that in turn causes these
channels to open
and that finally causes some changes in
the membrane potential.
So you're going from electrical activity
to chemical and back
to an electrical change again on the side
of neuron B.
Now you might ask yourself, why did
evolution go to
the trouble of, constructing such a
complex, you know, electrical,
chemical, an electrical connection when
you, you could have just,
used these, electrical connections with
gap junctions to begin with?
Any thoughts on that?
So why would, something like this, like in
a chemical
synapse be useful compared to just a
simple electrical synapse.
So here's a possible answer.
A chemical synapse allow you to change the
way that neuron B
is affected by the same spike by simply
changing the number
of ionic channels or the density of ionic
channels that you have on this side.
And so, if for example for the seams
spike, you want to decrease the amount of
excitation
in B, then you could, for example, take
out, or remove, some
of these ionic channels, and then just
leave a few of them.
And in that case, for the seam spike from
neuron
a you would have A lesser amount of
excitation in B.
But this is the reason why chemical
synapses have been suggested as being the
basis
for learning and memory in the brain as we
will look at a little bit later.
Here's a wonderful picture from the
Kennedy Lab at Cal Tech.
Now what does this picture remind you of?
Well it looks like it could be the picture
of
a city taken at night from an airplane
flying overhead perhaps.
Well actually it's the picture of a neuron
and all of its synapses.
Each of these bright spots corresponds to
a synapse.
And typically, a cortical neuron has up to
10,000
synapses on its dendrites and on the cell
body.
Now the synpases can be either excitatory
or inhabitary, so what does that mean?
An excitatory synapse is one that tends
to increase the post synaptic membrane
potential.
So what does that mean?
Well, if you have a neuron A and a neuron
B and you have a synapse between
them, then we call neuron A the
presynaptic neuron.
And we call neuron B the post synaptic
neuron.
So pre and post.
And so an excitatory synpase is one
that tends to increase the postsynaptic
membrane potential,
which means that it tends to excite the
neuron B.
On the other hand, a inhibitory synapse
is one that tends to decrease the
postsynaptic
membrane potential, which means that it
tends
to decrease the membrane potential of
neuron B.
Now how does that happen?
Let's go through the sequence of events
governing the action of an excitatory
synapse.
So first we have an input spike, so here
is the spike.
And the spike is going to cause
neurotransmitters to be released.
In the case of an excitatory synapse, the
neurotransmitter could be glutamate.
And when these neurotransmitter molecules
are released into the synaptic
cleft, they are going to bind with ion
channel receptors.
So in this case the ion channels could be
selected for sodium,
so you might have these sodium selective
channels opening.
And that in turn is going to cause, sodium
ions, to come
into the cell.
In this case, cell B, if this was cell A.
And now, what's going to happen, on the
side of cell B is, something called
depolarization.
So you might remember the word
from a previous lecture.
And that in turn, causes an EPSP, or
excitatory postsynaptic potential.
That's again something again we
encountered in the previous lecture.
And so, the sequence of events is
basically spike
and then you have a release of the
neurotransmitter glutamate.
And then the ion channels opening causing
sodium to come inside.
And that in turn increases
the membrane potential of neuron B.
Now lets look at an inhibitory synapse.
The sequence of events is quite similar to
what we had for the
excited resynapse except that now we have
a different neurotransmitter and different
ion channels.
So.
We begin again with an input spike which
causes neurotransmitters to be released.
And in
the case of an inhibitory synapses, the
neurotransmitter could be GABA.
Now that's not the famous cricket ground
down in Australia.
GABA stands for gamma-Aminobutyric acid
and it's a
kind of neurotransmitter that when
released binds with receptors.
And the Ion channels in this case, could
for example, be selective for potassium.
So when we have the ion channels
open, we'll have some potassium from the
inside of the cell B.
So this is our cell B here and here is our
cell A again.
Now since there's more potassium ions on
the inside of
the cell, we're going to have a diffusion
of the potassium
ions outside, which is going to cause a
decrease in
the membrane potential of cell B, and that
results in hyperpolarization.
So this is again a word we
encountered before, a decrease in the
membrane potential.
And, that's called hyperpolarization, and
that in
turn causes an IPSP or inhibitory
postsynaptic potential.
So that's the counterpart to the EPSP we
saw in the previous slide.
Now these chemical synapses are important
for
another function besides just
communication between two neurons.
Do you remember
what that function is?
Synapses are thought to be the basis for
memory and learning in the brain.
This is called the synapse doctrine.
This is quite deep actually.
Think about what this means.
This means that all of your memories from
the first time you rode your bike and
crashed
into your neighbors brand new car, to the
first
time you met the love of your life is
all stored in your synapses.
What's more, all of your skills from
typing, dancing,
driving to reading and writing, guess
where it's stored.
It's in your synapses.
Synapeses play such an important role in
our lives
that perhaps we should have a world
synapse day.
What do you think?
Okay.
Maybe not.
But how do synapses play such an important
role in memory and learning?
Let's find out.
Synapses allow learning in the brain
through a mechanism called synaptic
plasticity.
Let's look at one particular example of
synaptic plasticity and this
is Hebbian Plasticity, named after Donald
Hebb, who was a Canadian psychologist.
In 1949, Hebb proposed the following rule
for plasticity.
If a neuron A repeatedly takes part in
firing neuron
B, then we should strengthen the synapse
from A to B.
Now here's a diagram that represents
Hebb's rule for plasticity.
So here's neuron A.
And when neuron A fires, it participates
in firing neuron B.
So here is a single spike from neuron B.
Now, according to the rule for plasticity,
we need
to strengthen the synapse from A to B.
And that's shown by this bigger red
triangle.
So what happens?
So the next time A fires, so here's A and
here's the input from A.
You're going to get a bigger and perhaps a
faster response from neuron B.
Now why is this computational useful?
Well.
Suppose you are in a jungle and it is dark
and then you hear a growl.
Let's say the growl is conveyed by this
firing of neuron A.
Now right after that, neuron B fires, and
you see a tiger.
And you start running and barely escape
with your life.
Now if you increase the strength of the
connection between neuron
A and neuron B as we did here using
Hebbian plasticity.
Then the next time that you hear a growl
and that's given by neuron
A firing, you are going to predict that
you are going to see a tiger.
And you're going to start running even
before the tiger
appears, and you have Hebbian Plasticity
to thank for it.
Isn't that great?
Hebbian plasticity is so useful
that there's in fact a mantra that goes
along with it.
Here it is.
Neurons that fire together, wire together.
Some say that if you repeat this mantra
three times
every morning, you are sure to have a
great day.
But I'm skeptical.
Is there any evidence for Hebbian
plasticity in the brain?
In fact there is.
There's something called long term
potentiation
that researchers have observed in several
areas of the brain most prominently in the
area called the hypocampus.
So LTP is defined as an experimentally
observed increase in
the synaptic trend that lasts for hours or
even days.
And the way you can demonstrate that
experimentally is by measuring the size
of the EPSP is caused by a single input to
this neuron B.
So, if you demonstrate that the size of
this EPSP increases as you pair neuron A
with neuron B and you give lots of
stimulation to neuron A which in turn
causes
some excitation in neuron B.
Then, what you'll observe is an increase
in the size.
So here is the final size.
Here is the initial size, so you can see
that the effect of a single input
to this neuron B has increased from being
this little bump to this really big bump.
And that is taken to be evidence for an
increase
in the synaptic strength from A to B.
What about the opposite?
Is there something that, can decrease the
synaptic strength between two neurons?
And which could last for hours or days?
Well, there is something called long term
depression.
That's not the same as the kind of
depression you get when you fail an
exam or you get a rejection letter.
That's a different kind of depression.
This is an experimentally observed
decrease in the synaptic
strength from one neuron to another, that
lasts for hours or even days.
Now how do we experimentally observe long
term depression or LTD?
Well, you can do the same thing that you
did with the LTP.
You could look at the
EPSP that's generated when you stimulate
one neuron
you observe the EPSP in the second neuron
B.
Now if over time there's a decrease in the
size of the EPSP until
you get a smaller sized EPSP for the same
input, from A to B.
Then you have evidence, that there has
been a
decrease, in the synaptic strength from A
to B.
There's a recent twist to the story of
synaptic plasticity that I thought I
should mention.
And that is that synaptic plasticity
depends on spike timing.
So what we mean by spike timing?
Well, it turns out that whether you get
LTP or LTD
depends on the relative timing of the
input or output spikes.
So remember that we have a neuron A that
has
a synaptic connection to neuron B.
And so when you have an input, so we're
calling A the input, and B the output
neuron.
So when we have the situation that the
input spike is before the output
spike, which means that neuron A fired and
then you have neuron B firing.
Then, what you have is the situation such
as this, so here's the input output
pairing, so you have the input happening
before the
output spike, which is given by this spike
over here.
Then what we observe in this case is an
increase in the size of the PSP after
you have repeatedly paired the input and
output In
the sequence that the input occurs before
the output.
Now the interesting case is where you have
the reverse situation
where the input spike occurs after the
output spike.
So, you have B firing and then you have A
firing.
And in that situation, here is the
situation here, here is the pairing.
So you have the output spike here and
then you have the input spiking right
after that.
And so if you keep doing this pairing for
several repitions,
you're going to observe that the EPSP size
diminishes
from being like that to something that
looks like this.
And that's what we mean by long term
depression.
So we have both LTP and LTD.
So if you summarize this particular result
for different intervals between
the input and the output spike you get a
window of plasticity that looks like this.
So we have one portion of it has LTP, the
other LTD,
and here is the delta T.
Between the input and the output spikes.
So the input happens to be before, the
output.
So the delta t is, bigger than zero, then
you are in this portion, of this plot.
And you can see that if the input happens
before the output, you have, LTP
or an increase in the synaptic strength.
If the input occurs after the output where
the delta T is negative then you have
LTD or a decrease in the synaptic strength
from neuron A to neuron B.
So this is actually quite amazing because
you can see that there is a very
short interval of about 40 milliseconds
between the input and output.
And depending on where, the input spike
occurs with respect to the
output spike, you could get a dramatic
shift, from LTD, to LTP.
So, the neurons, in several brain areas
including the cerebral cortex,
seem to be very sensitive to, the timing,
of the input versus the output spike.
And we learn later that this type
of sensitivity to the timing of spikes is
very important for learning sequences to
make predictions.
Okay.
To summarize the last couple of lectures,
we seem to
know a lot about ionic channels, about
neurons, about synapses.
But what do we know about how networks of
neurons give rise to perception, behavior,
and maybe even consciousness?
Well, not as much.
This is actually one of the primary
motivations for the recent
large scale brain projects that have been
announced in Europe and the US.
In Europe, the human brain project, led by
Henry Markram, will
attempt to construct a large scale
computer simulation of the human brain.
While in the US, President Obama has
announced the brain initiative to map the
activities of hundreds thousands of
neurons simultaneously
in order to understand how large scale
networks of neurons give rise to
perception action and cognition.
In the next lecture, we will briefly cover
what we
do know about networks in the brain and
their function.
Until then, bye bye.

Hello neuro adventurers. 
We're nearing the end of our safari 
through the land of neurobiology. 
I have my adventure hat on. 
Do you? 
In the last two lectures, we had close 
encounters with those wild creatures 
called neurons, and their close 
companions. 
The synapses. 
In this lecture we'll stop at a few 
tourist destinations in the nervous 
system. 
The nervous system has two parts. 
The peripheral nervous system and the 
central nervous system. 
We'll make a stop first at the peripheral 
nervous system. 
The peripheral nervous system consists of 
two components, the somatic and the 
autonomic nervous systems. 
Let's first look at the somatic nervous 
system. 
The somatic system consists of nerves 
connecting to all of your voluntary 
muscles and your sensory receptors. 
Now what do we mean by nerves? 
A nerve is nothing but a bundle of axons. 
So you'll recall that axons are the 
output cables of neurons. 
And so, a collection of axons or bundle 
of axons would correspond to what we call 
a nerve. 
Now, let's take an example of where we 
see the somatic system in action. 
So, suppose you are about to shake the 
hand of one of your friends. 
Now in order to shake your friend's hand 
you have to move your arm and your hand 
towards the person's hand. 
So in doing so you're going to be using 
the somatic nervous system to send 
commands from your brain and spinal cord. 
To your hand and arm to move towards the 
person's arm and hand. 
And in return, when you shake your 
friend's hand, you're going to get the 
sensation of shaking your friend's hand 
through the sensory receptors. 
That are on your skin in your hand. 
And so, in doing so we are using two 
different kinds of nerve fibers. 
The first one is called, afferent nerve 
fibers, and these correspond to all the 
axons that are carrying sensory 
information from the periphery. 
Which in this case the hand to the 
central nervous system, which is your 
brain and your spinal cord. 
Now, in order to shake your friend's 
hand, you send some commands from your 
brain to the muscles in the hand. 
In doing so, you were using what are 
called efferent nerve fibers. 
These are the outgoing nerve fibres, or 
axons that are carrying information from 
the central nervous system, which is the 
brain and the spinal cord, outward 
towards the periphery, which in this case 
are the muscles in your, hand. 
The second component of the peripheral 
nervous system is the autonomic nervous 
system. 
And the autonomic system consists of 
nerves that connect to all of your 
internal organs such as the heart, blood 
vessels, various smooth muscles and 
glands. 
And the autonomic nervous system largely 
operates below the level of 
consciousness. 
And it regulates various vital functions 
such as your heart rate, digestion, 
respiratory rate, etc. 
And it's also responsible for some 
important functions, such as the fight or 
flight response. 
The other major part of the nervous 
system is the central nervous system, or 
CNS. 
The CNS consists of the spinal cord and 
the brain, and so let's first look at the 
spinal cord. 
Now remember the time that you were 
extremely hungry and you baked a pizza in 
your oven, and when you reached in to get 
your pizza out you accidentally touched 
the extremely hot pizza pan. 
Now even before you could say the word 
ouch. 
Your hand had already been drawn from the 
pizza pan. 
How did that happen? 
Well you can thank something called the 
reflex arc, that is implemented via the 
spinal cord, so here's a diagram of the 
reflex arc. 
And so when your hand touched the 
extremely hot pan, the information was 
conveyed to set off neurons in your 
spinal cord, and that in turn triggered 
spikes that activated the muscles that 
then withdrew the hand from the extremely 
hot pan. 
And so the spinal cord is responsible for 
such local feedback loops, that are 
called reflex arcs. 
Now if that's the only thing that the 
spinal cord did then things wouldn't be 
that interesting, and we would lead 
basically lives that are just full of 
reflexes. 
But instead the spinal cord also has. 
Information coming in from higher brain 
centers, so these are motor control 
signals coming from the brain and these 
activate spinal motor neurons in specific 
ways. 
So for example the brain could tell the 
spinal neurons to implement the procedure 
for walking and so you would be walking. 
through the help of your spinal neurons, 
while the brain could perhaps be doing 
other more interesting things such as, 
you know, talking to your best friend on 
your cell phone. 
Now, as you're doing that, you might 
actually trip over a rock that you didn't 
see. 
And so, in that case, you do need to 
commit that information up to your brain. 
And perhaps that'll interrupt your 
talking on the cell phones. 
In that case, these ascending sensory 
axons convey the information from muscles 
and the skin. 
Back to the brain so that the brain can 
take appropriate actions. 
And so, the spinal cord not only executes 
local feedback moves, but it also uses 
information from the brain to regulate 
the behaviors. 
And as well as convey information or 
feedback from the other parts of the body 
to the brain, so that the brain can take 
the appropriate actions. 
Now we've been talking a lot about the 
brain. 
What are the major regions of the brain? 
And what functions are these brain 
regions involved in. 
Let's do a quick tour. 
Let's start from the rear of the brain. 
This is the region called the hindbrain, 
and the hindbrain consists of three 
different regions. 
So, the first one is the Medulla 
Oblongata. 
And this structure controls basic 
functions such as breathing, muscle tone 
and blood pressure. 
The second structure is the pons, and 
that's the structure over here. 
And it's involved in sleep and arousal. 
And it's also connected to the 
cerebellum, which is the. 
Other major structure in the hindbrain. 
So the cerebellum is involved in a lot of 
things, including voluntary movements, 
maintaining a sense of equilibrium, and 
more recently it's also been implicated 
in language and attention. 
As we move towards the middle of the 
brain, we encounter, quite appropriately 
The midbrain and something called, the 
reticular formation. 
The midbrain controls things like, eye 
movements and your visual and auditory 
reflexes. 
So for example, if there was a loud bang 
in some particular location around you? 
You have a reflex, an auditory reflex. 
That will make you orient yourself 
towards where the loud bang came from, 
and that's being implemented by your 
midbrain. 
The reticular formation performs a lot of 
basic functions including breathing, pain 
perception, it's involved in some muscle 
reflexes. 
It also regulates your sleep and 
wakefulness, as well as, your arousal. 
Near the center of our brains, we find an 
important structure known as the 
thalamus. 
The thalamus consists of a number of 
nuclei. 
What are nuclei? 
Nuclei are essentially clusters of 
neurons that can be defined anatomically. 
The thalamus is traditionally regarded as 
a relay station for all the sensory 
information coming in from our sensory 
organs, such as the eyes and the ears. 
And the sensory information is conveyed 
to the cortex, the structure that we will 
consider in the next slide. 
Now, interestingly the sense of smell is 
not conveyed through the thalamus. 
It goes directly to the cortex. 
the thalamus also regulates sleep and 
wakefulness. 
The other structure that is right below 
the thalamus anatomically is the 
hypothalamus, and the hypothalamus 
regulates our basic needs or if you 
want to call it the four F's. 
Fighting, fleeing, feeding and mating. 
We finally reached the pinnacle of the 
brain and indeed some say the pinnacle of 
evolution. 
The cerebrum. 
The cerebrum consists of the cerebral 
cortex along with other structures, such 
as the basal ganglia, the hippocampus, 
and the amygdala. 
The cerebrum is involved in a diverse set 
of functions ranging from perception and 
action. 
Two various cognitive functions 
including, learning and memory. 
Now one of the most important structures 
in the cerebrum is the cerebral cortex. 
And the cerebral cortex plays a very 
important role in, especially, functions 
that define what it means to be human, 
such as language. 
So let's look at the cerebral cortex in a 
little bit more detail. 
The cerebral cortex is a layered sheet of 
neurons. 
It's about one eighth of an inch thick. 
The famous computational neuroscientist 
Christof Koch has likened the cerebral 
cortex to basically a 14 inch pizza that 
has been stuffed inside of your skull. 
Now here is the sheet that forms the 
cerebral cortex, and you can see that 
when you enlarge a portion of it. 
It consists of these layers. 
Of neurons. 
And the cerebral cortex contains about 30 
billion neurons. 
And each of these neurons makes 
approximately 10,000 synapses. 
And that leaves us with a staggering 
total of 300 trillion connections just in 
the cerebral cortex. 
Now here's an intriguing fact about the 
cerebral cortex. 
It has six layers of neurons, and it 
seems to be relatively uniform in 
structure across different cortical 
areas. 
So if I gave you some cortical tissue 
from that cortical area, and that 
cortical area, you'd be hard pressed to 
find. 
Any differences between this cortical 
area where it says another cortical area. 
And if you look at the way that the 
inputs and the outputs are organized. 
You again find some regularities. 
So the input which might come for example 
from the thalamus. 
Seems to always terminate in the middle 
layers, or layer four. 
And then the output back to the thalamus 
for example, seems to come always from 
layer six. 
And then the output to other subcortical 
regions, or regions that are below the 
cortex, tend to come from layer five. 
Similarly, the output to so called higher 
cortical areas. 
Or areas that process more abstract 
information. 
Tend to come from the, so called, 
superficial layers. 
Or layers two and three. 
Whereas the input or the feedback from 
these higher cortical areas. 
Come and terminate in layer one besides a 
few other layers. 
And so, the suggestion that's being made 
is that there might be a common 
computational principle that is operating 
across cortex. 
Now if that's true or not, or what is 
that computational principle. 
Is something that perhaps you as a 
computational neuroscience researcher can 
try to unravel, or we might make that a 
homework problem. 
Okay great, so now that we've learned 
about brain regions how do they interact 
to produce cognition and behavior? 
Well I'm sorry to say that we don't know 
fully yet. 
But, I'm an optimist, and I think in the 
next 30 to 50 years we might be able to 
figure this out based on a whole bunch of 
techniques that have been invented 
ranging from electrophysiological, 
optical optogenesis, molecular, 
functional imaging, psychophysical, and 
various anatomical, and connectomic. 
Studies, as well as traditional brain 
damage or lesion studies. 
Okay, so let's now do a head to head 
comparison of neural versus digital 
computing. 
Are you ready? 
Here we go. 
First, device count. 
The human brain has, as we've mentioned 
earlier, 100 billion neurons, and each of 
these neurons has up to 10,000 
connections. 
The silicon chip on the other hand has, 
you know, pretty comparable to the human 
brain in terms of the number of neurons. 
We have about 10 to the 10 transistors, 
but these transistors are very sparsely 
connected, so the brain wins in terms of 
the number of connections per component. 
What about device speed. 
So biology has up to a 100 microsecond 
temporal resolution for some neurons, but 
digital circuits are approaching now a 
100 picosecond clock or 10 gigahertz. 
And so digital circuits pretty much beat 
biology hands down when it comes to 
device speed. 
But then how does biology, the brain, 
actually outperform digital circuits in 
some particular tasks, such as perception 
of fast object recognition, speech 
recognition and so on. 
Well, the secret lies in the computing 
paradigm that the brain uses compared to 
digital computers. 
The brain uses massively parallel 
computation using networks of neurons, 
and furthermore it uses adaptive 
connectivity, as we saw in terms of 
synapses that can change the strength 
using LTP and LTD. 
Digital computers, on the other hand, for 
the most part use sequential information 
processing, using the traditional Von 
Neumann architecture with CPUs and fixed 
connectivity. 
And this, in turn, means that the two 
different types of computing systems have 
different capabilities, at least for now. 
Digital computers excel in math and 
symbol processing, whereas brains at 
least at the current point in time are 
better at solving what are called 
ill-posed problems. 
And these are problems such as speech and 
vision. 
Okay. 
So, let's summarize what we have learned. 
The structure and organization of the 
brain suggests specific computation 
analogies in terms of information 
storage. 
It appears that information in the brain 
is stored in the physical and chemical 
structure of neurons and synapses. 
In terms of information transmission, we 
know that neurons use spikes which are 
electrical, as well as they use synapses 
which are chemicals. 
So, the brain appears to be using both 
electrical and chemical modes of 
signalling. 
In terms of the primary computing 
elements, these are neurons, and finally 
what is the computational basis of the 
brain? 
Well unfortunately, we do not know the 
answer to that question, but we can be 
hopeful that perhaps at the end of the 
course, through all the discussions that 
all of you are going to have on our 
course discussion board, maybe we'll 
figure this out. 
but that still remains to be seen. 
But what we will do in the course, and 
that we will do for sure, is that we will 
try to understand computation in the 
brain through our three different kinds 
of models: descriptive, mechanistic, and 
interpretive. 
Thank you all for joining us this week. 
Hope you enjoyed the safari. 
Next week, our other guide, Adrienne 
Fairhall, will introduce you to 
descriptive models of neural encoding and 
decoding. 
I'll be back for the last three weeks of 
the course to tell you about networks and 
learning. 
Until then, this is Rajesh Rao signing 
off. 
Good luck, and don't forget to keep your 
neurons happy. 

[MUSIC]
Welcome to week two of computational
neuroscience.
I'm Adrian Fairhall.
Last week's introduction gave you a high
level overview of
some of the concepts we'll be covering in
the course.
Today we're going to start on the subject
of the
first half of the course which is neural
coding.
As you know and heard last week brains do
many, many different kinds of
things, but one of the best studied
is the representation and transformation
of sensory information.
So the first part of this course will be
an introduction to common concepts
that are used in thinking about and
quantifying the way that information is
represented
in the brain, and how we can extract that
information by monitoring brain activity.
Our ability to respond to the world and
think about it purely
inside our head requires a transformation
of information from one form to another.
Sensory signals or maybe written language
are converted into physical
processes inside our brains that contain
some version of that information.
A natural
way to talk about this is in terms of a
code and our task is
to discover the form in which information
is represented in the activity inside our
heads.
Today we'll be talking about how to go
about cracking this code.
So, first we'll discuss some techniques
for recording from the brain.
We'll then talk about tools for
discovering how the brain
represents information and models that
express our understanding of this
representation.
Next week we'll go on
to thinking about some methods for
inferring what the brain is
doing based on recordings of acts, of it,
of it's activity.
The following week we'll talk about,
information theory
which is a method to quantify neural
representations.
And finally in the fifth week we'll talk
about the biophysical
basis of how the brain processes inputs
and performs complex computations.
So the methods that we use to collect
brain activity determine the ways that
we can analyze it and also the
kinds of information that that activity
can contain.
So let's start by reviewing a few
techniques
that are currently in use to peak inside
brains.
We'll go from large scale to small scale,
starting with some methods that
allow us to record activity from people,
while those people are performing tasks.
So you've probably heard of functional
magnetic resonance
imaging, as this is also used as a
diagnostic
tool in hospitals.
This technique allows us to record from a
persons brain while they're
performing some task, as long as that task
does not require moving around.
The person is placed inside a scanner with
their head in the center of a large
magnet.
The scanner measures spatial perturbations
in the magnetic
field, which are caused by changes in
blood oxygenation.
As different parts of the brain become
active, blood
flows to those areas that support the
underlying neural activity.
This provides a measure of activity over
regions
of the scale of about a cubic millimeter.
Obviously this must represent the average
activity of millions of neurons.
Here's an example of some images that one
can collect from fMRI, showing in color
small regions
of the brains that respond differentially
to some experimental
condition, in this case the viewing of
some images.
While fMRI is a wonderful method for
discovering the approximate regions of
neural activity, as
we've said the responses that are recorded
are
averaged over many neurons and also
they're slow.
The blood flow response to changes in
neural activity
happens over timescales of seconds.
Another method that still relies on
averaging over the responses of
many neurons but has a much
faster response time is EEG
electroencephalography.
This method is faster because it captures
the changes
in the electrical fields of the underlying
neural circuits directly.
Here, one of our former grad students, Kai
Miller, is wearing
a cap covered by electrodes that are
making contact with his scalp.
The downside of EEG is it tends to be a
very noisy signal,
since there are many contributions to the
recorded signal.
Still, methods like fMRI and EEG
are very exciting because, although
they're limited,
they're non-invasive and so they can be
used on healthy, awake human subjects.
Ideally, of course, we'd like to have
access to the activity of single neurons.
In cases when we have direct access to
neural tissue, one
can use devices such as I'm showing here:
a multi-electrode array.
This one developed by physicist Alan
Litke.
Most of the device that you see consists
of electronics and
amplifiers for amplifying the tiny voltage
signals extracted from individual neurons.
At the center, down here, is the array
itself,
blown up on the picture here.
Each electron is about 10 microns across,
roughly the size of
a single neuron, and this array has 512
such electrodes spaced
60 microns apart, so one can record from
many neurons simultaneously,
as is being done here with the slice of
the hippocampus.
The multi-electrode array shown before is
great when it's possible to lay the
tissue directly on the array surface, for
example in experiments using brain slices.
But generally one would like to penetrate
into the brain to see
what neurons are doing when the organism
is carrying out normal behavior.
This rather scary looking electrode is
actually only about, about two millimeters
long.
The tip of each prong is an active
electrode.
In newer versions of such electrodes, that
are being
developed by groups at MIT and elsewhere,
these electrodes can
be moved, individually, into tissue,
allowing one to find active cells.
Other electrodes also have multiple
contact points along them
so that one can record at different depths
simultaneously.
Another beautiful technique for recording
for
many individual cells is through calcium
imaging.
Here, cells contain a calcium indicator
that changes
its florescent properties when calcium
binds to it.
Thus, the fluorescent light intensity is
an indicator
of the amount of calcium inside the cell.
Since calcium enters the cell during
action potentials this signal
acts as a record of the firing activity of
the neuron.
This technique
is being used to record, like here,
for many neurons at once, possibly even
thousands.
It's also possible to use fiber optics to
be able to
open windows like this, our neural
activity deep within the brain.
The electrical techniques mentioned so far
look at the changes in the
electric field outside the cell caused
by signals that the cells generate
internally.
It's also very interesting to look inside
the
cell to learn how these signals are being
generated.
To do this, we can use patch
electrodes, which the experimenter clamps
onto the cell
membrane, and can use to make a direct
electrical contact with the inside of the
cell.
So now we've seen some examples of the
kind of data we have to work with.
Let's move on to talk about the neural
code itself.
Let's start out discussion with some
example experimental data which we'll
take from a very important part of the
brain, the retina.
Your eyes are an outlying but very vital
part of your brain.
The retina is a sheet of cells at the back
of your eyeball that
take light that's focused through the lens
and converts those light signals into
electrical signals.
So by looking at these signals we can get
our first look at the language of the
brain.
Here's a rough cartoon of the experiment.
A retina is dissected from an eye and
placed
on top of one of those multi-electrode
arrays that
we already saw, and it's kept in some
fluid
that will help to keep it alive and
active.
A movie is projected down onto the retina
and
the neurons of the retina respond to the
movie.
So let's zoom in a little on the retina
itself,
which is an excuse, really, to show you a
beautiful image
drawn by Ramon y Cajal, a master Spanish
anatomist
who was active at the turn of last
century.
This is an example of very early
connectomics.
Well, in real life, the cells in the
retina are very densely packed, here
they're drawn schematically in this image
to get a sense of the circuit wiring.
So this image shows you the cells that
capture light,
here at the photoreceptors, both rods,
here, and columns here,
and the successive layers of cells that
accumulate and process the
information from the photoreceptors, until
they finally reach the output cells, here,
the retinal ganglion cells, whose axons
join the optic tack in
heading out of the eye and into the rest
of the brain.
Here's a typical way that we look at
responses of a single neuron.
This is called a raster plot.
Every tiny red dot here is an action
potential or a spike.
When we play the movie once, time goes
this way, the
neuron fires at some particular times,
marked by those red dots.
When we repeat the movie, we can plot the
responses in
that second repetition, and in some cases,
they're almost the same.
We can repeat
it again and again.
The responses for different repetitions
are staggered upward by a little
so that each horizontal strip represents
one repetition of the movie.
So now we're looking at the neural
activity of a group
of about 20 retinal ganglion cells while
the movie was played repeatedly.
In fact, many times, as you can see from
the
number of repetitions in one of these
individual raster plots.
So what you can see hear is that each cell
responds every time at specific sudden
points in the movie.
Sometimes the responses are strong.
Maybe here, and sometimes they're weak,
here.
There are some repeats of the movie, where
the cell doesn't fire at all.
What I hope you can see from this is
something extremely beautiful
and exciting, each neuron is encoding some
feature or features of the movie.
And each neuron is responsible for
a different set of features, sometimes
overlapping.
For example, Cell R and Cell P have a
lot of features in common, but sometimes
they're very different.
So our questions are, how do we use these
responses
to determine what in the stimulus is
making that cell fire.
Looking at this picture one also wonders
how should we think about this concerted
activity?
Does every neuron signal its own message
or is
the population responding as a whole in
some complex code?
Well, we'll not answer that question.
We will be discussing models that, that
describe single neuron and population
level responses.
So the questions we'll be addressing over
the next
few weeks involve how we read out this
code.
There are two ways of looking at it that
are of course quite related.
We can consider the end-coding problem.
How does a stimulus cause a pattern of
responses?
This leads us in the direction of building
quasi-mechanistic models
of our neural system that allow us to
predict it's response.
We can also remain agnostic about the
system
and simply ask what do the responses that
we
observe tell us about what the stimulus
was?
This is the approach one might see at work
in
say a neural prosthetic used to drive a
robot arm whose
job would be to read some measure of
neural data
and activate the arm to move in that
person's intended direction.
So our goal will be to build models of
this type.
Because neural systems are noisy or may
contain information about aspects
that we don't control, we think about the
model as inherently probabilistic.
We would like to know the probability of a
response, given a stimulus.
This is a conditional probability
distribution.
Conversely, in decoding, we'd like to know
the probability of the,
of a stimulus having been shown, given the
response that we recorded.
What we'll have to define and ultimately
discover
is what is the appropriate measure of
response?
What is the right way of thinking about
the stimulus?
And what's the relationship between them
that's embodied by our coding model?
So here's an example of neural
representation of information.
We'd like to find some stimulus parameter,
along which the neural response varies.
So we have stimulus parameter, in
everything that
follows I'm going to call my stimulus
parameter S.
And the meaning of S is going to change
from slide to slide.
And neural response, the simplest way we
can be think about this, and the
approach that we will be taking today, is
that the neural response is some
kind of average firing rate or probability
of generating a spike.
So here are a couple of classic examples
of what we call tuning curves.
Here's data from a neuron in primary
visual cortex, V1.
Such neurons respond to oriented bars of
white, here,
passing through a sudden location in the
visual field.
As the orientation of the bar is charged,
say from almost horizontal
to almost vertical, this particular neuron
at first does not respond at all,
but then starts to fire with a higher and
higher rate.
And then less again, as the orientation
moves away from that preferred one.
If you count up the spikes in a time
bend of say, 100 milliseconds, and pluck
those number
of spikes as a function of the
orientation, you'll
get a curve like this, which looks like a
Gaussian.
Here's another example, this time from the
motor cortex.
Now, as the animal in these experiments, a
monkey,
makes an arm movement in a certain angle
relative
to his body the firing rate of this neuron
is large for some angles and small for
other.
And this time the plot of firing rate
versus movement direction is more like a
cosine curve.
It turns out the neurons in primary visual
cortex are sensetive to many input
features and
often the sensitivity to those features is
distributed
in an orderly way, across the cortical
sheet.
So what you're seeing here is an image of
a piece of cortex, a piece of visual
cortex.
The color indicates the value of a
particular feature to
which the neuron or neurons in that
location respond most strongly.
This picture of response as a function
of location in cortex is called a
funcitonal map.
This is an example of some functional maps
in cat
and bush baby, and these classic what's
called pinwheel structures.
In this case, the neurons prefer an
angle encoded by color, changes
systematically in space.
The lower two panels show a map of
preferred spatial frequency
again in cat and bush baby that is the
width of lines
in a grating, to which these neurons have
a strong response.
Again, this shows an orderly pattern
across cortex.
FMRI studies show that there are localized
regions in the temporal
lobe that are responsive to semantic
categories, for example, faces or houses.
So the notion of a stimulus that drives a
neuron maximally can get quite complex.
And the idea of being able to draw a
tuning curve
as a function of some meaningful stimulus
parameter becomes quite difficult.
As we can see here in these famous
experiments.
So this group recorded from single
neurons, in the parahypocampal area in
humans.
This is normally not something one can do,
but here the experiment
has worked with patients who are
undergoing surgery for epilepsy and had
agreed to participate in experiments.
What you're seeing here is the response of
a particular neuron to different images,
where here the number of the image is not
arranged in any particular order.
It is on the x-axis.
So, clearly there are few images that
drive this neuron particularly well.
If we now look at the images that
correspond to those large responses.
It was found that these have a common
property, they all turn out
to be images of Brad Pitt and Jennifer
Aniston.
So interestingly this neuron did not fire
to images of either of
them separately, so see for example this
response of Jennifer Aniston by herself.
Well, its true that because these
experiments were done a long time
ago it's likely that not too many people
have this neuron type anymore.
Maybe some of you, who, unlike me, refuse
to read magazine covers
in the checkout line at the supermarket
are a little bit younger,
some of you may never have had a neuron of
this type.
So here's another intriguing example.
Again, some particularly large responses
to some subset of those images.
Now when we look at which images those
were, we see
that they all have a common property, in
this case, Pamela Anderson.
But interestingly, they're not just
photographs of Pamela Anderson,
here and here, but also drawings of Pamela
Anderson.
And although
you probably can't see it on your screen,
there's also a
case of a picture of Pamela Anderson's
name written in text.
So in later experiments, this group has
also showed that
such neurons also respond to audio clips
of that person's name.
So we can think about neurons like this as
embodying a concept.
So what's emerging from all this, is a
pictgure
of brain regions having increasing
complexity of stimulus representations,
starting from more geometric and becoming
more semantic, here
down in the retina and the thalamus,
lateral geniculate nucleus.
We see very simple forms of receptive
fields.
As we go to V1, we see oriented edges.
As we move up to V4, we see conjunctions
of edges that form contours.
And higher up into the, into the brain
where we see semantic categories such as
houses and
maybe specific houses, such as The White
House.
It's tempting to think about this as a
progression of
features being agglomerated into more and
more specific and complex features.
This also leads to increasing in variance.
Higher order areas are less sensitive to
details,
such as color or location in the visual
field.
So this idea of hierarchical features
being assembled in a feed
forward way is the basis for many powerful
and important models,
including ones that are enjoying a lot of
success in Machine Vision.
What makes the true computation very
interesting
is that these regions are massively
interconnected.
For example, while the thalamus generally
is thought of as a relay
station that takes in information as
represented in sensory receptors, like the
retina, and distributes it to various
other regions of the brain, in
fact it receives a massive amount of
feedback from all of these areas.
And this suggests that these
representations also feed back to
control what information is coming through
in the first place.
So this is how semantics, the meaning or
the value of the context
of an image, can end up influencing its
initial representation in V1 or V4.
So here where there's a role for learning
and for expectation.
What you think you're looking at can shape
what you actually see.
Here's maybe a
nice example of such top down effects.
You might find it hard, at first, to
figure out what this is a picture of.
Once you've seen it, whenever you see this
picture again, which will be regularly,
if you go into the field of
visual neuroscience, you'll always see it
instantly.
And while the role of these top-down
effects is
very interesting and quite an open area
that we've
love to talk about more in this course,
I'm
primarily going to stick with these lower
level, geometric representations.
While later in the course, Roger will tell
you more about how networks
can learn to maintain memories, which
presumably
form the basis of these semantic
expectations.
In the next section, we're going to talk
about how we go about constructing these
response models.
Let's take a break and be back for the
next section.

Welcome back.
We left off about to launch into the
construction of neural response models.
In this next couple of sections, we'll be
looking at models of
increasing complexity that incorporate
more and
more features of realistic neural
responses.
The main work that we'll be doing centers
on methods for finding out what components
of
a stimulus a neural system responds to and
the response function that links stimulus
to response.
For now, we're going to take the response
to mean a single spike
produced by a chosen neuron.
Further, we'll consider how the
probability of
seeing a single spike at a certain time,
or the firing rate, which we'll denote by
r of t, depends on the stimulus.
We'll talk more later, about the model for
how the
probability of seeing that spike,
explicitly depends on this firing rate.
But, for now let's move to a description
in which we try
to determine the rate r(t) as a function
of the input s.
So, what's the simplest, possible
relationship that one could imagine?
That the response at each time, is
linearly dependent on the stimulus, at
that time.
Or perhaps, at some small time in the
past.
That's a t minus tao.
If this were the case, then if we have
some
stimulus which is varying in time like
this, our response, r
(t), is going to look just like that
stimulus that's
scaled by some factor, perhaps delayed by
a little bit and
in the appropriate units.
So what could go wrong with a picture like
this?
In general we expect that the response
depends not just on the stimulus
at some particular time in the past but on
some combination of recent inputs.
If the response depends on some weighted
sum of previous inputs like this we
can still express that as a linear system
but in this more general sense.
So what we're seeing here is that by
starting
at some time, t, we step back over a range
of time points in the past here indexed by
k.
And at each of these time points, we
weight or
multiply, just like our firing factor in
the previous slide.
We weight the stimulus at that particular
time by some factor,
but now the factor depends on how far back
you're looking.
The weight factors are given by these f
(k), which we can write as a function,
f, here defined relative to this index k,
which is stepping backward.
So we take the kth element of our stimulus
weighted by the value of f(k), multiply
the two together.
Move along to the next step.
Do the same thing.
Add them together.
We can also write this in integral form,
where now the variable tau is a dummy
variable
that acts llike the index K.
Some of you might recognize what we've
written here as a convolution.
The set of weights f are equivalent to a
linear filter on the input.
Note the effects of temporal filtering
here.
Although this is just a cartoon, the fast
variations
in S can get smoothed out in producing R.
Let's think about some examples.
What if the response is proportional to a
running average of the input?
What would f look like in that case?
In this case what we expect is that our
response at some given time has to be
weighted by some linear function and
what's that
going to look like as a function of time.
Let's say we're averaging over some window
with n time steps.
So now we'd like to take each of our
values of s at those n time steps.
And weighted by a factor of 1 over n.
That will give us an average over a window
of length n.
And so now our filter f is simply going to
look, look like that.
Where the height here is 1 over n.
See if you can figure out what a leaky
average would look like.
What if r depended on s, averaged over
time, but with a memory that fades in
time?
So time points near to time t would be
weighted with a large value.
But as one goes back from time t, those
white, those weight values would get
smaller and smaller.
So if they're exponentially
decreasing, that's something called a
leaky integrator.
Such a system sums it's inputs, but with
a strength that decays exponentially into
the past.
We'll see a physical implementation of
this later.
The neuronal cell membrane behaves in
exactly this way.
So that was a first pass at predicting how
neuronal
responses might depend on a stimulus that
changes in time.
What if the neuron is sensitive to
patterns of light in space?
We can apply the same idea.
In fact, you've already met this idea when
you
were introduced to receptive fields in the
first lecture.
You might recall from the previous lecture
that receptive fields
in the retina, have what's called a center
surround form.
And that V1 neurons
are sensitive to oriented edges.
Let's think more concretely what that
means.
So our goal here is to take what
we just learned about temporal filtering,
and apply it
to inputs s, which are now defined, rather
than being defined over time, they're
defined in space.
So we're thinking about spacial receptive
fields.
We're thinking about the response of a
neuron to some particular region of visual
space.
So lets say this is our, our visual scene,
we
have a neuron in the retina, its response
is centered
at some point, x, y, in that space.
If the response were truly linear, then,
neglecting
for now, any time variation, the response
of
a neuron who's receptive field is centered
at
that point, x, y, could be computed like
this.
We would take the stimulus which is now a
pattern of light in this two
dimensional space, and weight it at each
point in the scene by the neuron's
receptive
field f, now defined over space.
And add up all those values to get one
single number, the predicted response.
So here's our s centered around x and y.
We now vary this index, x prime and y
prime.
At each point, x prime y prime, we look up
our filter, f,
which is a function of x prime and y prime
of this relative location.
We multiply those two together and add
them up.
Now when a retinal ganglion cell is
responding to some image, its response
is determined by how similar the image is
to its receptive field, f.
What does similarity mean?
It's exactly what we just did, the
weighting of
the image by the filter defined by that
shape.
We can also write this again as a
convolution.
Now let's be a bit more precise about the
cartoon that we drew before for our
receptive field f.
We drew it here in 2D, where the shading
roughly represented the
value of f at each point in space, or in
relative space.
Here's a much better picture.
Now again defining the filter over
the relative co-ordinates, X-prime and
Y-prime,
lets draw the values or weights of f going
into this third dimension.
Now you can see that f has
large positive values here in the middle,
and negative values here in the surround.
So lets think about how a receptive field
like this interacts with an image.
This means, if there's a bright part of
the image, around zero.
So let's center this now, again, at some
particular point, x and y.
We plunk this receptive field over the
image at that point.
We take each of
the values in the image, in this
coordinate frame, x prime, y prime.
We multiply them by the values of f at
those points and we sum them together.
That means that if there's a bright part
of the image at zero, that's going
to drive the neuron positively because f
there is multiplied by that large bright
number.
That increases the response r.
But there, if there is a bright part of
the image that falls into the
surround, that's going to contribute a
large, that
large, bright number, multiplied by a
negative weight.
And that's going to decrease r.
This model for a receptive field is often
approximated as a difference of Gaussians.
A narrow positive Gaussian blob in the
middle.
That's the excitatory center, and
subtracted from it, a
broader and shallower Gaussian to capture
the, the suppressive surround.
The effect of such a differencing filter
is to detect local changes.
Such a filter will respond strongly when
there's, when
there's a bright patch near to a dark
patch.
This is not just some crazy thing that
biology does.
Such filters are ones we use all the time
in image processing.
Look at this example from the gimp
software suite.
Here's again that photo of the Taj Mahal.
And here is the same image in black and
white, when a similar filter has been
applied to it.
Every point in the image on the right is
the result of
a difference of gaussians filter applied
to the image on the left.
You should be able to see that the new
image consists only of edges because of
its differencing property.
This filter cancels out regions in the
image that have constant intensity and
has a large value only when there's a
change in light level or contrast.
So now we've seen how a linear filter can
extract
a spatial feature that explains the
responses of retinal visual neurons.
We started with cases where we just had
temporal dependance and thought about
how a time varying response might depend
on temporal variations in the stimulus.
Now let's put them together.
Because in general the sensory neurons
response will depend
both on time and on other properties of
the stimulus.
In the case of vision, the spatial
distribution
of light intensity.
So what this implies is that we need a
filter f that depends on space and on
time.
So it could be thought about as a little
3D movie.
Here, you're just seeing frames of the
receptive field f over different points in
time.
So now let's go back to the simple case of
pure time dependence.
Imagine, for example, we're driving our
retina
with a blank screen whose brightness is
varying, or that this might be an auditory
signal going into a cochlear neuron.
While the idea of linear filtering is very
powerful
and useful, it can't quite be the full
picture.
So can you see some shortcomings?
So for example, can firing rates be
negative?
Can they increase indefinitely as the
input increases?
Both of those are a possible result from a
linear filtering operation like this.
Solving these two problems can be achieved
with a clever and simple fix.
Without abandoning the power and beauty of
the
idea of linear filtering, we can ensure
that our
output does the right thing by imposing an
extra
step, a nonlinear function applied to the
filtered stimulus.
For example, the function shown above,
this guy here,
will have the effect that, I'll call this
g,
when the filtered stimulus which we
represent here as
s*f or s convolved with f is small or
negative.
Go down to here the firing rate goes to
zero.
When it's very large then the firing rate
will saturate to some fixed value.
This nonlinear function g could have other
behaviors as well.
So now that we have these basic components
of a coding model, this linear filter, and
a nonlinear, static non-linearity that
transforms the filtered
signal into a, into an estimated firing
rate.
Let's take a break and in the next
section, we'll move
onto finding the components of a model
like this from data.

In the last section we argued that a good
basic coding
model for many neural systems is a
combination of a linear filter,
or feature, that extracts some component
from the stimulus, and a
nonlinear input-output function that maps
the
filtered stimulus onto the firing rate.
Our goal in this section is to understand
how to find the components of such a
model.
You'll be doing this for yourself in the
homework.
We'll then go on to think about how to
modify
this model to incorporate other important
neuronal properties.
Let's step back to the original problem,
which is to build a model like this.
To build this general model our problem is
dimensionality.
Let's caste our minds back to the case of
the movie we showed the retina.
We can define a movie in terms of the
intensity of
three colors in every pixel in, say, a one
megapixel image.
And to capture any time dependence, we'll
also need to keep
enough frames of the movie to go back for
maybe a second.
So each example of a stimulus is given by
3,000 times
maybe 100 time points, or in the order of
300,000 values.
That's just one stimulus.
To sample the distribution of possible
stimuli, when each is
specified by hundreds of thousands of
values is just impossible.
It would be impossible to fill up that
response distribution,
even if our stimulus was just 100
dimensions.
The amount of data needed is unmanageable.
So we need a strategy to find a way to
pull out one or two or a
few meaningful components in that image,
so that we
have any hope of even computing this
response function.
So to proceed at all we need to find the
feature that drives the neuron.
To do this, we'll sample the responses of
this
system to many stimuli but not to build
the complete
model, just enough so we can learn what it
is that really drives the cell.
That will let us go from a model that
depends on arbitrary characteristics
of the input to the one that depends only
on the key characteristics.
So, we're going to start with a very high
dimensional description.
Let's say, a time bearing wave form or an
image.
And pick out a small set of relevant
dimensions, that's our goal.
So how do we how do we think
about our arbitrary stimulus as a high
dimensional vector?
So we start with our s(t).
What we're going to do is discretize it,
so we take, time t1.
We take
the value of the stimulus at that time,
we'll call it s1, time t2, and we, we take
the value of the stimulus at that time,
and
we plot these two points in this
2-dimensional space.
As we keep taking more and more time
points, that gives us more
and more axes in this diagram in which
we're now plotting that stimulus.
So this is s(t), plotted as
the components of its representation at
these different time points.
So now we want to sample the responses of
the system to a variety
of stimuli so we can characterize what
it is about the input that triggers
responses.
One common and useful method to use is
Gaussian white noise.
Gaussian white noise is a randomly varying
input, which is generated
by choosing a new Gaussian random number
at each time step.
In practice, the time step sets a
cut-off on the highest frequency that's
represented
in the signal.
White noise, therefore, contains a very
broad spectrum of
frequencies, and in fact, depending on how
the noise
is smoothed in practical applications,
almost all frequencies that
are there are present in the signals with
equal power.
Here's an example of a white noise input
that's been smoothed a little bit.
You'll be using an example of white noise
in your problem sets.
Now each chunk of white noise, let's say a
hundred
time units long, can be plotted in a
hundred dimensional space.
The axes I've drawn here might describe
the value
at time t1, the value at time t2, et
cetera.
As we continue to stimulate with new
examples of white
noise, the different examples are plotted
in these different points.
And they start to fill up a distribution.
Remember, because each one of these
examples
is chosen randomly.
The prior distribution is the distribution
of stimulus
points, independent of what the neural
system is doing.
Because we've constructed our white noise
from Gaussian random numbers, the
distribution of the, of the stimulus along
any axis, is Gaussian.
If we were to take this multidimensional
distribution and project is onto just
this one axis, we would find that all of
those stimuli fill up a one dimensional
Gaussian distribution.
Now, a multidimensional distribution
that's Gaussian in
all directions is called a multivariate
Gaussian.
The beauty of such a distribution is that
it's Gaussian no matter how you look at
it.
If we chose to look at the distribution of
stimuli projected across
some other dimension that's not in our
original time points, but maybe
some linear combination of them.
Let's take a new vector and now project
our stimuli onto that new vector.
We would find that even along that
new vector, the distribution is again
Gaussian.
Now let's take a look at the stimuli that
trigger spikes to happen.
Here's one and let's say there's a bunch
more.
You'll notice that there's some structure
in this group of points.
Ordinarily if I were really plotting an
arbitrary
choice of three of the hundred possible
dimensions.
I wouldn't be able to see this.
I need to search for the right way to
rotate
this hundred dimensional cloud, so that I
can see that structure.
One way to find a good coordinate axis is
to take the average of these points.
Then the vector defined by that average,
the spike-triggered average in
general, is indeed a good direction in
which to see structure.
So let's imagine we now take this vector
through the data.
And let's project all these
spike-triggering points onto that vector.
They're all going to have projections that
are large and
similar to one another, so this will be
the distribution of points projected onto
the spike-triggered average.
So while I wanted to give you a
geometrical perspective
of what you're doing that might seem a
little abstract.
Operationally it's quite straightforward
and intuitive.
Let's say you gave this system a long
random white noise stimulus like
this one, which is just a scaler quantity
that's varying randomly in time.
And the system, this neuron spiked during
this presentation several times.
Here's a spike, here's another spike,
here's another spike.
For every time there's a spike, we look
back in time,
at the chunk of stimulus preceding that
spike, and grab it.
Put it down in this list.
This will be one example of your
spike-triggering stimulus set.
We repeat that for every spike in our
data, and then
the spike-triggered average is just the
average of all of these examples.
That's drawn over here.
So what you're doing
is approximating whatever is common to all
of the stimuli that triggered a spike.
So if all goes well, you'll see that
this average is much less noisy than the
examples.
And it's generally quite sensible looking.
So what this system apparently likes to
see is an
input that generally ramps up a bit, and
then goes down.
That is the feature that triggers this
system to fire.
Here's an example of the same procedure,
but when the stimulus
is not just a scalar value, but more like
an image.
Every column here, is an image, with
pixels of different colors,
maybe one that's been unwrapped into a
single vector of values.
The spike-triggered average, now average
over these chunks of spatiotemporal
data, that precede every spike, now has
both time,
a time dimension, and also a space
dimension.
Now let's go back to only deal with time.
So back in the time representation that we
introduced before, our spike trigger to
averages some vector.
We'll take it to be a unit vector.
Let's call it f.
This is the object of our desire, the
single
feature that captures a relevent componant
of the stimulus.
Now, recall the previous section of the
lecture.
What do we do with this identified
feature?
We used it as a linear filter.
Linear filtering we said is the same as
convolution.
And it's also the same as projection.
Let's take some arbitrary stimulus s,
remember we can
represent it as a point in this high
dimensional space.
And if we filter it by
this spike-triggered average, that's the
same as
projecting it as a vector onto the
spiked-triggered average, which is also a
vector.
So what does that mean?
We have this vector
of our stimulus s.
To project it onto s, f, means that we
take its component that's aligned along
the
direction of f.
This is s.f.
So that filtering operation takes the
high-dimensional s
and extracts from it by projection only
this value.
Only its length
along the vector defined by f.
Okay.
Now we've seen that a good way to find a
feature that drives the neural
system is to stimulate with white noise
and
use reverse correlation to compute the
spike-triggered average.
This is a good approximation to our
feature, f1.
Now, how do we proceed to compute the
input/output
function of the system with respect to
this feature?
Remember that we're trying to find the
probability of
a spike, given the stimulus, but where the
stimulus, here,
is now replaced only by the component of
the
stimulus that's extracted by the linear
filter that we've identified.
We can find this relationship from
quantities we can measure in data
by rewriting it using an identity
about conditional distributions known as
Bayes' rule.
We can rewrite this
probility spike, given s1, in terms of the
probability of s1 given a spike.
Probability there's a spike divided by the
probability of s1.
Let's see what this means.
We have this now in terms of two
distributions, here the prior, again, now
the prior only with respect to that
one variable that we've extracted from the
stimulus.
And here what we called the spike
conditional, conditional
distribution.
We run a long stimulus and collect a bunch
of spikes.
We project the stimulus onto our feature,
f1, extracting component s1.
Here's s1 and here are the spikes.
We use this long stimulus run to make a
histogram of s1 here.
Now which for our white noise experiments
is just going to be Gaussian.
We then pick out the values
of s1 at the times of spikes.
Here they appear to occur when S1 is
particularly large, and we make a
distribution of those.
Hopefully, that distribution is different
from the prior.
We take their ratio, as we see here, and
scale it by the overall rate as
probability of spike.
So now we have a method to calculate
our nonlinear input/output function that's
associated with f1.
Let's look at a couple of examples.
Let's say that our neuron fires at random
times, so when we build a histogram of our
stimulus, that is the prior distribution,
and a histogram
of the special stimuli that trigger
spikes, we'll find
that those stimuli are actually not so
special since
the stimulus points associated with spike
times suggest a
random sampling from the Gaussian prior,
their distribution is
just the same as the distribution of the
prior.
This could mean either that the stimulus
had nothing to do with the firing of the
neuron in the first place or else that we
chose
the wrong component and we filtered out
whatever it was
about the stimilus that this neuron is
actually responding to.
So we get an input output crave which is
just
flat, has no variation in response as a
function of S1.
What we want to see is a nice
difference between the prior and the spike
conditional distribution,
which is going to result in an
input/output curve that has structure
that's interesting.
So here our input/output curve tells us
that
the neuron, as we saw previously, tends to
fire,
so it has, predicts a high firing rate,
when
the projections onto our, our identified
feature are large.
This is success.
So now let's go back to the basic coding
model that we developed and think about
what's missing here.
We managed to get our dimensionality all
the way down to 1.
Was that necessarily a good idea?
Let's relax that a bit.
And add back something potentially
important.
The possibility of sensitivity to multiple
features.
Now the need for this should be intuitive.
We base all our decisions on many input
features and here's one of the most
important ones.
Unless you have a brain full of Pamela
Anderson neurons, though
personally I think I only have a Pamela
Anderson neuron, neuron.
Generally we choose a partner or a friend
on the basis
of many characteristics, flexibility,
generosity,
the ability to cook, political affinity.
There are also many characteristics that
enter into the
description of a person that may not
matter to you
at all for their suitability as a friend,
maybe
their eye color or their height or their
typing speed.
These are all filtered out of this
specific decision.
We select some subset of relevant features
from the whole sea of possible
descriptors.
To express this in terms of the models
that we've
been looking at so far, what we mean is
that
now we want to consider that there's not
just one but
several filters, each selecting a
different component of the input.
The non-linear response function now
combines the responses
of those different components in maybe
non-trivial ways.
Let's take a simple auditory example.
Let's imagine we have a core detecting
neuron.
So f1, the first feature,
selects frequency one, f2 selects the
second frequency.
But only when both frequency one and
frequency two are present in
the input will we get a large firing rate,
given this nonlinearity.
One could imagine many other possible ways
of combining features in such a
nonlinearity.
Let's go back to our picture of the white
noise
experiment to think how we could find
these features in data.
So we saw that we could take the
average of the points and compute the
spike-triggered average.
But we can extract more information from
that cloud of points.
One could also, for example, compute the
next order moment, or its covariance.
To do this, we apply a method something
like principal component analysis, or PCA.
I realize that most of you probably aren't
familiar with this technique, and we don't
really have time here to build up the
tools that we need to derive it properly.
So I'll just describe a little bit about
what it does.
Its job is to find low dimensional
structure, in that cloud of points.
So PCA is a general, famous, and kind of
magical
tool for discovering low dimensional
structure in high dimensional data.
Here's an illustration of what it gives
you.
Lets say you have a cloud of data where
each data point
has an XYZ co-ordinate and we plot it in
this three dimensional space.
But
in fact, unbeknownst to us, all the data
actually lie on a two dimensional plane.
So if we run PCA on this data
we'll discover that there are two-so
called principle
components and these comopnents correspond
to an orthogonal
set of vectors that span that
two-dimensional cloud.
So this feat of discovery doesn't look
super-impressive when
all we're doing is reducing three
dimensions to two.
We could have just
rotated our axes around and noticed that.
But what if when we start, as when we do
generally, we have hundreds
of dimensions and that we're hoping that
our data has some low dimensional
structure?
We'll never find it by plotting one
coordinate against another.
As those dimensions that are important are
some,
un, unknown linear combination of the
original coordinates.
Here we had x, y, and z and our plane
is defined by some linear combination of
our original axes.
Generally the dimensions that pick out the
relevant structure in the data will be
some linear combination of our stimulus
coordinates
in their original basis, perhaps time or
space.
For those of you with some linear algebra,
PCA gives us a new basis set in which
to represent our data; a basis set that
generally is a lot smaller than our
original representation.
So we get a lot of compression.
And also, it's a basis set that's well
matched to our particular data set, unlike
a
standard basis set, like a Fourier basis,
for example.
So here's a fun example that uses PCA.
Although it takes a lot of pixels to make
a picture of
a face, it turns out that faces have a lot
of common structure.
And most faces can be pretty well
reconstructed from a small set, maybe
seven or eight of principle components,
computed from a big bunch of faces.
So these are called eigenfaces.
If we have a new face that we want to fit
with this faces, we can construct that as
a linear combination
of, of Fred, George, Bob, and Bill.
So if we can represent any new face in, in
terms of sums of these computed
eigenfaces, instead of the
intensity values of each pixel in the
image, we can
represent the face using seven or eight
numbers instead of hundreds.
Dimensionality reduction using PCA has a
lot
of practical uses in neuroscience
experiments too.
For example it can be used to sort out
spike wave forms
that were recorded on the same electrode
from two or more different neurons.
Let's say one neuron would give a spike
that
has a nice clean signal that looks like
this.
The other neuron would give a, a clean
signal
that has a different shape, maybe a little
broader.
Any particular recording is going to look
like a noisy example or, of
one of the other, one or the other of
these wave forms.
PCA can pick out two components that
capture
the largest amount of variance in the
data.
Now you project each noisy data point,
each
example of a recording onto these two
components.
Usually, this will keep the two components
that
span the wave forms of the two neuron
spikes.
All the components that get thrown away
are just noise.
You can then plot all of the different
data,
data points that were recorded, project it
onto those two
features, so now you're seeing every data
point projected
into the space defined by feature one and
feature two.
And in this new two-dimensional coordinate
frame, the wave forms from the two cells
are now clearly separable.
So let's go back to white noise and neural
coding.
Here's an example from neural coding where
PCA was used to
find multiple features and where that
turned out to be very helpful.
Here you're looking at a scatter plot of
all
the stimuli that drove a retinal ganglion
cell to fire.
Each stimulus, each blue dot was 100 time
steps of a white noise flicker.
So just a scalar that varied in time.
But now we've reduced each
one of those stimuli to, to a point in two
dimensions by projecting
it onto the two features that we found,
feature one and feature two.
For this particular retinal ganglion cell,
the spike-triggered average
was close to zero and this picture shows
you why.
When we look at the stimuli that trigger
spikes it turns out to be two
group stimuli that drove the neuron and
the
average of the entire set is here
approximately.
It's near zero.
But if we just take the average of the
right group.
So we take this point in the middle.
We can actually look at what that point
looks like as a feature.
What we see, what we found, was that it
looks like, something like this.
This is a feature that likes to see the
light initially go down, and then go up.
This is what we might call, an on feature,
toward a brighter light.
If we now look at the average of all of
these stimuli, what
happens is that that turns out to be a
feature that is almost the same as the on
feature, except it turns out to be an off
feature.
So this neuron both likes it when the
light goes
on, and it likes it when the light goes
off.
If we average all of those stimuli
together, we'd get nothing.
But if we use this technique, where we now
could pull out two different
features, and plot our stimuli in that
two dimensional space, now that structure
is revealed.
It's important to realize that the two
features, f1 and f2, that
we found here are not themselves, the on
and the off feature,
but the analysis allowed us to find a
coordinate system in which we could see
that structure.
Okay.
I've been making a lot of use of your
linear algebra neurons.
Let's give them a bit of downtime with
the relaxing view of a, of a little
eigenpuppy.
Although we were not necessarily able to
go
into the details, I hope you got the
flavor
of the construction of these kinds of
models, and
a sense for why multidimensional models
can be useful.
There are a lot of good resources to learn
more
about these techniques, and we will post
them on the website.

We'll finish up this week's material by
considering a final couple of model
upgrades.
Let's go back and stare at this data
again.
There are a couple of issues that we
haven't yet addressed.
One is that we modeled only a time bearing
firing rate.
And of course this data is in the form of
spiked times.
In what sense the precise patterns of
these spike trains might be
meaningful is something that we'll return
to in a couple of weeks.
But in this section we'll address directly
the hidden assumptions
of models, like the ones we've been
developing, about the relationship between
that time varying firing rate, RFT, and
the currents of single spikes.
And we'll try to deal with the fact that
there does appear to be some
fine structure, here maybe, in the spike
trains that a smooth function RFT can
miss.
But first we'll talk about the fact that
this data was produced by showing the
retina a
natural movie and not white noise, which
was
the stimulus that we used in our previous
discussion.
In real life, neurons aren't living in a
world
of white noise and it turns out that the
statistics of this stimulus that you used
to sample
a model do affect that model that you
arrive at.
So we choose to use white noise rather
than
some more natural stimulus because no
matter how you filter
it, it's always Gaussian, which means that
there's no
special structure, no special directions
in the stimulus set itself.
Since it's already come up and it will be
coming up again let me just remind
you what a Gaussian function is.
So it's defined as follows.
Some coefficient multiplied by this
exponential factor,
which includes x minus some, some
parameter.
X not squared divided by 2 sigma squared.
So here, x not is the center of this
function.
And sigma is a measure of its width.
So for thinking about this function p of
x, as a Gaussian
probability, distribution over x.
Then x is mean, x bar, which is the mean
of x, is x naught.
And it's variance, defined as x minus
it's average, squared is equal to sigma
squared.
So that standard deviation is just the
square root of that, which is sigma.
Now, if you add together two or more
Gaussian random numbers, the new random
number also has a Gaussian distribution
and
that's just what you're doing by
filtering.
Taking linear combinations of the values
of
the white noise at different time points.
So with white noise, when we're using
geometrical
techniques like PCA, we're making sure
that we
have a stimulus that's as symmetric as
possible
with respect to those coordinate
transformations that filtering
give us.
There are no special stimulus dimensions
that are built
into the prior, into the, into the
stimulus ensemble itself.
Let's go back to the question that we
posed last time.
When have we found a good feature?
When have we identified a good, a good
filter, f?
We answered that by looking for the
response function with respect to that
stimulus component f, an input output
curve
that is interesting or has some structure.
So, recall, I showed you these, these two
cases.
In this one, the Gaussian prior here of
the distribution
of the filtered stimulus.
And the conditional distribution, so those
values of the filtered stimulus
that are conditional on, on the arrival
time of the spike.
In this case, those two distributions are
very similar, so when we take
their ratio to compute the input output
function, we get just a flat curve.
Now when those two distributions are, are
very distinct, when
they're very different, then their ratio
has some interesting structure.
So, instead of taking the average or doing
PCA
to find that filter, could we just go
directly to
these quantities, to these, to these
distributions, the prior and
the conditional distribution, and ask, can
I find an F?
A choice of F, that when I project the
stimulus onto it,
that the conditional distribution and the
prior are as different as possible.
So, what would it mean to be
as different as possible?
There's a standard measure that we use for
evaluating the
difference between two probability
distributions,
and that's called the Kullback-Leibler
divergence.
So here is the, the definition of the
Kullback- Leibler divergence, DKL.
So here is the divergence between two
distributions P of s and Q of s.
It's given by integrating over all the,
all the random variables.
So, in this case s, so we integrate
over s.
P of s, multiplied by the logarithm of the
ratio of those two distributions.
So what do we get if we use this DKL
between the prior and the spike
conditional distribution as a measure
of the success of the choice of f, and
just
try to find an f that maximized this this
quantity directly?
So this is the approach that was developed
by Tatyana Sharpee and Bo Bialek.
So now, I'm taking some arbitrary stimulus
distribution.
And here I've drawn it in a, you know,
pseudo high dimensional space.
P of s is the, is the distribution of all
possible stimuli.
We're going to take some filter, again a
vector, in this high dimensional space,
that's f1, and project all of the stimuli
onto it to compute the prior here in gray.
And now we'll project the spike-triggering
stimuli which
here, we pictured in, in yellow to compute
the spike-conditional distribution, here
in yellow.
And now, one can vary f around.
Right, so we can take different directions
of this f.
And repeat this procedure and compute the
DKL
between this prior and the spike
conditional distribution.
Here's another example of a different
choice of f, f2.
In that case our prior has a slightly
different shape because
the stimulus distribution has a different
shape in that direction and
the spike conditional distribution also
has a different shape.
You can see that these two distributions
are much more similar than these two are.
And so, we would prefer f1 as a better
choice of our filter than we would f2.
And so one can move around in this space
and keep evaluating these two
distributions, and look, search for an f
that maximizes the difference between
those two distributions.
Now, this turns out to be equivalent to
maximizing
the mutual information between the spike
and the stimulus.
So we're trying to find a stimulus
component that is as informative as
possible.
So observing a spike pins down our
estimate for the stimulus much better
for the f1 component, in this case, than
it does for the f2 component.
So notice that the stimulus here is no
longer Gaussian, we mentioned that.
It's no longer a nice, symmetric ball, and
I've draw it like that
because there's nothing about this
technique that
demands that our stimulus be white noise.
Since this is a stimulus with some
arbitrary distributions, you can see both
the prior
and the spike-conditional distributions
and varying with
the direction of f, but that is okay.
The fact that this method can be applied
to arbitrary inputs
means that this technique has been applied
to derived models using natural stimuli.
So one can then take, take this to the
next step and compute
the input-output function from the ratio
of the conditional distribution and the
prior.
So it's a powerful technique.
It generalizes to, to complex stimuli.
However, one of its downsides is that the
maximization
step is not guaranteed to converge to a
unique maximum.
That is,
it is a difficult optimization problem.
So to summarize, we saw how to build a
model
with a single filter, by taking the spike
triggered average.
We saw that we could generalize that to
multiple filters using PCA.
And, finally we introduced an information
theoretic
method that uses the whole distribution of
stimuli
to compute and optimal filter and this
light
less method removed the requirement for
Guassian stimuli.
So our next task,
as foreshadowed, is to deal with the issue
of the relationship between
our time varying r of t and the arrival
time of spikes.
So to go from r of t to spikes, the
assumption that we'll be making is that
every spike is generated independently
with the probability
that scaled by that time variant r of t.
What does this mean and how can we test
it?
Let's start from the most elementary
random process, the flip of a coin.
Says probability 1/2 of landing heads,
probability 1/2 of landing tails.
Now, let's take a biased coin, it only has
some small
probability, p, of landing heads up, and
that;s when the system spikes.
So now we can think of the arrival times
of spikes as,as obeying something as
simple as that.
We have some time, t.
We divide it into many time bends of size
delta t.
Let's say there's n of them, right, n is t
over delta t.
And that gives us a sequence of n time
bends and let's
assume that's the same probability, p, of
firing in each of those bends.
Now we'd like to know how many spike will
occur in the total time t?
This is, of course, a random number.
It will vary on every trial.
This random number has what's called a
binomial distribution.
Binomial meaning two value and those two
values have
the probability firing p and the
probability of not
firing one minus p.
So, what's the probability that we'll see
some particular
number of spikes, k spikes in those n time
bends.
How do we compute this?
All we need to do, is count, what's the
probability that there's a spike
at exactly k bends, it's the probability,
bend by bend, that a spike occurred.
So, we need probability to the power k.
And then the probability that a spike
didn't
occur in the remaining bends, so 1 minus
p.
How many bends did a spike not happen in,
that's n minus k.
And we don't really care which of the k
bends it occurred in,
so we need to count up the number of
different ways that we could arrange those
k spikes among the, among the n bends.
And that's a quantity often called
n choose k, and we can write that as n
factorial,
over k factorial n minus k factorial.
Where factorial,
let's give an example, three factorial is
three times two, times one.
So n factorial is n times n minus one,
times n minus two all the way down to one.
So write this here.
Now what's the average number of spikes?
That's just n times p, the number of bends
the number of
bends times the probability in a bend that
they'll be a spike.
What's the variance in the number of
spikes?
That turns out to be given by n p, 1 minus
P.
Now, in the limit that there are many time
bends and the probability of a spike
in any bend becomes very small, one can
show that the binomial distribution has a
limit.
That's the following form.
So we go from that distribution that we
just arrived,
in the limit of very small time bends and
now where
we set a parameter r, which is the
probability in
a time bend, divided by the size of the
time bend.
So the probability for a given time bend
is going to be
coming very small as the time bend size
becomes very size small.
So what we want to do is set some
parameter r, such
that that parameter stays finite as the
time bend gets very small.
And so that's the rate or probability per
unit of time.
So now that becomes our parameter in this
distribution.
So one can start with that previous
distribution of the binomial distribution,
do some calculations and end up with with
an expression like this.
Some of you might like to try that for
yourself or perhaps look it up on, on
Wikipedia.
This new distribution is called the
Poisson Distribution.
I've sub scripted it now, not by the
number of bends but by the total
time, t, as we again assumed that we've
taken limit where delta t becomes very
small.
So what are the properties
of the Poisson distribution?
It has a mean of r times t, which
hopefully feels intuitive.
The number of spikes is the rate times the
total time, slightly less intuitive.
So it has a variance that's given by r
times t.
So you might notice that that's the same
as the mean.
That is a very unusual propedate, and
because of that, a quantity called the
Fano factor, which is the ratio of the
mean to the variance, has become
a way to test whether a distribution is
Poisson or not.
If it has a value of one, then it's
Poisson.
Finally false spikes have been generated
through a Poisson process,
which fundamentally expresses the idea we
started from, which is
that they're generated in every time bin,
delta t, as
though they were independent with the
probability r times delta t.
Then they'll also have the property that
the
intervals between successive spikes has an
exponential distribution.
You can gets some intuition for why this
is by considering this distribution
above but, evaluated just for one spike as
a function now of the time, t.
You'll see the appearance of the
exponential, and the factorial goes away.
So comparing between them, the interval
distribution
doesn't have this factor t out the
front because it has to be normalized
over all time while the expression above
doesn't.
Now, the probability of seeing 5 spikes in
a
chunk of time, t, depends on the firing
rate in
this way, this is the Poisson
distribution.
So these are two strong characteristics of
a Poisson distribution.
One, that the final factor is 1.
And, second that the interval distribution
should
look like an exponential distribution of
times.
So here are some examples of the Poisson
distribution
for a few different choices of the firing
rate.
For low firing rate, the distribution is
almost exponential, whereas as the rate
gets higher, the Poisson distribution
looks more and more Gaussian.
Now in general, the rate is varying as a
function of time.
So if we want to see if this idea is
reasonable by looking at data, we need to
allow r,
the rate, to vary in time.
Here is a data from a neuron in monkey MT
cortex, which is sensitive to motion.
The monkey is watching the variable
patterns drift across the screen
and we're going to look in more detail at
this experiment next week.
The same pattern is being shown over and
over again.
You see that as in the retina there is
an over all modulation in the firing rate
over time.
But if you zoom in here on a short
interval of time,
that's drawn up here, the spikes are very
variable.
Now if you split the data up into these
little windows of time and plot the main
number
of spikes in a time bend against the
variance
in that time bend what would you expect to
see?
In every bend, if the spikes of Poisson
but with
a different rate, you could plot the rate
against the variance.
What would you expect?
Remember that the slope of that plot would
be the Fano factor.
So it expected, if it were Poisson, to
have a constant slope of about one.
And in the data you see that, that is,
that is very close to being true.
Here is the line, the line of slope 1.
You see that the data is very close to
that.
So, even though the firing rate is
changing in
each short time chunk, the cell's response
looks Poisson.
Where does this kind of variability come
from?
It's likely that while the neuron is
receiving a mean input that's
proportional to the stimulus, it's also
receiving a barrage of background input.
Remember that a cortical neuron gets
inputs from around 10,000 other neurons.
If that input is balanced, that is, if it
varies around zero, to
be both positive and negative, it won't
add much to the average firing rate.
But it will jitter at the spikes.
For example, here's the behavior of a
neuron model, that's driven by white
noise.
It also looks very close to Poisson, in
the sense
that the interspike interval distribution
looks very close to exponential.
I've emphasized that by plotting the
number
of intervals in log, against the interval
itself.
Which, if it's an exponential
distribution, should look like
a straight line with a negative slope,
given by
the firing rate.
So the Poisson nature of firing and the
randomness that we need to build
into our response models takes care of
the effects of random unobserved
background noise.
Still, let's zoom in on these very short
intervals.
At short intervals, the distribution stops
looking exponential.
This is for the very good reason that
a neuron is unable to fire arbitrarily
rapidly.
There are bi-physical processes that
prevent a neuron from firing immediately
after
an action potential, and you see here
that's
caused a gap of maybe a minimum of
10 seconds, in this case, between
successive spikes.
So we're going to talk about those
processes in a few weeks from now.
So, we might want to improve our model yet
more, by taking these intrinsic
limitations in firing seriously.
This can be very helpful, as these
intrinsic processes going on inside
the neuron, might add quite a bit of
structure to the spike trains.
For example, there may be some resonance
such the neuron likes to
fire at a certain frequency independent of
the fluctuations of the stimulus.
So these intrinsic effects can be built
into coding models.
They're elaborations of the ones we've
been looking at called generalized linear
models.
Here the setup is very similar, the
stimulus comes at similar, the
stimulus comes in, is filtered through
some feature, processed through a
nonlinearity.
Here the nonlinearity is drawn as
exponential.
I'll talk about that in a minute.
And there's, then there's an explicit
spike generation step,
explicit Poisson spike generation step.
If generation of the random process
generates
a spike, then a so-called post-spike
filter,
drawn here, is injected back in to the
input that's going into the nonlinearity.
So, of example, if the system is
refractory what you'd want for
this waveform is that it would quickly
move you away from threshold and
hold you away from it for some time, so
you
want a big negative pulse that might decay
back over time.
So we might want to add in something like
this that decays back over time.
So that would draw the neuron away from,
from spiking with an
initial big dip and then relax back over,
over the refractory period.
The one that's drawn here, taken from
this, this
very nice paper, is a little bit more
sophisticated.
It first draws the neuron away from
spiking, with
a big initial dip, so it has the
refractory property
built in, but then it becomes positive,
which is going
to promote spiking at some time after the
previous spike.
So that could give a neuron that has a
slight tendency to fire periodically which
is very nice.
So the spiking probability is now
proportional to
an exponential of the filtered stimulus as
before,
plus the filtered spiking activity as
we've
drawn, as we've written out right here.
So why this exponential non-linearity?
In the models that I've shown before,
we've allowed the non-linearity to be
something
that we've computed directly from the data
whereas here it's fixed as a
non-linearity.
Liam Paninsky showed that by fixing the
non-linearity to be exponential, or to
be in the exponential family, you become
able to, you become able to find
all the parameters of this model, all the
values of
these filters using an optimization scheme
that's now globally converted.
So you've sacrificed some generality for a
model that's more complete in another way.
You get more power in that you can add
more, more filters and it's guaranteed to
be solved reliably and repeatably.
So if we're going on adding additional
factors to what can influence the spiking
probability, why, why stop at that?
As Emery Brown and colleagues pointed out,
one
can also include many other intrinsic and
extrinsic factors.
In this paper, the group included the
influence, not
only of refractory effects, but also of
the firing of
other neurons in the network and applied
this to
the type of data that you saw from the
retina.
So including both self firing,
the output of the neuron itself, and also
the effects of the
firing of other neurons, they allowed them
to predict the spike patterns.
So they got, they were able to captured
these detailed
spike interval patterns that we saw in the
retinal data,
but also the correlations between the
neurons and the network,
and they were able to do that with, with
amazing accuracy.
So, I'll finish up with another beautiful
idea from, from Emory Brown's group.
We can use this Poisson nature of firing
to test whether we
have captured everything that we can about
the inputs in our model.
Let's say we have a model like the GLM,
where
the output depends on many influences, on
the stimulus, on
the history of firing in the neuron that
recoding from
on the history of firing in, in other
neurons as well.
Then we can our
output spike intervals and scale them by
the firing rate that's predicted by the
model.
So we take these intervals times between
successive spikes, we scale them by the
firing rate that our model predicted given
all the interactions that, that we've
incorporated.
If this predicted rate does truly account
for all the influences on
the firing, even ones due to previous
spiking, then these new scaled
intervals should be distributed like a
pure Poisson process, with an
effective rate of one, that is as a single
clean exponential.
So this is called the Time-rescaling
theorem and it's used as a way to test
how well one has done in capturing all the
influences on spiking with ones models.
So, we've reached the end of this stretch.
We've looked at some classical, and some
more modern ways, of thinking
about what spikes represent and how one
can predict them from, from data.
I'd like to emphasize that some of these
models and methods
are a very powerful way of thinking about
the neural code.
But there is a lot that they ignore.
These models, in particular, give the
impression that
neurons represent a particular thing, and
that's it.
In fact,
neural responses are modulated by many
other influences, by
how the animal is using it's body to
deploy
it's senses, to what it expects to see in
the environment, by the context in which
the stimulus appears.
We'll have a look at one example of such
influences in the later lecture.
But you should also, always keep in mind
that while I'm trying
to give you an overview of current
approaches to understanding the brain.
And these methods have made huge progress
in allowing us to make sense
of a lot of data even if under rather
limited circumstances.
It's likely that some of these ideas might
be overturned completely with a much more
general approach.
So the field is still really wide open to
new ideas
and concepts that will provide a richer
and a more powerful understand.
So to wrap up, I know this week has
started
to exercise maybe some math muscles that
might be rusty.
So please refer to the supplementary
materials online to see if there's
anything that can help you, and do hit the
forums.
There are a lot of knowledgeable people
among you, and
it's great to see questions being answered
and discussions developing there.
And, of course, our team is standing by,
ready to pitch in and to help, as well.
For next week, I hope you'll join us again
as
we start to learn how to use decoding to
read minds.
Back next week.

[MUSIC]. 
Hello and welcome back to computational 
neuroscience. 
This is week three and we'll be 
discussing decoding. 
How well can we learn what the stimulus 
is by looking at neural responses. 
We'll be covering a few different 
approaches, starting with some very 
simple cases in which one has to decide 
between one of two choices. 
Given the output of a single neuron, and 
then to the case where one has a range of 
choices, and has a few neurons that might 
be taking a vote to what the stimulus is. 
To finally thinking about how do we 
decode in real time to try to construct 
the whole time varying complex input that 
the brain might be observing. 
Or even ultimately the imagery or plans 
that the brain is concocting on its own. 
So let's say you're walking in the park 
and you heard a rustle. 
The rustle could be a breeze or there 
could be a tiger or a rabid raccoon 
hidden there. 
You have to choose. 
Do you stay or do you go? 
What does that look like, mathematically? 
Well, let's say we can arrange all 
possible bush rustling sounds along some 
axis. 
Some axis S, these are our sounds. 
Some of them, clearly the breeze, but 
many of them lie somewhere in the middle. 
So on the basis of this evidence, on the 
basis of the, of the sound that you 
heard, how should you decide what to do? 
Now imagine that all you had to listen to 
was your neurons. 
Actually, that is the case, but what if 
you only had one neuron, or a small group 
of neurons? 
So that's the problem we'll be starting 
with today. 
Here's a classic experiment that set out 
to probe how noisy sensory information 
was represented by noisy sensory neurons. 
And how the animal's decision related to 
the neuronal representation. 
So here's the setup. 
A monkey would fixate on the center of a 
screen and watch a pattern of random dots 
move across a screen. 
The monkey's being trained that if the 
dots move, for example upward, he should 
move his eyes or make a saccade upward 
into a, into a location. 
And then, he'll get a reward whenever he 
moves his eyes eyes in the same direction 
as the dot pattern is moving. 
So here's the difficulty, the dot pattern 
is noisy, and sometimes it's rather hard 
to tell, which way they're going. 
Moreover the experimenters, they would 
have changed the difficulty of the task 
by making the dot pattern more noisy. 
They did that by varying the number of 
dots that were actually moving in the 
chosen direction. 
The rest are made to move randomly. 
So at one extreme you have a stimulus, 
like this one, for which the dots are all 
moving together. 
So no noise, that's 100 percent 
coherence. 
At the other extreme, all the dots are 
moving randomly. 
And in this case there's in fact no 
correct answer. 
They're neither moving upward or 
downward. 
So let's take a look at what the neuron 
tells us. 
This data is taken from a neuron in MT. 
A region in the monkey brain that is 
sensitive to visual motion. 
The experiment was repeated many times 
with different particular patterns of 
dots. 
And on every trial, the number of spikes 
that the neuron produced was counted. 
Experimenters then made these histograms 
of the results. 
In black, you see a histogram of the 
number of trials in which that number of 
spikes was counted from the neuron. 
And then, in white, you see the number of 
trials in which that number of spikes was 
generated from the neuron. 
And the monkey made a saccade in the 
other direction. 
So these would be say, for upward 
choices. 
And these would be for downward choices. 
So now, the experimenters changed the 
coherence and now what you see is that, 
as one might expect, these two 
distributions of upward versus downward 
choices are moving closer together. 
There's less visual information that 
discriminates between left and right. 
And correspondingly, the firing rates are 
more similar in response to those two 
different trials. 
If we look at another example that where 
the coherence is almost zero, the motion 
signal di, discriminating left from right 
is very small. 
Those two distributions are almost 
overlapping. 
And so given that one sees a firing rate, 
one response, one trial from, from this 
neuron when trying to make a decision. 
How should one decode that, that firing 
rate in order to get the best guess about 
whether the stimulus was moving upward or 
downward? 
So here's how the monkey does on the 
task. 
The fraction that he or she gets right is 
a function of the noise level, such as 
the coherence in the dot pattern. 
Those are the dots here in black. 
In open circles is how a single neuron 
does, and amazingly it's very similar. 
So how does one go from that distribution 
of firing rates that we saw in the last 
step, to this measure of performance. 
This requires a decoding step. 
So here, we have distributions of 
responses. 
So, let's take a cartoon of the data we 
just saw. 
This is as a function of r. 
The probability of a response, given that 
the stimulus was upward moving, we're 
show in red. 
The probability of the response given 
that was downward moving we'll show here 
in blue. 
And these are, the averages r minus, and 
r plus. 
Decoding means that we'd like a policy 
that tells us if we see some value of r. 
We can map the stimulus onto either an 
upper going or a downward going stimulus. 
So what should we do in this case? 
We'd like to map some range of r, to 
upper going stimuli, and that means 
setting a threshold. 
I'll let you guess where it should go. 
Hopefully you intuitively chose here. 
Why? 
This choice of threshold z, is the one 
that maximizes the probability that you 
get it right. 
With that threshold, how are you going to 
do? 
The probability of a false alarm of 
calling an upward when it was, in fact, 
downward, is going to be the area under 
this curve. 
These are all the cases where the, where 
the stimulus was, in fact, going, going 
downward. 
But the response was larger than our 
threshold z. 
So this is the probability of a response 
being greater than or equal to z when in 
fact the stimulus was going down. 
Whereas the probability that you got the 
call upward right is going to be the area 
under this cut off. 
These are all the cases where the 
stimulus was going upward, and you in 
fact answered upward, because r was 
greater than that threshold. 
So this is the probability of r being 
greater than or equal to z, given that 
the stimulus was in fact plus. 
So this choice of z maximizes the total 
probability of being correct. 
P correct, that is the probability that 
the stimulus was in fact output 
multiplied by the probability that you 
called the output. 
Probability that the response was greater 
than or equal to z, given it was going 
upward. 
Plus the probability that was in fact 
going downward, so that's now going to be 
1 minus probability of response being 
larger than or equal to z, given that it 
was minus. 
So the number of false alarms is this 
probability the number of good calls is 
this probability. 
And this choice of z maximizes the total 
probability correct. 
A conditional probabilities, p of r given 
minus and p of r given plus, are also 
known as the likelihood. 
They measure how likely are, we are to 
observe our data, r. 
A firing rate given the cause, the 
stimulus. 
So, notice that what we're doing by 
choosing z where it is, we're choosing 
the value of the stimulus for which the 
likelihood is largest of walking along 
these curves. 
And if this probability the response is, 
is downward is the larger. 
We'll map those values of our to minus. 
Once we've crossed over at this point, 
now the probability of response being 
positive is larger and we'll map all of 
these values to plus. 
Alternatively, we could think about this 
as putting a threshold on the likelihood 
ratio itself. 
So, what we are really doing is saying 
that we want the likelihood ratio. 
Probability of r given minus, to be 
greater than 1, whenever we choose plus. 
So it turns out that the likelihood ratio 
test is the most efficient statistic we 
can use to analyze our data, in that it 
has the most power for the given size. 
This is called the Neyman-Pearson lemma. 
The data showed that there's a close 
correspondence between the decoded neural 
response and the monkey's behavior 
itself. 
Which of course raises the question, why 
then do we need all these neurons, 
especially in cortex, when many of them 
seem to be doing approximately the same 
thing? 
I think that's a, a good mystery for you 
to ponder and we'll hit on one of the 
answers later on. 
Now let's say we're able to observe 
outputs from the unknown source for quite 
a while, so we should be able to use that 
extra information to set our confidence 
threshold quite high. 
Assuming that in every time band, in 
every moment of time, we're getting an 
approximately independent sample, we can 
now accumulate evidence in favor of one 
hypothesis over the other. 
So let's say we observe some particular, 
some particular noise, say here. 
So it's most likely, as we can see, to be 
due to the breeze. 
So what's our evidence in favor of a 
tiger? 
That's going to be the likelihood ratio, 
l of that observation s. 
Which is the probability of s given a 
tiger. 
Over probability of s given, that it was 
the breeze. 
So in this case, the likelihood ratio is 
going to be, going to be less than 1. 
So now, when we accumulate this evidence 
over time, every sample is independent, 
we're really multiplying these 
probabilities together. 
So instead, let's take the log and sum 
them. 
So let's start, we'll start here at zero. 
At time, time t equals 0. 
We don't have any evidence either way, 
and start to take observations. 
Now if the likelihood of the breeze is 
higher, that makes our sum go down. 
So now the, if the likelihood in this 
case was less than one, then the log of 
that likelihood is negative, less than 
zero, and so that's going to give us a 
negative flip in this sum. 
And of we get another observation that's 
also, has a negative look likelihood, but 
now we might hear. 
So a growly sound suddenly takes us in 
favor of a tiger but then no, it was just 
a rustle. 
And so similarly, we'll just keep taking 
observations until at some point we'll be 
completely confident given our sequence 
of observations that will hit that bound, 
that will hit a bound and say, at this 
point. 
For sure, given all of my observations 
I'm willing to say that that's the 
breeze. 
So here's some evidence for such a 
process taking place in the brain. 
In this task, the monkeys are doing 
almost the same task that we saw earlier. 
They're viewing a pattern here. 
So they fixate and they start to view a 
pattern of moving dots. 
And they have to indicate which direction 
the dots are moving in. 
Here, the directions are left and right. 
What's different about this task is that 
the monkeys can respond whenever they 
want. 
They are under some time pressure to 
respond quickly because they get a reward 
when they answer, and if they take too 
long, they take a timeout when they can't 
get any juice for a while. 
So now the recordings, in this case, were 
made not from NT but from area lateral 
intraparietal cortex or LIP. 
This area is part of the circuitry for 
planning and executing eye movements, and 
now when a neuron was found, the region 
in space to which it was sensitive was 
located. 
And that was chosen as the place to which 
the monkey had to move his eyes, or 
saccade, to show that he'd understood 
which direction the dots were moving in. 
Now let's look at the neurons firing 
rate, aligned to the onset of the trial. 
The firing rate gradually ramps up 
through the course of the trial. 
The different colored curves correspond 
to experiments with different coherences 
or motion strengths. 
So which would you guess is the strongest 
motion strength? 
I'm sure you chose the dark brown one, 
and that's correct. 
It seems that when the evidence is strong 
the firing rate increases fastest. 
Suggesting that the firing rate 
represents that integrated evidence, for 
in this case rightward motion, when the 
evidence is weak, it accumulates more 
slowly. 
So now let's see the trials aligned at 
the time of the saccade. 
It's really cool to see that these firing 
rates seem to peak at a common point, and 
this has been interpreted as exactly that 
bound that we thought about before. 
That the firing rates ramp up until they 
reach some threshold of confidence at 
which point the monkey is willing to make 
a move to signal his decision. 
So let's go back to our single-neuron, 
single-trial readout case. 
We use the likelihood ratio to tell us 
what values of the sounds should be 
interpreted as tiger. 
But straight away, you've probably 
realized that this is not the smartest 
way to go. 
After all, the probability there actually 
is a tiger is very small. 
So, if we're thinking correctly, we 
should include in our criterion the fact 
that these distributions don't generally 
have the same weight. 
They should be scaled. 
Up and down by the factors. 
Probability of the breeze, and this one 
by the probability that there was in fact 
a tiger. 
That means that we need to take into 
account the role of priors, these prior 
probabilities that these, that these, 
stimuli were in fact present. 
Here's a very specific example where 
biology seems to build in that knowledge 
of the prior explicitly. 
This is work from the lab of Fred Rieke. 
He'll be presenting a guest lecture this 
week about this intriguing and beautiful 
result. 
But I'll summarize very briefly for you 
now with some cartoons. 
So rods in the retina, these cells that 
collect light. 
are capable of responding to the arrival 
of single photons, what you are seeing 
here is a current recorded from a photo 
receptor, and you can see these photons 
arrival events here as the large 
fluctuation in that current. 
You also see that there is a lot of 
background noise, here is a sample of 
noise we make a distribution of that 
noise, it has some width. 
If we now consider cases in which there 
was a photon arrival event, that has 
another distribution that's separated 
somewhat from the noise, but not entirely 
because the amplitude of this noise is 
quite large compared to the signal. 
So if you sit downstream from the, from 
the photoreceptor and want to know when 
one of these event occurred. 
How should you set a threshold so you can 
catch as many of these events as you can 
without being overwhelmed by the 
background noise? 
Our signal detection theory understanding 
suggests that we should put the threshold 
at this crossing point in the 
distributions. 
However, what does biology do? 
Biology that is in the form of the 
synapse that takes the signal from the 
photoreceptor to the bipolar cell. 
Instead, it sets the threshold way over 
here. 
So what's going on is that at this light 
level, these photon responses in any one 
photo receptor are very rare. 
So that most of the time the fluctuations 
are due to noise. 
If one takes that into account, that is 
the prior probability of signal and of 
noise, then the two distributions now 
look more like this. 
Now the crossing point is way over, and 
the response properties start to look a 
lot more sensible. 
This cover of Nate Silver's book neatly 
summarizes what's true for many important 
decisions. 
There's a small amount of signal in the 
weld, as in the case of the photo 
receptor current. 
An awful lot of noise, relative to any 
particular decision for the same reasons 
as we discussed in our last lecture. 
A given choice establishes a certain set 
of relevant stimulus aspects. 
And all other information which may be 
very useful information for other 
purposes. 
Becomes noise and deciding whether to 
invest energy in reacting your running 
away from the tiger, calling in the bomb 
squad to detonate a shopping bag, asking 
a girl for a date. 
The prior probability isn't the only 
factor when also my, might want to take 
into account the cost of acting or not 
acting. 
So, now, let's assume there is a cost or 
a penalty for getting it wrong. 
You get eaten, and the shopping bag 
explodes. 
And the cost for getting it wrong in the 
other direction. 
Your photo gets spoiled. 
You miss on meeting the love of your 
life. 
So, how do we additionally take these 
costs into account in our decision? 
Let's calculate the average cost for a 
mistake. 
Calling it plus when it is, in fact, 
minus we get a loss, which we'll call L 
minus penalty, penalty weight, and for 
the opposite mistake we get L plus. 
So, our goal is to cut our loses, and 
make the plus choice when the average 
loss for that choice is less than the 
other case. 
So we can write this as a balance of 
those average losses. 
The average or the expected loss for 
making the wrong decision for choosing 
minus when it's plus is, is this 
expression. 
The weight for making the wrong decision 
multiplied by the probability that that 
occurs. 
And now we can make the decision to 
answer plus. 
When the loss for making the plus choice 
is less than the choice for the minus 
choice, that is, when the average, when 
the average loss for that decision is 
less than the average loss in the other 
case. 
So now, let's use Bayes' rule to write 
these out. 
So we now have L plus times the 
probability of response given, minus 
times probability minus divided by 
probability of the response. 
We want that to be less than the opposite 
case. 
Probability of response given that it was 
in fact plus. 
There's that prior divided by probability 
of response. 
So now you can see that when we cancel 
out this common factor, the probability 
of response. 
I rearrange this, in terms of our 
likelihood ratio because now we have here 
the likelihood, probability of response 
given minus, on this side, the likelihood 
for the probability of response given 
plus. 
We can now pull those factors out as the 
likelihood ratio. 
And now we have a new criterion for our 
likelihood ratio test. 
Now one that takes these loss factors 
into account. 
That's where we're going to stop for this 
section. 
In the next few sections of this lecture, 
we'll be talking about methods for 
decoding from populations. 
The very next point that we'll pick up is 
the, is what's called the population 
vector, a way of allowing many neurons to 
vote for a given stimulus outcome. 

Hi, we're back and launching into 
population coding. 
Of course in general, many neurons 
participate in the making our decisions. 
And so we'll now consider some decoding 
algorithms, that allow for the influences 
of many neurons, in interpreting the 
stimulus. 
We're going to start with a simple and 
elegant example, the cricket cercal 
system. 
Crickets are very sensitive to air 
movements, and these signals help them to 
escape moving predators or fly swatters. 
They're able to sense these motions, due 
to two antenna-like structures, called 
the cerci, here on the back of the 
abdomen. 
These are covered with fine hairs. 
If you look at them in close up, you'll 
see these tiny fine hairs that cover 
their entire, cover their entire length. 
The cercus in this case, was about one 
centimeter in length. 
So the wind velocity is transduced into 
an electrical signal, by the movements of 
these hairs. 
Their innovated at their base, by neurons 
that are able to sense their mechanical 
forces, caused by their motion. 
So the question is, how do these neurons 
as a group communicate wind velocity to 
the animal, to aid in its escape? 
Here at the top, we're showing the firing 
rate response cause for different 
neurons. 
What's rather lovely, is that in among 
this group of neurons there are only four 
types of these tuning curves. 
Neurons have peak responses in one of 
four cardinal directions, and these 
happen to be those be those at 45 degrees 
to the animal, left and right, front and 
back. 
These curves like the ones we saw in the 
previous week's lecture in the motor 
system, are approximately shaped like 
cosines, so that these neurons respond 
proportionally to the cosine of the 
angle, between their preferred direction 
and the direction of the wind, S. 
Notice that for simplicity, we simply 
normalizing away the [UNKNOWN] response 
by its maximum. 
Now, a way to interpret a cosine response 
like this, is that the neuron's firing 
rate is proportional to the projection of 
the wind velocity. 
So, here's the true wind velocity, onto a 
vector defined by the preferred direction 
of the neuron. 
So here is the projection of the wind 
velocity in that, in the direction C. 
If we want to compute this projection, we 
can use the fact that the cosine of S 
equals r, over the length of v. 
Now, because we're only dealing with the 
direction of the wind and not of it's 
strength, this is simply one, and so now 
our response, is just cosine of s. 
So we can write, we can rewrite this 
response in terms of, this dot product, 
this projection of the wind velocity, 
onto the preferred direction of neuron A. 
Now we can take the responses of all the 
neurons, and compute something called the 
population vector. 
Each neuron adds in a component in its 
preferred direction, with a weight given 
by its firing rate, which approximates 
the projection of the wind direction, 
onto that preferred direction. 
Now, comparing the estimate that we get 
from this population, population vector, 
we can see that there's a small, although 
systematic error in that estimate, of the 
entire 360 degree range of wind 
directions. 
Note that here, the number of basis 
factors used to define the wind velocity 
is four, since this system seems to 
encode in two dimensions. 
It seems like we have what's called, an 
over complete basis. 
In principle we only need two basis 
vectors, to represent velocity in two 
dimensions. 
So can you think of a reason why there 
might be four vectors? 
This method can also be applied to motor 
neurons in cortex. 
And in fact, this is still one of the 
leading methods for trying to decode 
motor cortical activity in order to 
control a brain computer interface, like 
a cursor on a screen or a robotic arm. 
Each response, each neuron's response is 
parameterized in terms of a tuning curve, 
like this one that we have already seen. 
So, around some, some baseline firing 
rate, r0, this is the form of a cosine. 
So here's that spike train data 
represented a bit differently. 
Now every time the r moves in a certain 
direction, that direction is plotted here 
for each neuron, with the length of the 
vector given by the firing rate on that 
trial. 
So once again, we're going to interpret 
the firing rate as the cosine of the 
angle between the arm movement and the 
neurons preferred direction. 
And once again, we can compute a 
population vector by weighting each 
neuron's preferred direction, CA, 
individual for every neuron, by its 
firing rate. 
So if we have enough neurons in our 
population, this average, this vector, 
this population vector average is going 
to converge and be parallel to the 
direction of arm movement. 
So, this is a neat technique, but you've 
probably got the sense that it's rather 
specific to this kind of response. 
What do we do with tuning curves, they're 
not cosine so they're not well 
interpreted as the projection onto a 
population vector? 
What if some neurons are better tuned 
than others? 
What about noise? 
Some neurons are more noisy than others. 
And we'd like to be able to weight the 
contributions from those different, from 
those different neurons appropriately. 
So, given the limitations of the 
population vector approach, we try to try 
something a little bit more general. 
We've seen Bayes' law a few times now and 
you'll be getting the correct sense that 
it's a very useful concept. 
So here we're going to use it for some 
definitions. 
So to remind you, here's the basic 
breakdown. 
The probability of a stimulus given a 
response, of course, can be written in 
this, in this alternate form. 
So now, we've seen this conditional 
distribution, probability of response 
given stimulus,very often, and we've, 
we've called that the likelihood function 
and worked with that the last few 
examples. 
P of s is the prior distribution, as 
we've also discussed. 
And the probability of r, it's just 
called the marginal distribution, and 
that's really just a normalization 
constant. 
So the probability of response, we can 
write as an integral over all the 
stimuli, probability of response, given 
the stimuli, times the probability of the 
stimulus. 
So here on the top you see an example of 
that term, that's in the integral, for 
one particular stimulus. 
And we're just going to average that over 
all the stimuli. 
And so that gives us this globalization 
we require. 
And finally we have the a posteriori 
distribution. 
The distribution you get once you 
incorporate all these factors, that the 
conditional distribution and the prior. 
So the two distributions that we'll focus 
on are first the likelihood. 
That's the one we've already worked with, 
in signal detection theory. 
And second is the a posteriori 
distribution. 
Now we can define two important general 
decoding strategies which utilize these 
two distributions. 
The first is Maximum Likelihood up here, 
we first mentioned in the context of 
fitting coding models, we saw again in 
the decoding of two alternatives and now 
we're going to use a slightly more driven 
way. 
Your maximum likelihood, when searches 
for the special stimulus value, s star, 
that maximizes the likelihood in our 
alternative choice. 
That's how we mapped any value of the 
response onto a stimulus. 
We chose the stimulus value that had the 
larger likelihood. 
Alternitavely, we can look for a stimulus 
that maximizes the upper posteriori 
distribution. 
That's called maximum a posteriori, or 
MAP. 
In general, these are going to differ 
because the role, because of the role of 
the prior. 
That's what makes the two sides differ. 
That means that in maximizing the a 
posteriori distribution, we're biasing 
our choice to what we know about the 
stimulus and events. 
We'll see an example of this later. 
So the plan for today is to work through 
a classic example of the applications of 
these techniques to a somewhat idealized, 
yet reasonable version of a population 
code. 
Let's imagine that we have a population 
of neurons that encode some particular 
stimulus parameter S. 
That's the orientation of an edge. 
Let's also take our inspiration from b1 
responses, and assume that the form of 
the response function is Gaussian. 
So let's say neuron one has a response 
that looks like this. 
There's a function of S, it has some 
preferred, preferred dir, orientation. 
So this, let's call this neuron one. 
So that has a, a preferred response at 
stimulus one and some firing curve F1 of 
S. 
And now imagine that we have many other 
neurons that have centres or orientations 
that are, that are along this line. 
And they all have their own Gaussian 
response functions. 
So we're going to make two particular 
assumptions, one is that each neuron 
fires independently. 
That is, even though they're driven by a 
common input. 
And some that have overlapping receptive 
fields will then necessarily fire at 
similar times. 
There's variability in their responses 
that's independent and private to each 
neuron. 
We'll also need an explicit model for 
that variability and that's going to give 
us an opportunity to make good use of 
what we learned last week about the 
Poisson Distribution. 
So here, here this is drawn more nicely, 
here's our stimilus axis, so it's a 
function of stimulus. 
Here's the response curve, f a of s for 
neuron a. 
And then as one scrolls along different 
neurons indexed by a, you see that that's 
going to span the whole, the whole 
possible range of s. 
And that we're going to define these 
tuning curves to be Gaussian. 
We're also going to assume that these 
receptor fields span the range of s 
uniformly. 
That is to say, no matter what value of s 
we take, there's always plenty of neurons 
that are encoding that region of stimulus 
space. 
And so what that's going to correspond to 
is that if we choose some, some value of 
s, and we look at the responses about 
population of neurons, there's a number 
of neurons that are going to be 
responding to that value. 
In this case, here, here, here, here, 
here, and here. 
If we add up the responses of all of 
those neurons to, to that stimulus, we're 
going to get some overall firing rate. 
And that firing rate is constant, 
independent of s. 
So, we're going to express that as you 
know, the sum of all the neurons in the 
population, take their average firing 
rate as a function of s, and that doesn't 
depend on s, so that we get the same 
average firing rate no matter what our 
stimulus value is. 
So, here's the response of 11 such 
neurons, each of which as a different 
preferred orientation. 
This is being driven by some stimulus. 
The referring rate of each neuron is 
plotted against that neuron's preferred 
orientation. 
The neuron in the middle responds 
maximally when driven with a, a value of 
the stimulus s equals, sa equals 0. 
While the neuron with a preferred 
orientation, say, of minus 5, is not 
driven at all. 
Now, you see that, for this simulated 
data. 
There is, like in real data, a lot of 
variability. 
If we did many trials of the presentation 
of this stimulus, and we plotted the same 
data, the average of all of the responses 
would lie among this dotted line. 
But there would be a standard deviation 
around the average, some error bars on 
this, is given by, do you remember? 
So what would it be for a, for a poisson 
firing neuron. 
So here's the expression for the 
probability of seen case fights in a 
chunk of time, of lengths T and let's do 
a quick review of what we learned last 
time about, about Poisson firing. 
Spikes are produced randomly and 
independently in each time band with a 
probability given by the instantaneous 
rate. 
And so what that implies is Is that if we 
have some length of time t we walk 
through, at least the steps of how one 
comes about calculating the probability 
of K spikes occurring in that, in that 
length of time T. 
Now let's see how we apply this 
expression in the particular case that we 
have set up. 
So let's start the figuring out each 
individual neuron's response. 
Let's say that neuron a has response ra 
to a given stimulus, to stimulus S. 
So we'd like to write down the 
probability of that response for this 
particular neuron for neuron labeled by A 
to that input stimulus. 
So what is that probability? 
Now, we're going to use this Poisson 
model. 
Let's see how we substitute the times in 
this Poisson model for times that, that 
we've developed here in our example. 
So now we want the probability given time 
t of k spikes. 
So, k spikes because we have a, we have 
firing rate, are a for our given neuron. 
So, r a probability of ra is going to be 
equal to. 
Now, this parameter r is the mean firing 
rate, so what's the mean firing rate of 
our neuron that's given by its response 
curve, and that was, we called that fa of 
s,s. 
And now in this case, we're giving it, 
we're giving the neuron stimulus s, and 
so now our mean response is going to be 
given by that tuning curve value, fa of s 
times t, now k again is the number of 
spikes, ra, that we actually observed, 
times t. 
Now exponential minus and now again we 
need this parameter r. 
That's a tuning curve value. 
Gives us the average response, T divided 
by, ra. 
Now, how do we right down the full 
distribution for all of the neurons at 
the same time. 
So, we're going to represent by vector r. 
The vector of responses to all, of all 
the different neurons rn. 
So, what is the probibility of all those 
responses. 
Because we said they're independent, 
that's just going to be the product of 
all of the individual probabilities. 
Probability of ra equals 1 to n. 
All right, so when we condition that on 
the stimulus, there's a probability r 
given a stimulus. 
In a product again over all the neurons. 
So now if we write in the explicit 
expressions for the individual neurons we 
get a complete expression for our 
population response that's of this form. 
So, now that we have our expression for 
the total stochastic response, let's 
calculate the maximum likelihood solution 
to do that. 
It's useful, as we've done before, to 
work with a log likelihood, rather than 
the likelihood directly, especially when 
we look at this form of this equation. 
So let's take the log. 
So log, probability of r given s is going 
to be equal to so now taking the log of 
that product is going to give us a sum. 
Over r a t comes down from the exponent 
times the log f a s T. 
Now this is going to come out from under 
the exponential, fa s of t minus log of 
ra with t factorial. 
Sum of that overall neurons. 
So now we need to take, now we need to 
find the s, value of s for which that's 
maximized. 
So how does one do that in general, if 
one has some function like this, f as a 
functoin of s? 
And we're looking for the s prime, which 
is that, which takes, for which s takes 
its maximum value. 
And what we do is take the derivative of 
f, with respect to s, and set that equal 
to 0. 
And then solve this equation for S. 
So now I would call this the calculate 
the derivative of, of this equation, 
before we do that we should notice that 
there are a couple of terms here that are 
not going to, not going to survive. 
So, this guy doesn't depend on s, so the 
derivative with respect to s is going to 
go away. 
We should also notice this time. 
So, what do we have here, we have the 
sum, over f a s times t over all the 
neurons. 
A equals 1 to n. 
Now, if you recall we required that this 
be a constant. 
This was a condition that we on our 
tuning curves that we had good coverage 
of the parameter, parameter s across the 
entire range so that the total firing 
rate for the leverage firing rate which 
is given by this term doesn't depend on 
s. 
And so now, when we take the derivative 
with respect to s, that term's going to 
go away. 
So, what do we have left? 
We now have T times sum over a, equals 1 
to N. 
Ra, now the log of some function of s, 
take its derivative with respect to s, 
we're going to get the derivative of that 
function, f prime times T over the 
function itself. 
So now we're going to get T sum over a ra 
f, prime of s, over f of s. 
And now we will set that to 0 and solve 
that for the value of s, that solves that 
equation. 
So now, here's the expression that we'd 
like to, we'd like to solve for, for s 
star. 
So, how do we do that? 
So, f as a function of s is equal to some 
prefactor A e to the minus 1 over 2 sigma 
squared, s minus Sa that central value, 
and now f prime of s equal to A. 
And now we bring down this factor. 
There's, there's a squared that cancels 
with the half. 
There's a minus sign that cancels with 
this minus sign so we get s minus Sa over 
sigma squared. 
Multiply the same thing we had before 1 
over 2 sigma squared S minus Sa squared. 
And so now this equation is quite simple, 
so we have a equals 1 to N ra. 
What do we have left? 
The f prime and the f are going to cancel 
out these exponential factors in the A. 
And so all we're left with is S minus Sa 
over sigma squared equals 0. 
So now one can solve that for s equals s 
star, and we're going to find the 
following solution. 
So s star is equal to ra times sa divided 
by sigma a squared. 
And that's divided by the total sum of 
the firing rates, where each is again 
normalized by its variance. 
Now if all the signals are the same, 
these factors cancel above and below. 
And we get back an expression which 
should look very familiar. 
Its very similar to the population 
vector. 
So what do we gain by all this procedure? 
Well, we made a lot of assumptions to 
make that work out so nicely, and the 
Gaussian distribution is one in which 
that came out so cleanly. 
If we go back, this one step, we see 
something that we would have hoped for. 
That each neuron's contribution is 
weighted by the inverse of it's variance. 
That is if a neuron has a sharply peaked 
tuning curve. 
If it's very informative about the 
stimulus parameter, it's variance will be 
small and it's contribution will be 
large. 
So this method takes care of weighting 
the various neuronal opinions according 
to their reliability and their 
informativeness. 
So, now let's calculate the maximum a 
posteriori solution. 
So, now we have to maximize, not the log 
of the of the likelihood, but the log of 
the a posteriori solution with respect to 
s. 
So, we can write that down. 
We can use Bayes law to just write that 
in terms of the likelihood. 
And in terms of the prior and also in 
terms of, of the marginal distribution. 
And now let's just consider those terms 
that that will depend on s. 
So, as before, we can throw away or, or 
leave out for now, any terms that, that 
don't depend on s, that only depend on r. 
So, that gets rid of this term. 
So now here's our log, of the, of the, 
likelihood that we wrote down before. 
And, the log of the prior. 
And we also have gotten rid of that term 
that was a sum over, over the fine rates, 
because we're again assuming that to be 
constant. 
And won't depend on the stimulus so it 
will dissappear in the derivative. 
So now taking the derivative of seven to 
0, throwing away the sum equals constant, 
we get a new expression that we now need 
to solve for, s prime. 
And you'll see that all we've done is add 
to that the, the this expression that 
depends on, on the prior probability. 
Using again the Gaussianity of the tuning 
curves, you can write down that, that 
solution. 
I'll leave that, I'll leave that as an 
exercise for you. 
And so what you find is that we have 
these original terms that we got from 
the, the likelihood, but now they're 
shifted. 
We now get a solution for s, for s star. 
That's shifted by the prior. 
So, what prior have we assumed here? 
We've taken a Gaussian prior, where the 
mean is less prior. 
And it has some standard deviation, sigma 
prior. 
And now, the effect of such a prior, is 
to add on this additional tone that's 
going to, that's going to bias our 
estimate of s star, and now you can see 
that as one changes the standard 
deviation, the width of this prior. 
So, let's take the case that sigma 
squared is very small, so that we have a 
prior that is very peaked around s prior. 
So that means that this term, this 
additional term is going to get very 
large and now s prior is going to have a 
stronger effect in biasing s star. 
Whereas if the prior is very broad 
equivalent to saying we don't have very 
strong information about exactly, exactly 
what value s takes on in general, then 
this time it's going to be weak. 
And in the limit of, of a constant prior, 
where sigma squared goes to infinity, 
this time would disappear altogether, and 
we'll get back to the maximum likelihood 
solution. 
So what are the limitations of these 
approaches? 
One is that it stays within the confines 
of the description in terms of tuning 
curve and mean firing rate. 
So, it doesn't really incorporate rapid 
time variations. 
It also doesn't take into account 
correlations in the population. 
What was very important to have been able 
to write down a full distribution of 
responses given the stimulus, is it we're 
able to assume that neurons respond 
independently. 
The question of correlations in the 
neural code is, as we've just seen, 
really fundamental to how we go about 
deciphering the firing rates, the firing 
responses of many neurons. 
And this is a major part of the agenda of 
neuroscience for the next decade, 
including with methods such as I'm 
showing here, with calcium imaging. 
Or we can, we can record from many, many 
neurons simultaneously. 
So it's not surprising that understanding 
correlations both experimentally and 
theoretically is a major current area of, 
of investigation. 
My friend, and University of Washington 
colleague, Eric Shae-Brown, will be 
joining us as a guest lecturer in the 
course, to talk about this issue in more 
depth. 
So we're going to stop here, for this 
session and come back to finish this 
lecture with a discussion of, of 
reconstruction. 
How one recreates estimates of time 
varying conflict stimuli from neural 
activity. 

We're going to finish up today's lecture 
with a discussion of reconstruction. 
I think we all share the dream that one 
day we might be able to record brain 
activity, during our sleep, and in the 
morning play back our dreams. 
So while the dream state is still not 
well understood, how close are we from 
being able to reconstruct even awake 
sensory experience from neural activity. 
We can apply the methods that we've 
discussed in the past two lectures to 
think about how to do that. 
So in the previous parts of this lecture, 
we talked about methods to find an 
estimate of the stimulus using Bayesian 
decoding. 
So now we'd like to extend our decoding 
procedures to the case of the responses 
and the stimuli. 
At varying continuously in time. 
Let's go through a simple example of a 
decoding strategy that meshes with the 
problem set that you have been working 
on. 
Some of you, anyways. 
Let's say that we wanted to find an 
estimator, S Bayes, that gives us the 
best possible estimate of our stimulus, 
S, given that we've observed some 
response, R. 
So, how should we compute s bayes? 
So one startegy that makes sense is to 
ask for an estimator that is only average 
as close as possible, to our stimulus. 
So I'm going to intriduce some error 
function which we'll call L and then 
minimize this error. 
Averaged over all possible stiulus 
choices that are consistent with our 
response R. 
So now, we need to choose a form for this 
error function, a very natural choice. 
Is the main square error. 
We'll take L to be just the mean squared 
difference between our estimator and the 
true stimulus squared. 
Now to derive an expression for S bayes 
that solves this problem, we need to 
minimize the average error. 
So remember how we minimize a function. 
We take the derivative of that function 
which respect to the parameter that we're 
interested in. 
So here as base and we set as equal to 0. 
So lets just do that calculation. 
So now we want to take d by ds, this call 
SB integral DS. 
Now let's substitute in our expression 
for our error squared. 
Probability of s given i. 
So now we take the derivative of that 
with respect to s b. 
So that's going to be equal to interval d 
s. 
The only time that depends on SB is this 
one, so the derivative of a square is 
just S minus SB times 2 times the 
probability of S given R. 
And now we set that equal to zero. 
So hopefully you can see that the 
solution is of this form. 
So how did we get that, let's just write 
that out. 
So we have integral DS, we can separate 
these two terms out and put them on two 
sides, so and S P of S given R, is going 
to be equal to integral ds, s Bayes, 
probability of s given r. 
And now, if we integrate the probability 
of s given r, over s, sB here is just a 
constant and will come out. 
Now, the integral over this probability 
distribution, since the probability 
distribution is normalized, it's just 
going to be equal to sB. 
And so here's our solution. 
So, we have sB is equal to this 
expression, which we already, already 
have here. 
Now, I want you to take a look at that 
for a moment and see if you recognize it. 
So, what if our response is just a single 
spike? 
So what does this expression amount to? 
Well it does the spike triggered average, 
right, it's the stimulus triggered by the 
response from the spikes. 
So we're going to take all the stimuli, 
wait them by this probability that they 
occurred in response to a spike and 
average over them all. 
So how do we apply this to reconstructing 
a simple stimulus? 
So imagine that this is our spike 
triggered average. 
So now every time there's a spike, so 
that our measured spike train, we're 
going to paste in the spike trigger that 
our, our conditional average. 
So at low firing rates, this is not 
looking very good. 
But at higher firing rates, so now you 
see that we're getting closer and closer 
to a, a smoothly bearing function. 
You might've realized already that there 
are some issues with a filter or feature 
of, of this exponential form as we drew 
before. 
Which is that it can never capture a 
negative fluctuation in the input. 
This is actually an issue with the fly 
neuron data that you've looked at in the 
problem set. 
The fly has two h1 neurons. 
One that encodes leftward motion and 
another that encodes rightward motion. 
So if you've tried to construct a, 
reconstruct a velocity stimulus with only 
one of your H one neurons, you'll only 
ever be able to recover either leftward 
or rightward motions. 
In the book Spikes which is a very nice 
exposition of this kind of 
reconstruction, at considerably more 
depth than I can give here, the authors 
actually simulate the other H1 neuron by 
playing the original stimulus, but with 
the opposite sign. 
And that now gives us enough information 
to reconstruct both positive and negative 
inputs. 
So now let's see this kind of decoding in 
action. 
The movie you're about to see is based on 
the activity of multiple neurons in the 
lateral denucleate nucleus of the cat. 
This is work by Yang Dan of Berkeley done 
about 15 years ago when she was a PhD 
student. 
By convolving the spike trains from 
multiple neurons in LGN with the spatial 
temporal receptive fields of those 
neurons Activity allows a noisy, but 
comprehensible reconstruction of the 
scene. 
So in this case, it's a cat being 
recorded now from, while, while 
anesthetized. 
But the LGN neurons are giving a pretty 
good reconstruction of what the cat is 
looking at. 
Hopefully you can see Yang's advisor 
looming into view. 
That's Joe Attic whose work applying 
information theory, to understand 
receptor field structure, is coming up 
next week. 
Now let's forward a few years to 2011. 
I'm going to finish up this week lecture 
with a rather impressive example of 
decoding, that starts to get us closer to 
that mind reading fantasy. 
It also neatly brings together the idea 
we covered last week and this week. 
In this set of experiments, Jack Gallant 
also at Berkeley and his colleagues 
recorded from visual cortex of humans 
viewing movies using FMRI. 
And used the recordings to reconstruct 
one second long movie sequences. 
So here you're seeing reconstructions of 
single scenes. 
But these are in fact stills from one 
second long movies. 
Nonetheless I hope you get a sense of how 
impressive these reconstructions are. 
So how did they do this? 
So here's the basic idea. 
Going back to this model that we've used 
over and over again. 
The researchers here are trying to find a 
movie clip s that maximizes this a 
posteriori distribution. 
So they use a library of 18 million clips 
and take the prior p of s to be uniform 
across those samples. 
So what's missing is the likelihood. 
To compute the likelihood of a given clip 
from the database, they develop an 
encoding model that they fitted from a 
different training set of movies, so that 
they can evaluate the predicted response 
for an arbitrary input. 
Then they can evaluate this likelihood 
measure by computing how well the 
predicted response to a movie from the 
library matches the true response. 
Let's take a peek at the encoding model, 
as it uses several of the ideas that we 
developed last week. 
So here's the model that predict 
responses. 
As you might recall, we mentioned last 
week that fMRI relies on blood 
oxygenation or, or BOLD signals, so it 
has a slower response time than in 
neuro-, than neural activity. 
So in this model the neural response 
despite is separated from the bold signal 
separately fit how is the neural response 
model. 
Let's zoom in, its predicted as in the 
models of last week by first filtering 
the input. 
Here's a couple of different filtering 
stages to filter the input to extract 
certain features. 
Let's focus on this part in which the 
image is filtered through a pair of 
oriented filters here. 
At different phases, just as we described 
last week for complex cell responses in 
V1. 
Now the outputs of those two filters are 
squared and summed. 
This means that one gets a large response 
independent of spatial phase, as we also 
mentioned last week. 
Then the output of that filtering stage 
Is passed through a compressive 
non-linearity. 
In this case the function is taken to be 
a log function. 
And then this is temporally down sampled. 
That is, it's smoothed from a 15 hertz 
signal to a one hertz signal in order to 
reduce noise. 
That's taken to be the predicted neural 
response. 
This newer response is then passed 
through an additional filter that 
accounts for the slow response of the, of 
the blood oxygenation level. 
So now here's the full procedure. 
An encoding model, like we just saw, is 
fitted for each voxel, each volume unit 
in the brain region being image/g. 
And then that model is used to predict 
the response to the millions of images in 
the database. 
The stimuli with the highest likelihood, 
which in this case, is equivalent to 
those with the highest poterior and those 
best account for the predicted respionse. 
So here are the predicted responses, in 
this column. 
The map solution, the most likely 
solution or the highest a posteriori 
solution would be to simply read off the 
maximum value. 
Because the clips are full of highly 
specific detail, one can in this case do 
a lot better by averaging those out. 
So what they're going to do is to rank 
these images by the degree to which their 
pricted, predicted responses fit the true 
response. 
And take the top sequence of images that 
have the highest match. 
So here they're drawing the top 30 
highest posterior clips. 
So the, the 30 clips that have the 
highest degree of match to the predicted 
response. 
So one could simply take this best value 
but because of all of that, because of 
all the specific detail in these sample 
images from there prior, one doesn't look 
better by combining them. 
So now if you look at the cumulative 
average of many of these high probability 
clips, then what you see is that as one 
gets To larger and larger numbers of 
them, you're getting quite a good match. 
So remember this is one of these 
examples. 
Perhaps this one. 
So in this case, you can see the effect 
of that averaging. 
So now you no longer see a crisp a crisp 
image that you would get from a single 
choice. 
From your prior distribution. 
Instead, you average over many of them. 
But now, what that does is to remove 
specific features, and give you a general 
gestalt that's much more similar to to 
the stimulus that was presented. 
So I hope this demonstrates that we are 
within reach of that dream. 
That we will be able to look at neural 
activity, and using clever models of the 
type that I showed you just now, we'll be 
able to reconstruct naturlistic images 
from that neural activity So that brings 
us to the end of my lecture for this 
week. 
You'll also find online a special guest 
lecture by my colleague Fred Rieke, a 
world-acknowledged wizard of retinal 
processing. 
Next week we'll be moving on to a 
consideration of information: how is 
information defined? 
What exactly does it quantify, and how 
can it be useful in neuroscience? 
I hope you've enjoyed this week and that 
we'll see you back next week. 

Welcome to our first guest lecture in 
Computational Neuroscience. 
We're honored to have with us today 
Federiki, my colleague in the department 
of physiology and biophysics at the 
University of Washington. 
Fred was an undergraduate and graduate 
student at the University of California 
in Berkeley. 
His graduate work [UNKNOWN] focused on 
building theoretical studies of signal 
processing in the nervous system. 
Fred then went on to a post-doc with Eric 
Schwartz at the University of Chicago, 
working on mechanisms of synoptic 
transmission in the retina. 
He then did a second post-doc with Dennis 
Baylor at Stanford, where he worked on 
light transduction in photo-receptors. 
So, Fred is truly remarkable in having 
made a transition from truly elegant 
theoretical work. 
Which led to the publication of what's 
been a highly influential book, Spikes, 
to truly elegant experimental work. 
The creativity and excellence of Fred's 
work has lead to his recognition as an 
investigator at the Howard Hughes Medical 
Institute. 
In his work, Fred combines a mastery of 
technique with a beautiful clarity of 
thought. 
And we're delighted to give you this 
opportunity to hear from him a little bit 
about his research. 
 >> Thanks for the introduction. 
My lecture today will be about vision and 
starlight and the mechanisms that let us 
see under these conditions. 
We know, from a long history of 
behavioral measurements, that our ability 
to see under these conditions is limited 
more by the division of light into 
discreet photons. 
The physical nature of light itself, than 
it is by biological noise and 
inefficiencies. 
As we'll see in a minute, that raises 
some general computational issues. 
First, let me introduce the retina 
itself, which is where the visual process 
begins. 
This first slide is an EM picture of a 
piece of frog retina. 
The photoreceptor outer segments where 
light is transduced or converted into an 
electrical response are here on top. 
These long cylinders are the rod 
photoreceptors which are exquisitely 
sensitive to dim light. 
In between, here, here, and here are the 
cone photo receptor which mediate vision 
up at higher light levels. 
Signals produced in rod and cone outer 
segments, are processed by several layers 
of cells within the retina before they're 
passed on to the optic nerve in the 
brain. 
And we know from more than century 
behavioral measurements that the darkened 
opted visual system and hence the retina 
itself. 
Can detect the absorption of just a few 
photons, spread across a large array of 
rod photo receptors. 
So a few photons coming and being 
absorbed within a pool of several hundred 
or up 1,000 rods, generate electrical 
signals that are not only reliably 
detected by the retina. 
But they are reliably sent to the brain 
where they can give rise to perception. 
This raises a general computational 
issue. 
Which is how to detect sparse signals in 
an array of noisy detectors? 
So, the situation we're thinking about is 
a few rods out a of a pool of a thousand 
absorb photons, all of the rods are 
generating noise. 
We want to know how to pool signals 
across those rods to reliably extract 
signals from those rods that absorbed 
photons. 
Those sparse signals can reject noise 
from the remaining rods. 
This is a situation where averaging, you 
normally think about averaging as being a 
good strategy to extract weak signals. 
Under these circumstances, averaging is a 
disaster. 
That's because the signal is not 
uniformly spread across the array of 
detectors. 
It's a little bit like a situation where 
you're in a football stadium. 
There's a 1000 people yelling of you, you 
care about a few of them. 
Under those circumstances, a good 
strategy for extracting signals from 
those few people you care about would not 
be to stick a microphone at the 50 yard 
line. 
And average across everybody, all those 
sources of sound in the stadium. 
Instead you would need to go seat by seat 
and make a selection, is this likely to 
be the person who I care about, based on 
some prior information about them. 
Say the, the people you care about gotta 
be a little bit louder than average. 
So you go seat by seat, make a selection 
about which people to retain, which 
people to reject and then average those 
resulting signals. 
So, looking for something analogous here. 
That is, we're looking for some kind of 
threshold, which selectively retains 
signals from those rather likely to be 
absorbing photons. 
And rejects noise from the remaining from 
the remaining rods. 
The behavioral consequences for getting 
this right are fairly extreme, that's 
because the signal is so sparse. 
If we can reject noise from the 99.9% of 
rods that are just generating noise and 
selectively retain signals from the on 
order of .1% of the rods, that absorb 
photons. 
We stand to win considerably. 
Okay. 
So, what we're looking for then is some 
kind of thresholding non linearity, which 
retains signals from those rods that 
absorb photons and rejects noise from the 
remaining rods. 
They mention this as a general issue, 
it's one that comes up in many other 
cases in the nervous system. 
Cases in which you have convergence of 
many inputs onto a downstream cell and a 
small subset of those inputs are active 
while all of the inputs are generating 
noise. 
There are a number of conceptual and 
technical advantages for studying this 
issue in the context of photon detection 
in the retina. 
One of those is that we have access to 
the signal and noise properties of the 
rod photo receptors, so we can measure 
the responses of the rods to single 
photons. 
We can measure the noise and the rod 
responses and we can summarize those by 
constructing distributions, that capture 
the probability that the rod generates a 
given amplitude response. 
We can plot that as probability versus 
amplitude. 
For those rods in black that failed to 
absorb a photon and are just generating 
noise. 
And those rods in red that absorbed a 
photon and are generating single photon 
response. 
Those give us the basis for making 
theoretical prediction about how to make 
a selection between signal and noise. 
Particularly, we might think that this 
threshold in non-linearity should come 
in, and slice out and eliminate responses 
from those rods that are generating 
noise. 
And retain responses from those rods that 
generate single photon responses. 
It's nice because we have a theoretical 
basis for what an appropriate readout 
might be, for the rod array under these 
conditions. 
We also know a great deal from the 
anatomy about where such a thresholding 
non-linearity might be implemented. 
In particular, the rod signals traverse 
the retina through a specialized circuit. 
The first cell in that circuit is known 
as a rod bipolar cell. 
And rod bipolar cells receive input from 
multiple rods. 
So, that means that they have already 
combined signals from multiple rods. 
If they do so in a linear fashion, in 
other words they equally weight inputs 
from rods that are generating noise and 
signal. 
You've already begun to average the rod 
responses and you've begun to 
inextricably mix signal and noise. 
So, the last opportunity, where we have 
access to the responsive individual rod 
photoreceptors. 
We have the full capability of making a 
selection of those rods that are 
absorbing photon, and generating single 
proton response. 
Versus signals or noise from those rods 
that fail to absorb a photon. 
Is here at the synap between the rods and 
the rod bipolar cell. 
So, anatomically we have a good 
prediction about where such a 
thresholding non-linearity might occur. 
It should occur at the synapse between 
rods and rod bipolar cells. 
Indeed if we record from rod bipolar 
cells, we see evidence for such a 
thresholding non-linearity. 
We can now take the measured distribution 
of rod signal and noise, and ask what the 
appropriate non-linearity is to predict 
the bipolar responses. 
I summarize that here. 
So, again these, this is plotting 
probability versus the amplitude, the 
distribution of rods that are generating 
noise responses. 
And the distribution of responses from 
rods that absorbed the protons. 
On the same scale, I plotted the 
estimated non-linearity at the synapse 
between the rods and the rod bipolar 
cells. 
That's here in blue. 
I plotted gain of the non-linearity 
versus the amplitude. 
So, you think of this non-linearity as 
everything to the left get's eliminated. 
So, all this noise and a good chunk of 
the single proton response distribution 
gets eliminated. 
And only those responses that are to the 
right of this transition from the 
non-linearity between the gain of 0 and 1 
are retrained. 
So, we see evidence for non linear 
threshold between rod and rod-bipolar 
cells, it's kind of what predicted. 
Interesting thing here is, that we not 
have predicted the location of this non 
linearity. 
In particular, it is located well up into 
the single photon response distribution. 
Naively we might look at these and say, 
well I should really put a line here, 
right at the crossing point between the 
noise distribution and the signal 
distribution. 
That would be choosing a location for 
this threshold in non-linearity, which 
makes a decision based on the likelihood 
of the given amplitude response. 
If the amplitude of the response is, is 
if a given amplitude response is more 
likely to have arisen from this 
distribution of single photon responses, 
we would retain it. 
If it's more likely to have risen, arisen 
from the noise distribution, we would 
eliminate it. 
Instead this thresholding non-linearity 
seems to be pushed off to the right. 
In other words, we're eliminating many 
single photon responses, which seems like 
exactly the opposite of what we'd like to 
do, to build a system that operates on 
low light levels. 
However, this particular way of plotting 
the data, is somewhat misleading. 
An what we've not accounted for here, is 
the prior probability that the rod 
absorbs a photon. 
You can think about that as there is some 
area under this curve, the noise curve 
which represents the likelihood if the 
rod is generating noise. 
There is some area here under the single 
proton response distribution curve, which 
is the likelihood of probability the rod 
absorbs a photo. 
These are roughly equal in area the way 
I've depicted them. 
But I told you that vision is working 
under condition where 99.9% of the rods 
are generating noise and only about 0.1% 
or less are generating single photo 
responses. 
So, we really want to scale these areas 
to represent that prior probability of 
absorbing a photon. 
In the next slide I've re-plotted these 
distributions to take into account those 
prior probability. 
So, here I"m not plotting log of the 
probability versus amplitude, and then 
again, the gain of our non-linearity 
versus the amplitude. 
These are exactly the same distribution 
that you say in the previous slide. 
So, the distribution of responses from 
those rods that are generating noise. 
The distribution of responses of those 
rods that absorbed a photon and are 
generating a single photon response. 
This is at light levels which are 
producing on average about one absorbed 
photons per rod. 
So, the area under this noise 
distribution is similar to the area under 
the distribution of signal proton 
responses. 
And again the [INAUDIBLE] thing is that 
the non-linearity that we estimate is 
occurring between the rod and rod-bipolar 
synapse. 
Here in blue, is shifted far to the right 
of the crossing point of the signal and 
noise distributions. 
We seem to be throwing away many too many 
single photon responses. 
However we really want to think about 
these distributions as what would happen 
near visual threshold, when something 
like one in ten thousand rods absorb a 
photon. 
And that's what I've plotted over here. 
So, now the area underneath the noise 
distribution and the signal distribution 
had been scaled to represent this prior 
probability that something like 1 in 
10,000 robs absorbs a photon. 
So, the area under the signal 
distribution here is 10,000 times smaller 
than the area under the noise 
distribution. 
That shifts this crossing point of the 
signal and noise distribution far out to 
the right. 
And it shifts it to a point that's very 
close to the location of the transition 
of the non-linearity between a gain of 0 
and a gain of 1. 
In other words, if you're simply applying 
a rule like maximum likelihood, you get a 
given amplitude response from the rod. 
And you're going to associate that with 
noise, if that amplitude is more likely 
to have originated from the noise 
distribution. 
You're going to associate it with signal 
if that amplitude is more likely to have 
originated from the signal distribution. 
That simple rule can predict the position 
of this nonlinearity and predict in 
particular that you should throw away 
many single photon responses. 
You should do that because the cost of 
accepting those amplitudes, down in here, 
is to allow lots of noise to come through 
the system. 
Much more noise than you would like to 
allow to come through. 
So, the basic bottom line here is the 
nice example in which the prior 
probability has an important impact on 
how we think about signal detection 
theory working. 
Now we think about appropriate strategies 
for extracting sparse signals from many 
noisy inputs. 
This is one of the many challenges that 
we face in understanding how vision works 
at low light levels. 
That includes issues about the 
photo-transduction process, how rods 
themselves generate responses to single 
photons. 
I've emphasized this role in synaptic 
transmission that is what the 
opportunities are. 
And some of the general computational 
challenges are facing the retinal 
circuits that read out the rod signals. 
And there are also challenges that we 
face, in neural coding and understanding 
how the responses generated by the 
absorption of a few photons. 
Produce a reliable change in the output 
of the retina that can be interpreted by 
the brain. 
Thank you. 

[MUSIC]. 
Hello and welcome back to week four of 
computational neuroscience. 
This week we will be talking about 
information theory. 
We'll be exploring information theory as 
a way to evaluate the coding properties 
of a neural system. 
So going back to thinking about spiking 
output as binary strings, of 0s and 1s. 
How good a code do these spike trains 
represent? 
We'll explore using information theory 
and related ideas as a way to understand 
how the coding properties, of our nervous 
system might be specially structured to 
accommodate the complex structure of the 
natural environment. 
So today we'll be addressing three 
things, we're going to start by talking 
about entropy and information, defining 
our terms. 
Then we're going to talk about how to 
compute information in neural spike 
trains, and then finally we will explore 
how information can tell us about coding. 
So, let's go back to our well worn 
paradigm, a monkey choosing right from 
left. 
And suppose we're watching the output of 
a neuron while different stimuli 
appearing on the a screen. 
Here's an example, spike train, is the 
time sequence in which we're marking 
spikes, in a given time bin, with a one, 
and a silence with nothing. 
Now, here's another example. 
And another. 
So, hopefully, when these oddball symbols 
appeared, either stimulus or spike, you 
felt a tiny bit of surprise. 
So, information quantifies that degree of 
surprise. 
Let's say there was some overall probably 
p, that there's a spike in sometime bin. 
And 1 minus P but there's silence, then 
the surprise for seeing a spike is 
defined as minus log 2, so log based 2, 
of that probably, that's the information 
that we get from seeing a spike. 
And the information that we get from 
seeing silence is minus log 2 of 1 minus 
p, of the probability of seeing the 
silence. 
So why does the information have this 
form? 
Like my husband and I some of you 
probably play squash. 
And if you do you'll know that what 
you're trying to do is put the ball 
somewhere that will surprise your 
partner. 
If you're a remarkable player you can put 
the ball anywhere in the court. 
If your partner has one bit of 
information, he knows which half of the 
court the ball is in. 
There was an equal probability of being 
in either, but once he gets that bit he 
knows which half. 
Each additional bit of information cuts 
the possibilities down, by an additional 
factor of two. 
So, what we're really doing, is 
multiplying the probability, the 
probability of being in this half, is p 
equal one half. 
The probability of being in the front 
half of the court, is an additional one 
half. 
Taking the negative log base 2 turns this 
into 1 plus 1 two bits to specify being 
in the front left corner. 
So now that we have a sense of that 
information we can understand entropy. 
Entropy is simply the average information 
of a random variable. 
So entropy measures variability. 
I'll warn you right now that in the 
future, I'll usually drop this, this base 
2 on the log, and just assume it. 
Entropies are always computed in log base 
2 and their units are in bits. 
An intuitive way to think about this, is 
that the entropy counts the number of yes 
no questions, as we saw in the case of 
the squash game, that it takes to specify 
a variable. 
So here's another example, let's say I 
drive down from Seattle to Malibu, and 
park in the valet parking. 
When I come back to get my car, the car 
park attendant is not very helpful and 
won't tell me where my car is. 
He'll only grunt for yes answers. 
So the car could be in any of these, say, 
eight spots. 
How many questions will it take before I 
can find it? 
So, let's say, is it on the left? 
Grunt. 
Is it on the top? 
Grunt. 
Is it on the top left? 
Grunt. 
So, what's the entropy of this 
distribution? 
Let's, let's calculate it. 
So, remember we defined the entropy. 
I'll call H. 
As the sum over the probabilities times 
log of the probability minus. 
So what is pi? 
In this case, the probability of being in 
any one of these locations is 1 8th. 
And that's the same for every location in 
this. 
In this, car park. 
And so now H is equal to minus 1/8, sum 
from i equals 1 to 8, log base 2 of 1/8. 
Now what is that? 
Remember that 8 equals 2 to the power of 
3, so the log base 2 of 8 is 3. 
So here we have, now, sum of 1 8th times 
minus 3. 
Now we add that up over the eight 
possibilities and we get 3. 
So as we saw, it took three questions to 
specify our car, and that's exactly the 
entropy of this distribution. 
So now let's go back to our coding 
sequences, here's a few different 
examples, so which of these do you think 
has the most intrinsic capability of 
encoding? 
Encoding relies on the ability to 
generate stimulus driven variations in 
the output. 
If an output has no variation, such as in 
this case. 
We're not very optimistic about its 
ability to encode inputs. 
So these three sequences differ in their 
variability. 
Which do you think has the most inherent 
coding capacity? 
So we can use the entropy to quantify 
that variability. 
So what does having a large entropy do 
for a code? 
It gives the most possibility for 
representing inputs. 
The more intrinsic variability there is, 
the more capacity that code has for 
representation. 
So in this simple case, we can compute 
the entropy as a function of the 
probability p. 
Where, again, the other, the other 
possibility has probability 1 minus p. 
So entropy, again, is going to be given 
by a minus p log p minus 1 minus p, log 1 
minus p. 
So now when one puts that function as a 
function of P of r plus, which here would 
call p, we find that there is a maximum. 
So, what's the value of P at which H has 
a maximum. 
That's the value at which. 
P equals one half. 
In that case, in this distribution, these 
two symbols are used equally often. 
So, this is a concept we'll come back to 
at the end of this lecture. 
Let's go back to squash. 
So, we had a possibility of the ball 
being anywhere in the field. 
Generally, you're not able to put the 
ball anywhere with equal probability. 
It's exactly this reduction in 
possibility that makes it even possible 
to play. 
You could model your opponent's 
probability of x, the probability of 
placing the ball somewhere in the court, 
and you can, to some extent, predict 
where the ball is. 
The lower the entropy of your partner's p 
of x, the more easily you'll defeat him. 
So, let's come back finally to our spike 
code. 
We now appreciate that the entropy tells 
us of the intrinsic variability of our 
outputs, by obviously we really need to 
consider the stimulus, and how it's 
driving those responses. 
So here's an example. 
The stimulus can take one of two 
directions, and each is perfectly encoded 
by either a spike or no spike. 
So here's the stimulus, here's the, the 
spiking response. 
Every time there's a rightward stimulus, 
we get a spike. 
So how about this case? 
We'd probably still be comfortable to say 
that the response is encoding the 
stimulus. 
These two are perfectly correlated. 
On the other hand, there are, there are 
several other events that, that are 
misfires. 
So in this case, the stimulus occurred 
with no spike, in this case there was a 
spike with no stimulus. 
But how about this? 
At least at a glance, there seems to be 
little or no relationship between the 
responses and the stimulus. 
So just as a side bar, what if the 
problem were not so much that our code 
were noisy, but that we haven't exactly 
understood what the code is doing. 
That is, maybe there's some temporal 
sequencing S that should be more 
appropriately thought about as the true 
stimulus. 
This is really the question that we were 
addressing in week two. 
How do we know what our stimulus was? 
But let's go back to the main question. 
What we really wanted to know is; how 
much of the variability that we see here 
in R is actually used for encoding S? 
We need to incorporate the possibility 
for error. 
So, let's do that by assuming now that 
was, when a spike is generally produced 
in response to stimulus plus. 
So here, there's also some possibility 
that there will be no spike, we'll 
quantify that using the error probability 
q. 
So probability of, of correct response in 
this case is 1 minus q, and the 
probability of a incorrect response is q. 
And let's assume the same error in this 
case for a silence response. 
So, now we would like to know, how much 
of the entropy of our responses is 
accounted for by noise, by these errors. 
Because that's going to reduce the 
responses capacity to encode S. 
The way we can address that is to compute 
how much of the response entropy can be 
assigned to the noise. 
That is if we can give a stimulus plus, a 
plus stimulus, a right way stimulus and 
get a variety of responses. 
Those conditional responses for a fixed S 
have some entropy of their own. 
Similarly when we give stimulus minus. 
So we call these stimulus driven 
entropies, the noise entropy. 
So this brings us to the definition of 
the mutual information, the amount of 
information that the response carries 
about the stimulus. 
This is given by the total entropy minus 
the average noise entropy. 
That is, the amount of entropy that the 
responses r have of some fixed s, 
averaged over s, and that's drawn out 
here. 
So, here's the total entropy of the 
responses, and here's the conditional 
entropy. 
So the, the entropy of the responses 
conditioned on a particular stimulus s, 
averaged over s. 
So now let's go back to our binomial 
calculations, and see how the mutual 
information depends on the noise. 
Now fixing p. 
We're going to take p to be the one that 
maximizes the entropy, so p equals one 
half. 
Let's vary the noise probability, and 
again assume that the noise is the same 
for spike and silence. 
That is, there is one value q. 
So this should be intuitive. 
When there's no noise entropy the 
information is just the entropy of the 
response, which in this case is one bit. 
As the error rate increases, as the error 
probability grows larger and larger. 
Spiking is less and less likely to 
actually represent the stimulus S, and 
the mutual information decreases. 
When the error probability reaches a 
half, that is, responses occur at chance, 
there's no mutual information between R 
and S. 
So, let's just check that everyone's 
still on board. 
More generally, what are the limits? 
So, if the response is unrelated to the 
stimulus, what is the probability of 
argument S? 
Its simply, the probability of the 
response. 
Because there's no relationship between 
response and stimulus. 
So the noise entropy is equal to the 
total entropy, and then the difference of 
response in noise entropy is zero. 
At the opposite extreme, the response is 
perfectly predicted by the stimulus. 
So in this case the noise entropy is 
zero. 
So the mutual information will be given 
by the total entropy of the response. 
All of the response's coding capacity is 
used in encoding the stimulus. 
So let's just see how that works for 
continuous variables. 
We've talked a lot about, about binary 
choices. 
Let's think more generally about cases 
where we have some continuous r, and, 
some response variability for the 
encoding of a stimulus s by r. 
So here's an example where we've given 
several different stimuli. 
Each of these distributions is the 
probability of the response given a 
particular trice of the stimulus. 
And now that's going to be weighted by 
the probability of that stimulus. 
And when we add all of these conditional 
distributions together, we get the full 
probability, P of r. 
Now, what we're doing by computing the 
entropy is we're going to compute the 
entropy of this blue distribution. 
That's going to be the, that's going to 
give us the total entropy. 
And then we're going to compute the 
entropy of these conditional 
distributions. 
And now we're going to average them over 
the stimulus that drove them. 
So for these two cases, they differ by 
the amount of intrinsic noise that each 
response has. 
So we give a stimulus s in this case, 
there's some range of variability that 
takes out some of my range of r. 
In this case, when we give that same 
stimulus, now the degree of noise 
stretches over a much wider range of the 
response distribution. 
So much more of the variability in R is 
accounted for by variability in responses 
to specific stimuli. 
And so, I hope you can see that this kind 
of response, set of response 
distributions, is going to encode much 
more information about S, that the 
information about S and R is much larger 
in this case. 
Then it is for this case. 
Let's play a little bit with these 
distributions, because I want to 
demonstrate a couple of things that I 
think really illustrate why information 
is useful and an intuitive measure of the 
relationship between two variables. 
I'm using capital letters to denote the 
random variable, and lower case letters 
to denote a specific sample from that 
random variable. 
So, what I'd like to show you is that the 
information quantifies how far from 
independent these two random variables R 
and S are. 
To demonstrate that, I'm going to use the 
KullbackLeibler divergence; a measure of 
similarity between probability 
distributions that we introduced earlier. 
It's the mutual information, measures 
independence, then we'd like to quantify 
the difference between the joint 
distribution, of R and S, and the 
distribution, these two variables would 
have if they were independent. 
That is, that that, that joint 
distribution would simply just be the 
product of their marginal distributions. 
So, first, to refresh your memory, D KL, 
let's redefine it. 
D KL, between two different probability 
distributions, say P and Q, is equal to 
an integral. 
Over probability of x times the log of P 
of x over Q of x. 
So now let's apply that to these two 
distributions. 
So, let's compute that, we have a 
integral over ds and over dr. 
Joint distribution, times the log the 
joint distribution divided by the 
marginal distributions. 
Now we can rewrite that, using the 
conditional distribution. 
In the following form, we can rewrite 
that as the probability of r, given s 
times the probability of s, that's just 
equivalent to the joint distribution, 
divided by P of r, P of s. 
And now, you can see that P of s cancels 
out, and we can rewrite this as, now the 
difference of those two distributions. 
So we'll just expand that log. 
All right. 
Now let's concentrate on this term. 
Going to be equal to the negative ds dr, 
probability of s and r, times the log of 
P of r, plus integral ds dr, P. 
Now, let's break that up into P of s, P 
of r given s. 
Just dividing up the, the joint 
distribution again into its conditional 
and marginal. 
Times the log of P of r given s. 
Now, let's look at the terms that we've 
developed here. 
We can see that we can just integrate 
over ds. 
We can integrate the s part out of this 
joint distribution. 
And this part is just simply going to be 
the entropy of P of r. 
Whereas this one is going to be the 
entropy of P of r given s, averaged over 
s, ds P of s. 
And so what I've shown you is that this 
form, in terms of the KullbackLeibler 
divergence, gives us back the form that 
we've already seen. 
The entropy of the responses minus the 
average, minus the average over, over the 
stimuli. 
Of the noise entropy, for a given 
stimulus. 
What I hope you realize is that 
everything we've done here in terms of 
response and stimulus we could simply 
flip, response and stimulus, redo the 
same calculation, and instead end up with 
entropy of the stimulus minus an average 
over the responses, of the entropy, of 
the stimulus given the response. 
So information is completely symmetric, 
in the two variables, being computed 
between. 
Mutual information between response to 
stimulus is the same as mutual 
information, between stimulus and 
response. 
So here's our grandma's famous mutual 
information recipe. 
What we're going to do to compute this 
mutual information, is to take a 
stimulus, s, repeat it many times, and 
that will give us the probability 
responses given s. 
We're going to compute the variability 
due to the noise. 
That is, we'll compute the noise entropy, 
of, of these responses. 
So, for a given value of s, we'll compute 
its, the entropy of the responses for 
that s, we'll repeat this for all s. 
And then, average over s. 
Finally, we'll compute the, probability 
of the responses, that'll just be given 
by the average over all the stimuli that 
we presented, times the probability of 
the response given the stimulus, and that 
will give us the total entropy of the 
responses. 
So, in the next section, we'll be 
applying that idea to calculating 
information in spike trains. 
There'll two methods that, that we work 
with. 
One will be starting by calculating 
information in spike patterns. 
And then we'll be calculating information 
in single spikes. 

Hello, again. 
So now were are moving on to calculating 
information in spike trains. 
And in this section of the lecture, we're 
going to be talking about two methods one 
of which is how to compute information in 
spike patterns. 
And the other is how to compute 
information in single spikes. 
So let's go back to our, our grandma's 
information recipe. 
So remember that we're calculating the 
mutual information, which is the 
difference between the total response 
entropy and the mean noise entropy. 
So, what was the strategy, we're going 
to, test strategy. 
We're going to take a single stimulus S, 
repeat it many times to obtain the 
probability of the responses given S, in 
that response distribution, via the noise 
entropy. 
We're going to repeat that for all s, and 
then average it over s. 
Finally, we'll compute the probability of 
response, and from that the total 
response entropy. 
So now, let's go ahead and compute 
information in spike patterns. 
So far we've really only dealt with 
single spikes or firing rates, so what 
we'd like to ask here is, what 
information is carried by patterns of 
spikes? 
By these interesting sequences of 0s and 
1s that occur here in the code. 
What this allows us to do is to analyze. 
Patterns of the code and to ask how 
informative they are. 
So the way we're going to turn out our 
spike train into, into a pattern code, is 
that we're going to chop up segments of 
these responses, so we take our voltage 
train when we divided into time bins Of 
size delta t. 
If there's a spike in, in that time, then 
we'll put a one. 
If there's no spike, we'll put a zero. 
And now we'll chunk up these zeros and 
ones into words of some length, big T. 
So now that we, we've defined these 
binary words. 
With the letter size delta t and length 
of T, we can now walk through our data. 
So, so, here's a raster plot produced by 
a stimulus that was randomly chosen on 
every trial. 
And so, if one converts such a raster 
plot into sequences of zeros and ones, 
you can look through that and pull out 
many, many examples of these words, again 
of length T and type in delta t. 
So now, one can form a distribution over 
these words. 
So here, the most common word was 
silence, there was no spike in this set 
of eight consecutive time bins, the next 
most common was that one spike appeared 
and of course, we can have that one 
appearing at different locations 
throughout the word. 
These are the next most common set of 
words. 
Then one starts to get combinations of 
spikes occurring at different locations 
throughout the word. 
So now we can walk through our data and 
calculate these probabilities and then 
calculate the entropy of that word 
distribution. 
Now the information, is the difference 
between, that entropy, and the 
variability, due to noise, averaged over 
stimuli. 
So here was our total entropy. 
Here's how we're going to compute our 
noise entropy. 
So, in this case, the same stimulus was 
given every time, and now, what one sees, 
over many repetitions of that stimulus. 
Is that on the first trial, you see a 
word, zero, zero, one, zero, zero, zero, 
zero. 
On the next trial, you have the same 
word, but now you see that there are some 
times when there was no spike, and some 
times when that spike appeared in a 
different bin. 
What that's going to do is generate a 
distribution of different wads. 
Now that distribution is going to be 
considerably narrower than the total 
distribution. 
And it's exactly this reduction in the 
entropy from knowing nothing about the 
stimulus, to knowing something about the 
stimulus that information will be 
capturing. 
Alright, so let's go ahead and apply 
Grandma's recipe. 
We'll take a stimulus sequence and repeat 
it many times, by how we're sampling the, 
this probability of stimulus. 
We're going to use a bit of a trick, 
which is that instead of averaging over 
all possible stimuli, we're going to take 
a long random stimulus and average it 
over time. 
So, now, time is standing in, for the 
average over stimulus. 
So now, for each time, in the repeated 
stimulus, we're going to get a set of 
words, p of w, given stimulus at time t. 
And our noise entropy, our average noise 
entropy, is now going to be averaged over 
those different time points, i. 
So if we choose a length of repeated 
sequences long enough, that will allow us 
to sample the noise entropy adequately. 
So let's have a look at the application 
of this idea to data from the LGN in a 
classic paper by Pam Reinagel and Clay 
Reid. 
They carried out this exact procedure, so 
as you saw before they ran a random 
stimulus over many trials. 
Then they ran a fixed stimulus, call it 
frozen white noise, which has some 
structure, in fact here it is. 
It's the stimulus as a function of time 
and you can see that in response to the 
stimulus spikes appeared in a time lock 
sequence. 
And now for an averages across those 
repeats one finds a PSTH, that is a Post 
Stimulus Time Histogram, Where these 
events show these large modulations in 
the time varying fine rate produced in 
response to that stimulus. 
Now, if one zooms in on a tiny piece of 
these responses, you'll see something 
like this. 
So, at, at very Fine time scales. 
There's quite a bit of jitter in those 
responses. 
Now our goal in computing the 
information, and what the author has 
examined in this paper was ask on what 
time scale, do these responses continue 
to convey information about the stimulus? 
So one can see by looking at this picture 
that there's quite a bit of variability 
in the spike train, and so that defines 
some kind of window around which a spike 
can jitter and still signal the same 
information about the input. 
So the questions we'd like to understand 
is how finely do we have to bend our 
spike train and pay attention to the 
individual timings of spikes in order to 
extract all that the neural code has to 
tell us about the stimulus? 
So one can do that by exploring the 
information produced by the spike train 
as a function of these two parameters, as 
a function of delta t, the Binning time 
width and also of the length of the word, 
as the word gets longer our coding symbol 
is able to capture more and more of the 
correlations in the input. 
And so, to what extent does increasing L 
continue to capture more and more 
information about the stimulus. 
So here's what the authors found in the 
LGN, they varied both DT, both the 
temporal resolution of their words and 
the total word length. 
So, here drawn as a function of 1 over L. 
And have plotted here the information 
that they calculated for different 
choices of those parameters of the 
definition of the word. 
So, clearly, there's going to be a 
problem in going to this limit of very 
large word lengths. 
So, as the word gets longer an longer, 
for a finite amount of data, you're going 
to have very few samples of a word of 
that length. 
And so when one tries to estimate the 
entropy of the distribution of words of 
this length, it's very unlikely that you 
will have seen them all. 
And so not surprisingly, if you now look 
at the entropy, plotted as one over the 
word length The entropy drops off at this 
limit indicating that the information is 
not completely sampled. 
So what can be done is to compute the 
entropy for different lengths of words 
and you can see that these form almost a 
line. 
And so one can simply extrapolate the 
tendency of this line back toward 
infinite word length. 
And extract an estimated value for the 
entropy at that limit. 
That's not what was done in this figure 
this was purely the information directly 
captured. 
And so one can look over different delta 
t's and different word length to see how 
information depended on these parameters. 
So what you should notice is that there 
is some limit. 
To DT, beyond which the information 
doesn't grow anymore. 
As one looks at the woods in higher and 
higher temperol resolution. 
So one takes into account finer and finer 
details about how those spike patterns 
are generated. 
and so that's what's being quantified as 
we move down this axis. 
As the time discordization of the wood. 
These bin sizes, is getting smaller and 
smaller, that's able to capture more and 
more of the variability, in the spike 
train, that's actually signaling 
something different about the stimulus. 
But that at some point, it seems that 
that, information, stops increasing. 
So, this red, we're at about, you know, 
between 80 and 100 bits per second, is 
the information rate. 
And you see that that stops increasing 
with delta t, and of delta t of about 2 
milliseconds. 
So hopefully you'll remember from the 
jitter in the spike trance that we looked 
at, that they seem to be repeatable on a 
time scale of about a millisecond or 2 
milliseconds. 
So that time scale dt corresponds to the 
time scale in which the jitter in the 
spike train. 
Still allows one to read that off as an 
encoding of the same stimulus. 
It's going to quantify approximately 
what's the temporal with that one can 
discatize this spy train and still 
extract all the information about the 
stimulus that distinguishes it from other 
stimuli. 
So in this example we've seen one case 
where we didn't have enough data to be 
able to sample say very long words. 
In general this is always true. 
When one's trying to calculate 
information theoretic quantities, one 
needs to know the full distribution of 
responses, and the full distribution of 
stimuli. 
And there's simply never enough data to 
come up with really reliable estimates 
for information, unless one has very 
simple experimental setups. 
And so a lot of effort has been put into 
finding ways to correct the sample 
distributions for the fact that there is 
a finite amount of data. 
And there's been some very interesting 
work by a number of groups over the last 
15 years or so, that has made significant 
advances in being able to compute 
information theoretic quantities from 
finite amounts of data. 
Now we're going to turn to a different 
approach, this one proposed by [UNKNOWN] 
Brenner and [UNKNOWN]. 
How much does the observation of a single 
spike tell us about the stimulus? 
Now this is similar to the case that we 
started with at the beginning of this 
lecture, but now we're going to address 
the question that we noted then What if 
we don't know exactly what it is about 
the stimulus that triggered the spike. 
It turns out that, as in the case we just 
went through, is straightforward to 
compute information with an explicit 
knowledge of what exactly in the input is 
being encoded. 
This is because the mutual information 
allows us away to quantify the 
relationship between input and output 
without needing to make any particular 
model of that relationship relationship. 
So, the paradigm is exactly the same as 
before. 
We're going to compute the entropy of 
responses, when the stimulus is random, 
and the entropy, when given a specific 
stimulus. 
So, here, things are a little simpler, 
than in the case of Wuds/g, without 
knowing the stimulus, the probability 
that a single spike acud/g, is given by 
the average firing rate times the bin 
size. 
Similarly, the probability of no spike is 
just 1 minus that. 
Now the probability of a spike at a given 
time during the presentation of a 
stimulus r of t times the time then, when 
now r of t is the time varying rate 
caused by the changing stimulus We can 
get an estimate of that time varying rate 
by repeating the input over and over 
again. 
The variability in these responses means 
that these events show a continuous 
variation, and have some width as we saw 
before, depending on the jitter and the 
spike times. 
So let's go ahead and compute the 
entropy. 
We're going to define, for the moment, p 
equals r bar delta t and p of t to be r 
of t delta t. 
The information will simply be the 
difference between the total entropy, 
we've already computed that in the 
beginning of the lecture For, for this 
binomial case to minus p log p minus 1 
minus p log 1 minus p and we need to 
subtract from that the noise entropy. 
Now the noise entropy would take on a 
value at every time t depending on the 
time variant firing rate. 
Now again every time t represents a 
sample of stimulus S. 
And averaging over time is equivalent to 
averaging over the distribution of s. 
This ability to swap an average over the 
ensemble stimuli, for an average over 
time, is known as ergodicity. 
At different values of S are visited in 
time with the frequency that's equivalent 
to their probability. 
So now we have our expression for the 
information between response and 
stimulus, we can do some manipulations on 
it. 
So we're placing back P by R delta T. 
We can take the time average firing rate, 
to be equal, to the mean firing rate, so 
that's equivalent here to this, to the 
integral, over, the probability as a 
function of time, in the mean, going 
toward that main firing rate. 
And getting rid of some small terms, we 
have here a couple extra, extra pieces 
that turn out to be small, we end up with 
a rather neat expression for the 
information per spike. 
let's take a closer looks at this 
expression, as we've emphasized already 
This method of computing information has 
no explicit stimulus dependence. 
Meaning no need for any explicit coding 
or decoding model. 
It relies on the repeated part of the 
stimulus being a good estimate of the 
distribution of a possible stimuli. 
Note also that although we computed this 
for the arrival or not of a single spike, 
this formulism could be applied to the 
rate of any event. 
For example the occurrence of a specific 
symbol in the code. 
So this is a way to evaluate how much 
information might be conveyed by a 
particular pattern of spikes, for example 
a sudden inter spike interval. 
We can also examine what determines the 
amount of information in the spike train 
/g. 
So looking again at this expression, we 
can see that it's going to be determined 
by two things. 
One is timing precision. 
That's going to blur this function R of 
T. 
So if events are blurred so that R of T 
increases and decreases slowly, without 
reaching large values, this will reduce 
the information. 
At the extreme, let's imagine, that the 
response is barely modulated at all by 
this particular stimulas. 
In that case, r of t goes towards the 
average firing rate. 
And one gets no information. 
The more sharply and strongly modulated r 
of t is the more information it contains. 
The other factor is the main firing rate. 
If the spike rate is very low then the 
average firing rate is small and 
information is likely to be the large. 
The intuition is that the low firing rate 
signifies that the neuron response to a 
very small number of possible stimuli so 
that when it does spike its extremely 
informative about the stimulus. 
Note that this is the information per 
spike. 
The information transmitted is a function 
of time, for the information rate is 
going to be small for such a neuron. 
So let's look at some hypothetical 
examples. 
Rat hippocampal neurons have what's known 
as a place field such that when the rat 
runs through that region in space, the 
cell fires. 
Let's imagine the place cell looks like 
this. 
As the rat runs around the field, Is 
going to pass through that place field, 
and what's the firing rate going to look 
like? 
Here, as it moves through the field is 
going to go from zero, ramp up kind of 
slowly, go down again. 
Because that place field is quite large, 
the red is likely to pass through it 
farely often. 
So we're going to get some R of T of that 
form. 
Now let's imagine that the place field is 
very small. 
Now, rat runs around. 
Very, very rarely passes through that, 
that place field. 
And so, now, going to get almost no 
firing and then some blip of firing as it 
passes through that field. 
Now, what if the edges of the place fill 
the very shop? 
So now again rat runs around. 
Very, very rarely passes through that 
field, so now as the rat runs around, it 
passes through that place field very 
rarely, but when it does, the firing rate 
increases very sharply toward its 
maximum. 
So that's going to increase the 
information we get from such a receptor 
field. 
Okay, so now we're done with computing 
information in spike trains. 
Next up we'll be talking about 
information and coding efficiency. 
We'll be looking at natural stimuli. 
What are the challenges posed to our 
nervous systems by natural stimuli? 
What do information theoretic concepts 
suggest that neural systems should do 
when they encode such stimuli? 
And finally, what principles seem to be 
at work in shaping the neural code? 

In this section we'll be addressing how 
information theoretic ideas can help us 
to understand how the neural code may be 
specially adapted to the structure of 
natural signals. 
We'll briefly first look at some of the 
special properties of natural inputs. 
And then some theories of how code should 
behave. 
Finally we'll sum up with some 
suggestions from the principles that may 
be at work in shaping the neural code. 
So I'm going to show you some photos that 
we taken by one of our Post-Docs, Fred 
Sue, as he was sitting in his apartment 
on one of our typical sunny Seattle 
afternoons, looking out at the view. 
He tried to take a picture that both 
encompassed his beautifully furnished 
apartment and the grand view outside. 
You can see that he had to change his f 
stop over a wide range in order to be 
able to capture information both about 
the scene inside and about the world 
outside. 
Now this is something that our eye does 
effortlessly. 
If you were sitting here at this table, 
you would be able to see both the inside 
and the outside with perfect fidelity. 
So looking even at this familiar example, 
we can see two properties that are 
characteristic of natural inputs. 
One is that there's a huge dynamic range. 
There are variations in light level and 
contrast that range over orders of 
magnitude. 
We can see signs of another property by 
comparing these two boxes. 
Because of effects of depth and 
perspective, there's similar structure, 
similarly well defined shapes and objects 
at very different length scales. 
This is reflected in the power spectrum 
of natural images. 
If one computes the power in different 
spatial frequency components this 
function has a, this function has a power 
log form. 
That is it scales, like the frequency, to 
the power minus two. 
This reflects the lack of any 
characteristic scale. 
The similar structure are not. 
Despite these scale differences and the 
very large variations in light and 
contrast across the image, we'd like to 
be able to distinguish detail at every 
point in it. 
Unlike this camera. 
These basic issues arise for almost all 
of our senses. 
Here's an audio track of a chunk of 
speech. 
The signal is full of complex 
fluctuations that carry detailed 
information about pitch and nuance. 
However, these fast variations are 
modulated by the relatively huge 
variations in amplitude that make up the 
envelope of speech. 
We're perfectly capable of understanding 
all of these signal components regardless 
of the overall amplitude, even when there 
are multiple speakers, or they're far 
away. 
So how can a neural system, with a 
limited range of responses, manage to 
convey the relevant information about 
details in the face of these huge 
variations of scale? 
We found that the entropy, we found that 
the entropy, reached it's maximum, when 
it was of the form of these two symbols 
were used equally often. 
Now if we're thinking about maximizing 
the mutual information. 
We also have to take into account this 
noise term. 
But generally the amount of noise for a 
given stimulus may not be something 
that's easily controlled. 
While the total response entropy is 
something that's in the hands of the 
coder. 
Let's see how. 
Let's imagine that the stimulus that a 
system needs to encode Is varying in 
time, this is s of t, it has some 
distribution, p of s over here. 
Our job as an encoder is to map the 
stimulus onto the symbols that we have at 
our disposal. 
Let's imagine that we're constrained to 
use some maximal firing rate, so we have 
some limited range of possible symbols at 
our disposal, say zero to 20 hertz. 
How should we organize that mapping so 
that we end up with the most efficient 
code? 
We'll get the most information by 
maximizing our output entropy. 
That is, by using all of our symbols 
about equally often. 
So what does that imply for the shape of 
this curve? 
So what we should do is move along our 
stimulus distribution and encode equal 
shares of that distribution with each 
symbol. 
If we have 20 symbols lets count up 1 
20th of our total area under this curve, 
and assign that to symbol one. 
What this amounts to is a response curve 
that's given by the cumulative integral 
of the stimulus distribution. 
Another name for this is histogram 
equalization. 
So this implies that for a good coding 
system, its input output function, this 
function here, should be determined by 
the distribution of natural inputs. 
So here's a classic study in which this 
idea was tested directly. 
In the early 1980's, Simon Laughlin went 
out into the fields with a camera, and 
measured the typical contrasts, that is 
deviations in the light level, divided by 
the mean light level, that would be 
experienced in the natural world, for 
example, by a fly. 
So, that's this distribution here. 
If the response does indeed follow the 
distribution of natural inputs... 
Then the response curve, here, should 
look like the cumultive probability 
determined by integrating p of c. 
And in fact, that's a very good match to 
what he did actually observe in the 
response properties of the fly large 
mono-polar cells, the neurons that 
integrate signals from the fly's 
photo-receptors. 
Now, a study like this poses a challenge. 
While it makes sense that our sensory 
systems would, over evolution or 
development, set up response codes that 
are adjusted to natural input statistics. 
It seems that much more work is needed to 
handle the problems posed by this huge 
natural variation, that stimuli take as 
one moves from indoors to outdoors or 
even moves one's eyes around a room. 
The contrast distribution is varying 
widely. 
Might sensory systems rather adjust 
themselves on much shorter timescales to 
take these statistical variations into 
account. 
So let's take a patch of the image, and 
look at the, the variations in contrast 
in that image. 
Here for example, that contrast 
distribution might take, might be narrow 
like this. 
Wheras over here, it might be much 
broader. 
What our code should do is take the 
widths of these distributions into 
account in setting up a local. 
Input, output curve, that accommodates 
this structure of the, currently measured 
statistics of the input. 
So that's the question that we tested 
here, in the h1 neuron. 
In this experiment, we took a white-noise 
input, of the type that you used in the 
problem sets, so some s of t. 
Looks like that. 
And we multiplied it by some time 
varying, slowing time varying envelope. 
Call that sigma of t. 
And that's what you see here. 
So we repeated the same sigma of t. 
This is a 90 second long chunk of 
stimulus. 
Repeated the same sigma of t. 
In every trial, but we changed the 
specific white noise. 
Stimulus. 
And that allowed us to pick out spikes 
that occurred at different time points 
throughout this presentation of, of sigma 
of t, where in every trial the cell would 
have seen a different specific stimulus. 
And to calculate the input output 
function described by those spikes, in 
those different, in those different 
windows of time. 
So now one, when one analyzes spikes 
across these different windows, and pulls 
out their input output function using the 
methods that we talked about in week two, 
one finds that for example, here in this 
window, one gets a very broad input and 
output curve. 
Where, when the stimulus is varying very 
little, one finds a very sharp input and 
output curve. 
Now, it turns out that if one normalizes 
the stimulus by its standard deviation, 
or by this envelope sigma of t, all of 
these curves collapse onto the same 
curve. 
What that says is that the code has the 
freedom to stretch its input access such 
that it's accommodating these variations 
in the overall scale of the stimulus. 
And it's able to do that in real time as 
this envelope is varying. 
This is being seen in several other 
systems, including the retna and the 
auditory system. 
But here's an example from rat barrel 
cortex. 
This is somatosensory cortex of the rat. 
In particular. 
The part that encodes the vibrations of 
whiskers. 
So, from extracellular in vivo recordings 
of responses to whisker motion, whiskers 
were stimulated with a velocity signal 
again, s of t, that looked like this. 
So this is a slightly simpler experiment. 
The standard deviation was varied between 
two different values. 
And now one can pull out spikes that are 
generated in these two epochs that 
presentation. 
The high variance case and the low 
variance case. 
And one can compute input output curves 
for spikes that occurred under these two 
different conditions. 
So in the low-variance case, one sees 
this input output curve, in the 
high-variance case, one sees this input 
output curve. 
And hopefully you won't be surprised that 
if I now divide the stimulus. 
By its standard deviation, we now see a 
common curve. 
So now we see again that this input 
output curve has the freedom to stretch 
itself such that its able to encode 
stimuli in their natural dynamic range. 
So what I've shown you is that as one 
changes the characteristics of the 
stimulus. 
In this case, in the cases we've talked 
about, by changing its overall amplitude, 
changes can occur in the input output 
function. 
So here we've found that if a stimulus 
say, took on this dynamic range, it might 
be encoded with an input output curve 
like that. 
Now you should be able to see that if one 
increased the range of the stimulus and 
stayed with that same input output curve. 
Most of the time, your stimuli would be 
giving responses that were even zero or 
at saturation point. 
Similarly, if you now decrease the range 
of the stimulus you'd be hovering at the 
central part of the curve. 
So, ideally one would like to use one's 
entire dynamic brain by defined by this 
input output curve. 
And so, one would like to match it to the 
range of the stimulus. 
And that's exactly what we saw in the 
experiments. 
Now this adaptive representation of 
information is not confined to change us 
in the input output function. 
It's also been seen that changes can 
happen in the feature as the statistics 
of the inputs are changed. 
The feature that's selected by a neural 
system can also adapt to changes in the 
stimulus statistics. 
And information theory has also been used 
to explain the way in which this occurs. 
For example it's been used to explain how 
the spatial filtering properties of 
neurons in retina, and in LGN change with 
light level. 
Joe Addick and his colleagues pose the 
following question: If we consider that 
the retina imposes a linear transfer 
function, or a filter on its inputs, 
what's the shape of that filter that 
maximizes information transmission 
through the retina? 
The solution turns out to depend on two 
things. 
The powers spectrum of natural images and 
the signal-to-noise ratio. 
At high light levels, or high signal to 
noise, one would predict a filter shape 
like the one we've seen already, the 
Mexican hat shape. 
This acts like a differentiator, looking 
for edges of the stimulus, but at low 
light levels, the predicted optimal 
filter is integrating, and simply 
averages its inputs to reduce noise. 
And indeed in retinal receptive fields 
it's seen that the surround becomes 
weaker at low-light levels and the center 
braoder which qualatatively matches these 
predictions. 
We can also use information theory to 
find out what it is about a stimulus that 
drives a neuron to fire. 
We looked at this method in week two. 
In this case, this is called the, the 
method of maximally informative 
dimensions. 
One can choose a filter, so one can 
extract from the stimulus some component 
that maximizes the Colbeck-Libler 
Divergence between the spike conditional 
and the prior distributions. 
This turns out to be equivalent to 
maximizing the information that the spike 
provides about the stimulus. 
One can use this method to search for the 
optimal feature that explains the coating 
properties of a system. 
When it's being presented with stimuli of 
a particular distribution. 
Distribution. 
So for example if one initially starts 
with a Gaussian white noise distribution, 
that's a Gaussian, that's vertical 
Gaussian, in this, in this 
representation. 
One might find a particular feature. 
But now if one changes the distribution 
to say natural images, which will have 
some very different distribution. 
The filter that maximizes the, the 
information between spike and stimulus 
maybe different and that's being shown to 
be the case for cortical receptive fields 
among other systems. 
So, finish up by discussing briefly an 
influential idea that Ragesh mentioned in 
the first lecture. 
That might explain my cortical receptor 
fields have the shape that they do. 
Many years ago, Horace Barlow proposed 
that because because spikes are 
expensive, neural should be trying to 
encode systems as efficiently as 
possible. 
What does this mean for a popular of 
neurons? 
If you consider the joint distribution of 
the responses of many neurons, here lets 
just take two. 
Maximizing their entropy should imply 
that they code independently. 
That is their joint distribution should 
factor into the product of the two 
marginal distributions. 
This is a strategy that would maximize 
their entropy. 
Why is that? 
Because the entropy of a joint 
distribution is always less then or equal 
to the entropy of the distributions of 
the marginals added together. 
So this idea is known as redundancy 
reduction. 
The neural system should be optimized to 
perform as independently as possible. 
However in the past years, it's been 
realized that correlations between 
neurons can have some advantages. 
For one. 
Having many neurons that encode the same 
thing may allow for error correction and 
more robust coding. 
It's also been realized that correlations 
can actually help discrimination, and 
indeed, neurons in the retina have been 
observed to be redundant. 
That is, that their joint distribution is 
very different from the product of 
independent distribution. 
More recently, Barlow proposed a new 
idea, that neuron populations should be 
as sparse as possible. 
That is that their coding properties 
should be organized so that as few 
neurons as possible are firing at any 
time. 
This idea was developed formally by 
Olshausen and Field, and also Bell and 
Sejnowski. 
Here's the idea. 
Let's say that one can write down a set 
of basis functions, phi i, with which to 
reconstruct a natural scene. 
Then any image can be expressed as a 
weighted sum, with coefficients ai over 
these basis functions with perhaps the 
addition of some noise. 
Now this basis function should be chosen 
so that as few coefficients ai as 
possible are needed in general to 
represent an image. 
This is carried out by minimizing a 
function that includes the reconstruction 
error. 
So here, the root mean squared difference 
between the reconstructed image and the 
image itself. 
So that one gets a good match to the 
images, but that also includes a cost 
term, whose role, whose role is to count 
how many coefficients are needed, so one 
simple choice of this cost function, is 
just the absolute value of these 
coefficients. 
[INAUDIBLE]. 
The coefficient lambda, weights the 
strength of that constraint. 
The job of this term is to penalize 
solutions that require too many basis 
functions to represent an image. 
Too many coefficients ai, that are, that 
are different from zero. 
A fourier basis for instance, represents 
the images as a sum of signs and cosines. 
While the fourier basis is guaranteed to 
be able to represent any image. 
One might already be able to guess that 
coding with such a basis is not sparse. 
Because, as you probably recall, the 
power spectrum is broad, which means, 
that many coefficients are needed. 
When one runs an algorithm to find the 
best basis functions, the best values of 
phi i, for natural images, one finds, 
instead, a set of functions that look 
like this, like localized oriented 
features, like those that we see in v 
one. 
So this implies that when we view an 
image using neuronal receptive fields 
that look like this, this excites on 
average a minimal number of neurons. 
This is called a sparse code. 
So we've touched upon several different 
ideas about coding principles. 
The idea of coding efficiency, that 
neural codes should represent input 
stimuli as efficiently as possible. 
We've seen that this implies adaptation 
to stimulus statistics. 
As one changes the statistics of the 
stimulus, one should see aspects of the 
coding model changing to ensure that it 
remains efficient. 
We've also brought up the idea of 
sparseness. 
That it would be ideal if the neural code 
needed as few neurons as possible to 
represent its input. 
And this brings us to the end of our 
discussion of coding. 
I've shown you some classic and state of 
the art methods for predicting how 
stimuli are encoded in spikes. 
We've seen models for decoding stimuli 
from neural. 
Responses. 
We've discussed information theory and 
how it's used to evaluate coding schemes, 
and we've taken a very quick glance at 
how coding strategies might be shaped by 
the statistics of natural inputs. 
There's a lot that we've missed. 
In particular, let's just go through the, 
the typical cycle of behavior of an 
organism. 
Where we have invested some time is the 
idea, that we go from complex 
environments, animals extract some 
features from that environment to solve 
problems, and that's represented in 
neural activity. 
What the brain is then doing is 
extracting that information and 
synthesizing it to drive decisions. 
We talked about some examples of using 
maximum likelihood methods that might in 
fact have neural implementation. 
These decisions then generate motor 
activity which drives behavior. 
Muscles work together to perform actions 
that drive behavior output. 
And these actions effect subsequent 
sensation. 
So, we didn't really address any of this 
part of the, of the behavioral feedback 
loop. 
Next week, we'll be moving onto a new 
topic. 
Rather than handling data analysis, we'll 
be moving more into the realm of 
modeling. 
And we'll start that with a brief 
introduction to the bio-physics of 
coding. 
How do single neurons generate action 
potential. 
We'll talk about neuronal excitability. 
And we'll close up with some simplified 
models that capture neuronal firing 
before moving on to the second part of 
the course where you'll be learning about 
network modeling. 
So that's all for this week. 
Looking forward to seeing you next week. 

[MUSIC]. 
Hello, this is Adrienne Fairhall. 
Welcome back for my final week of this 
part of Computational Neuroscience. 
This week, we're going to be talking 
about computing in carbon. 
We're going to be zooming down from the 
very high level picture that we had of 
Hugh Jackman neurons, those I, those I do 
have, to the, the single neuron level. 
And thinking about how the brain, in 
fact, instantiates the kinds of 
computations that we saw in previous 
lectures. 
So we're going to be talking about the 
basics of neuroelectronics, the 
membranes, the ion channels, and the 
wiring that make our brains work. 
We'll be going from there to talk about 
single, simplified neuron models. 
And thinking there about the basic 
dynamics of neuronal excitability. 
Can we as modellers come up with simpler 
models that capture the essential 
dynamics that neurons instantiate? 
Finally, we'll be talking neuromal 
geometry. 
We see that neurons have very complex 
structures. 
So how do these matter? 
How do they affect the computational 
properties of the neuron? 
And in what way do we need to take them 
into account? 
Our goal for today is going to be to take 
this somewhat realistic picture of a 
neuron complete with a soma with 
dendrites that collect inputs from, from 
other neurons. 
And an axon that transmits action 
potentials generated in or near the soma 
to other neurons into a circuit diagram. 
We're going to start with a model of an, 
a small patch of membrane that generates 
action potentials only. 
And I hope to show you exactly what gives 
a neuron its very distinctive behavior, 
the ability to fire an action potential, 
using the classic Hodgkin-Huxley model. 
Just to note that while we've been using 
quite a variety of mathematics in this 
course so far, including linear algebra 
and probability, today's method's 
going to be based largely on differential 
equations. 
We'll be making quite heavy use of first 
order differential equations, and that 
should be enough to understand what's 
going on, which is interesting as you'll 
see. 
What I hope you'll take from today's 
topics is an understanding of ion channel 
dynamics and a general sense of how one 
can either make these models more simple 
or more complex, depending on your goals. 
So let's start with a review of simple 
circuits. 
I imagine that for some of you, it's been 
a while, if ever, so I'll go over the 
basics. 
First remember that along any line in a 
circuit diagram, that represents a wire, 
and so current can flow freely and 
there's no voltage drop along the wire. 
All voltage changes around a circuit are 
associated with circuit elements, such as 
a capacitor here, or a resistor, or a 
battery. 
A capacitor is an insulator which 
accumulates charge on either face. 
A resistor resists the flow of current, 
like a narrowing in a pipe which slows 
down the flow of water. 
A battery causes a potential drop. 
At any junction of wires, the total 
current is zero. 
This is called Kirchhoff's law. 
For example, here at this junction, let's 
imagine that we're driving the circuit 
with some input, let's say i x. 
In that case, the sum of the, the current 
flowing out here through the capacitor 
and here through the resistor must add up 
to the current that's coming in to i x. 
How does the potential change across a 
resistor? 
There's a voltage drop that currents 
times the resistance. 
This is Ohm's law. 
Often in neuroscience, we use an 
alternative measure of resistance, the 
conductance, which is just one over the 
resistance. 
So now let's look at a patch of the 
membrane that encloses our neuron. 
How are we going to model it, using such 
a circuit? 
Remember that the membrane consists of 
two layers of lipids or fats. 
These are good insulators. 
Embedded in the membrane are ion channels 
which allow ions to pass through the 
membranes selectively. 
So let's use those laws to write down the 
first pass at an equation for the neuron. 
For now we'll leave aside the ion 
channels, we'll come back to them. 
The membrane still allows some small 
amount of charge to flow through. 
So how does the membrane itself behave? 
The lipid bilayer behaves like a 
capacitor. 
As some charge can pass through the 
membrane, we also need a resistor in 
parallel. 
This gives us this very simple circuit. 
So how do we write down an equation for 
this circuit's behavior? 
We'd like to obtain an equation for the 
voltage, here v, across the membrane. 
Now we can use Kirchhoff's law to find 
it. 
All the currents have to sum up. 
So by putting some external current, pie 
X'd. 
What is IR? 
What's the current through the resistor? 
Well we can get that from Ohm's Law. 
It's just V over R. 
How about the capacitative current? 
We get that from the definition of the 
capacitance given here. 
So, the capacitance is defined to be the 
charge that can be stored across the 
capacitor divided by the voltage. 
Now, we can take the time derivative of 
this expression, so let's rewrite it as q 
equals c times b. 
And take the derivative of that. 
And dq dt equals C. 
C is just a constant dvdt. 
Now dqdt, the time derivative of the 
charge, is just a current Ic. 
So now we see that Ic is just C times 
dvdt. 
We throw these terms back into 
Kirchhoff's law and we get the following 
differential equation. 
So this differential equation is linear 
and it's first order in V, that is it's 
dependence on V is just simply linear in 
V. 
So people paying sharp attention will 
have realized that while this might be 
all well and good for a little chunk of 
membrane floating around in some 
solution, it's not really what's going on 
with our cell. 
Remember from the first lecture that the 
membrane encloses a solution in which the 
concentrations of various ions are 
different from the outside. 
So this now is more like it. 
Outside the cell is the, is the harsh 
world, basically the sea with high 
sodium, high chloride, and also high 
calcium concentrations. 
Inside, the cell has hoarded some 
potassium, which are, which is necessary 
for many of its life operations. 
It maintains these ionic gradients with 
specialized pumps that exchange sodium 
for potassium. 
Now this concentration gradient gives us 
a battery and maintain potential 
difference across the cell. 
There are two opposing forces at work. 
By osmosis, ions tend to move down their 
concentration gradient, at least until 
that osmotic force is opposed by 
electrostatic forces. 
The potential difference at which these 
forces are in equilibrium is called the 
Nernst potential. 
This is given by the ratio of the 
concentrations on the inside to the 
concentrations on the outside. 
Take the log, and now that's multiplied 
by this set of constants. 
Now kbT, this is the Boltsman constant, 
this is the temperature. 
Q is the ionic, ionic charge, and z is 
the, is the number of charges in the ion. 
Now how does our equation change in the 
presence of this battery? 
The effect is that part of the voltage 
drop V, that occurs across the resistor 
is lessened by the battery, right? 
So before, we had all of that voltage 
drop occurring across the resistor. 
Now, part of the voltage drop v is, is 
separate from the resistor. 
And so now, the part of the voltage drop 
that occurs across the resistor is v 
minus v rest. 
So that's going to reduce the current 
through the resistor. 
So now, our IR, the current through the 
resistor is going to be v minus v rest 
over R. 
So here's our new equation. 
Some of you will be familiar with the 
solutions of such a differential 
equation. 
They take the form of exponentials. 
So let's put this equation in a 
convenient form, where we can read off 
some important quantitites. 
With a little bit of algebra, we can 
rearrange this equation into this form. 
The constant, tau, here, is known as the 
time constant. 
Perhaps you can figure out, what is tau 
in terms of the original constants? 
We also have this term, v infinity. 
What's the meaning of v infinity? 
So, let's set the time derivative to 
zero. 
That is, assume that whatever changes we 
might have made in, in i x happened long 
ago. 
And we've let the system settle down. 
If you set dvdt equals zero, then v 
equals v infinity. 
So it's just the steady state. 
So imagine we're putting in a constant 
input current. 
What is v infinity in that case? 
Here's an example solution of this 
equation for a square pulse of 
[INAUDIBLE] input current. 
Initially v is zero. 
Then it rises exponentially until it 
reaches V infinity. 
When the current is switched off again, 
it falls exponentially back to zero. 
So, the solution here looks like V 
infinity 1 minus e to the minus t over 
tau. 
And in the falling phase, this looks like 
v infinity e to the minus t over 10. 
We said that the potential was given by 
the ratio of ionic concentrations inside 
and out. 
If ions could all flow in the same way 
through the membrane, we wouldn't need to 
distinguish between concentrations of 
different ionic species, we'd just 
consider the total charge. 
However, the way that ions flow through 
the solids via ion channels, and the, 
these do distinguish between different 
ions. 
What are ion channels, really? 
They are amazing little molecular 
devices, and there are many types. 
Some are voltage-dependent, some are 
neural transmitter dependent, some depend 
on calcium, some are mechano-sensitive 
and some are heat-sensitive. 
The property we'll be focusing on today 
is voltage sensitivity. 
So let's look at the current that comes 
through a single ion channel as it opens 
and closes stochastically. 
We can still apply Ohm's law to this one 
tiny channel. 
So the current is just the voltage drop 
across the, across the channel, divided 
by r, the resistance. 
Or we can also write that as I equals Vg, 
where g is the conductance of that 
channel. 
So two factors set the size of the 
current. 
The voltage drop across the membrane and 
the conductance of the channel. 
So, it turns out that each ionic species 
has its own associated equilibrium 
potential. 
Then when the membrane allows a 
particular ion type to flow through it, 
through an appropriate channel, then that 
current will tend to pull the membrane 
potential toward the equilibrium for that 
ion. 
So here are some examples of the values 
of equilibrium potentials for different 
ionic species. 
So, sodium has a positive equilibrium 
potential. 
Whereas potassium has a very negative 
equilibrium potential. 
Here's the one for, for calcium. 
Also very negative. 
And for chloride. 
6 at minus 60 millivolts at minus 60 
millivolts which is around the rest 
potential. 
We going to be concentrating today on 
these two, on sodium and potassium. 
What you should notice here, and the most 
important thing to notice about the rest 
of our discussion today, is that sodium 
and potassium have opposite tendencies. 
Sodium currents tends to depolarize the 
membrane. 
That is to move it to more positive 
potentials. 
While the potassium current tends to 
hyper polarize it. 
That is to take it toward more negative 
potentials. 
So while there are many different types 
of ion channels that allow different ion 
species to pass through the membrane, 
we're going to focus on sodium and 
potassium. 
Each ionic path through the membrane has 
its own battery potential, as we just 
learned. 
And its own conductants. 
So that the current through each branch 
of circuit is by Ohm's law, v equals i r. 
But let's use conductants, and we have to 
discount the membrane voltage drop. 
Right, we have v across the membrane. 
You have to discount the membrane voltage 
drop by the equilibrum potential for each 
ion. 
So now we can see that the current 
flowing through each of these branches 
indexed by the ionic type is the 
conductance for that ion, multiplied by V 
minus Ei where Ei is, is the equilibrium 
potential for that ion species. 
So this is all interesting. 
There are different ions; they have their 
own battery. 
But we haven't yet seen what makes the 
transformation that a neuron makes on its 
inputs qualify as a computation. 
Simply linearly transforming the current 
through the passive membrane properties 
doesn't seem to have the flavor of 
computing. 
Computing, one could say, requires doing 
something irreversible. 
Selecting something, getting rid of other 
things, qualitatively changing something 
into something. 
As we discussed in week two, a linear 
transformation simply re-represents the 
input in another coordinate basis. 
One can always invert that tranformation 
and get back what you started with. 
And the coding models that we looked at. 
The linear filters extracted a new 
representation, but the non-linearity 
threshold at that new representation and 
made the transformation non-invertible. 
Information about other components was 
irretrievably thrown away. 
So here at the single neuron level, you 
can see how this linear behavior breaks 
down. 
These traces all look comfortably linear, 
nice scale versions of the same response 
as one changes the sign and amplitude of 
this current input. 
Exactly as a linear system should do. 
But for some input currents, something 
totally different happens. 
This is called excitability. 
So let's take a break here and come back 
to figure out exactly what's going on. 

In the last section we left off with this 
picture, a neuron responding to current 
steps of different, of different 
amplitudes, and we saw that it's on the 
threshold here, of excitable behavior. 
So our goal in the remainder of this 
section is to uncover the nonlinearity 
that leads to this vital property. 
In the last section, we realized that 
each ion has its own pathway for current 
to flow, and a battery of potential 
difference that's associated with each. 
We can represent that as these different 
branches. 
So let's assemble them now, back into, 
back into our circuit. 
We have a branch for sodium. 
A branch for potassium, and we're also 
going to include one for the non-specific 
current flow of the passive membrane. 
going to call that g link. 
Now, if the ion channels had a fixed 
conductance, we'd still have a linear 
circuit, just a bit more complicated than 
our original passive, our c circuit. 
What gives this system its interesting 
behavior, is that these conductances are 
not fixed, that's what this variable 
resistor symbol stands for, they depend 
on the voltage. 
So let's take a closer look. 
So now we're zooming in down to the level 
of a single potassium channel. 
The channel is an elaborate molecular 
machine that contains a gate that 
prevents ions from entering, and a 
voltage sensor here, that controls the 
configuration of the gate channel. 
The open probability increases when the 
membrane is depolarized. 
The gate here consists of four sub-units 
that need to be in the correct 
configuration in order for ions to flow 
through. 
So the open probability of the channel is 
the product of the open probability of 
these four sub-units, so the probability 
goes as, the open probability of a single 
sub-unit raised to the power 4. 
So what is the assumption we're making 
here? 
Let's try to visualize this a bit. 
So here's the closed channel. 
Each sub-unit fluctuates, open and 
closed, at some rate. 
So let's call n the probability that this 
sub-unit, one of these sub-units is in 
the open state. 
Then the probability that it's closed, is 
1 minus n. 
So now, one of those other gates can open 
and close. 
Independently, another one flicks open. 
Sometimes we have more than one open. 
Finally when all four happen to be open 
together, the ion channel allows a 
current to flow. 
A trans, a transition between states 
occur at volt, let's say this red circle 
represents the total probability of the 
state of one of the sub-units. 
It has probability n of being in the open 
state, and probability 1 minus n of being 
closed. 
Call this open and this closed. 
So there's a rate of transitions between 
the open and the closed state. 
So the closed to open state we're 
going to call the transition probability 
between closed and open alpha, and that's 
voltage dependent. 
And there's also some rate, and there's 
also some rate of transitions between 
open and closed, which your going to call 
rate beta, that also is voltage 
dependent. 
So now the time derivative of n is then 
given by the following equation. 
So this first term, represents how much 
is added to the open state, and the 
second term, how much is lost? 
The amount that's added, is proportional 
to amount that's in the closed state, 
times the rate of going from closed to 
open alpha. 
And this is the amount that's lost, the 
amount that's actually in the open state, 
n, times the rate of moving from open to 
closed. 
So let's do what we did with the RC 
circuit, and rewrite this equation in 
terms of tau and n infinity. 
So when we do this, you can easily show 
for yourself that the rates determine the 
quantities tau and infinity in the 
following way. 
Tau is 1 over alpha plus beta. 
N is alpha over alpha plus beta. 
We'll come back and use this relationship 
shortly. 
But before we do, let's take a look at 
the sodium channel. 
So here's the sodium channel. 
It's similar to the potassium channel, 
but with one important difference. 
The sodium channel is open to the joint 
opening now of three sub-units, but, 
similar to the previous case. 
But additionally, to pass current, this 
channel requires that along with the 
activation or opening of the three 
sub-units, there's also an additional 
gating mechanism. 
A kind of ball in the socket mechanism, 
and that's required to not be in place. 
That is that there's an additional gating 
factor that must be de-inactivated. 
We can express the probability of one of 
these three sub-units being open, as m 
and the probability that the gate is not 
in place as h. 
What's interesting is that while voltage 
increases, the degree of activation or m, 
it also decreases h, the level of 
de-inactivation. 
So there's a kind of voltage window, in 
which sodium is able to flow. 
Generally this results in sodium currents 
being transient or self-limiting. 
As soon as sodium starts the flow, 
because these m gates are open, the 
voltage moves towards the sodium 
equilibrium potential, and that increases 
v and in-activates h, thus closing the 
channel again. 
This is one of the mechanisms at work in 
switching off the spike. 
So now we have these three gating 
variables, one for k, that was n, and two 
for sodium m and h. 
We can also re-express them in terms of 
tau and their steady state. 
So as we did for n we can also write 
these two equations, a tau for the h 
variable and a h infinity, and also a tau 
for m, and a m infinity. 
So now what do we do with these 
activation and activation variables? 
We want to combine them to give us these 
voltage dependent conductance's for the 
channels. 
So the probably of the potassium channel 
being open goes like n the 4th. 
You're going to multiply that by the 
total conductance of the channel, and 
that will give us this voltage dependent 
conductance. 
Similarly the probability of the sodium 
channel being open is given by m cubed of 
h. 
We multiply that by sub-maximal 
conductance for sodium, and now we get 
the voltage dependent and time dependent 
conductance for sodium. 
So now let's pull it all together. 
The voltage across the membrane changes 
as a result of changes in the, in the 
external driving current, and also 
because these opening and closing 
probabilities cause the conductance's of 
these, of these branches to change. 
And the amount of current going through 
will change will both with changes in 
voltage, and with changes in overall 
conductance. 
So we can write down that equation here. 
So we have our Capacitative current, 
that's the current coming through this 
branch. 
We have the Ionic currents, which come 
down through each of the Ionic branches 
separately with sub-scripted each of the 
ions with i. 
And, and that includes our, our leak 
which includes non-specific movement of 
ions through the, through the membrane, 
and then our external applied current. 
So, what this gives us is Hodgkin and 
Huxley's equation, here, in it's, in it's 
full glory. 
So we have our equation for the voltrage. 
And we're going to add to that, these 
three equations for the different 
activation and inactivation variables, 
that specify the conductance's for the 
different ionic types, sodium potassium. 
Now let's see how we get to use our 
understanding of the activation dynamics, 
to understand the spike. 
So, remember again, n governs the opening 
of the potassium channel, and both n and 
h must be large for the sodium channel to 
be open. 
Here's how these activation steady states 
depend on voltage. 
They all have this kind of sigmoidal 
form. 
As we see from the behavior of n 
infinity, the potassium channel will have 
a higher probability of opening for 
larger voltage. 
While the sodium channel first has an 
increase of probability of opening with 
increasing voltage, because the increase 
in m with voltage. 
But then because h is going down to 0, 
as, as, voltage increases the, the sodium 
channel will close. 
It's also very useful to look at the time 
constants, going back to our equation, 
this time constant governs how quickly n 
will approach its final steady state. 
So the time constants dictate how rapidly 
each variable responds to a change in 
volt. 
Remember the exponential solution. 
Let's say one changes v, so that's going 
to give us a new value of the steady 
state, as a function of v. 
And then we wait for everything to 
adjust. 
Each activation variable will tend toward 
the steady state for that voltage, with a 
rate given by this time constant. 
So, which of these variables here reacts 
fastest? 
The variable with the shortest time 
constant, that is m. 
So that means that the fastest response 
to a voltage change, is a change in 
sodium activation. 
The dynamics of h and n are slower, these 
time constants are larger. 
And you can see that they're on a similar 
scale. 
So let's also remind ourselves what the 
resting potentials are. 
Remember that when a potassium current 
flows, it would be tending to move the 
membrane voltage toward the potassium 
potential, down here at minus, minus 80. 
Well, sodium moves it up here. 
So let's imagine we're sitting near rest. 
Rest is about minus 60 milivolts, and 
then some input comes along that 
depolarizes the membrane, that is move it 
to larger, larger voltages. 
So because the time constant for m is the 
shortest, as we change voltage the first 
thing to adjust is going to be the m 
value, its going to approach its steady 
state value, at the new value of the 
voltage. 
That starts to open sodium channels. 
Sodium current comes in, and starts to 
move the membrane toward the sodium 
equilibrium potential. 
That's going to further increase sodium 
conductance, and that's a positive 
feedback. 
So what's going to counteract that, and 
stop the voltage from just ending off to 
this large value? 
So at a slight delay because of these 
slower dynamics for, for h and for n, two 
things are going to happen. 
One is that h goes to h infinity. 
So finally the dynamics of h catch up, 
and h is going to approach its steady 
state value. 
And you could see that as voltage 
increases, that steady state value is 
going down. 
And remember that for the sodium channel 
to be open, we need a combination of m 
cubed and h, so if h is going towards 0, 
then those channels are closing. 
Also, to help things along, the potassium 
channel also activates more. 
So now finally n will also catch up, and 
we'll see that the potassium channel 
starts to open more and more with larger 
voltages. 
Now what does that do? 
That starts to pull the voltage back down 
here, toward the equilibrium potential 
for potassium. 
So finally the membrane will come back to 
rest. 
So this is just to show the time cost of 
these events. 
Voltage increases. 
Here, there's a fast change, you see this 
very fast slope in m. 
There, there's a positive feedback in 
which this increases very rapidly, until 
its rise is truncated by the delayed 
effects of h, now going down and, and 
closing the sodium channels. 
And n's starting to increase, and allow 
that potassium current to bring the 
membrane back toward the potassium 
reversal potential. 
So, you see here that the action 
potential is this exquisitely timed 
change of molecules and charges. 
So, what's so wonderful about Hodgkin and 
Huxley's model, is that they inferred all 
of these dynamics without any knowledge 
of ion channels. 
And particularly without any knowledge of 
sub-units. 
All the dynamics here are explained by 
simple linear equation by a linear 
circuit, or a simple rate equation, 
except for two things. 
The multiplicative factors that relate 
the sub-unit behavior to the channel 
conductance's and the voltage dependence 
of the sub-unit dynamic's. 
So, from this fundamental basis there are 
two quite different directions that we 
could go in as a modeler. 
So, one can delve into the dynamics of 
ion channels, understanding how they come 
about from the microscopic level, and how 
different signaling cascades influence 
these dynamics. 
There are, of course, hundreds of 
different channel types dependent on 
calcium and chloride, and even 
combinations of multiple ions in very 
different time scales. 
And this wide range of dynamics 
influences the way in which information 
is processed by single cells, and 
dictates which neuron types carry out 
different roles in the brain. 
Furthermore, realistic neurons are not 
just patches of membrane. 
As we've looked at here, they're large 
distributed structures. 
So how does this figure into our 
understanding of computation at the 
neuronal level. 
The other direction to go in, is rather 
than to complexify, to simplify. 
Can we write down simpler models that can 
capture the essentials of these dynamics, 
but are maybe analytical tractable, so 
that we can learn something 
mathematically. 
Or at least be rapid enough to be able to 
put into large scale simulations, that 
still respect something about the 
underlying biophysics of neurons. 
So we're going to head in these two 
different directions for the rest of the 
lecture. 
In the, in the next part of today's 
lecture, we'll, we'll first deal with 
these simplified models, where some 
examples of reduced models that people 
have developed that, that are based on 
Hodgkin Huxley like neurons. 
And in the last part of today, we'll ju, 
just touch on one of these topics, that's 
geometry. 
How do we deal with Exter, such as 
dendrites, in modeling single neurons? 

Hi. 
In this part of the lecture, we're moving 
beyond Hodgkin-Huxley to think about 
simplified models. 
So, can one build simple models that 
capture the behavior of, of true neurons? 
But are either, analytically tractable, 
so that one can do some analysis on them 
and understand, maybe, how different ion 
channels contribute to their interesting 
dynamics. 
Or else, can one build a simulation, a 
large scale simulation, that has models 
that are as simple as possible that is, 
that involves little computational time 
as possible to model and yet capture the 
relevant and interesting dynamics of real 
neurons? 
So here are a few different examples of 
firing patterns from real neurons being 
driven by a noisy input. 
On the top, you see cortical neuron early 
in development and then here thalamic 
neurons that have been recorded under 
different depolarizations. 
So here, in particular, you can see a 
very characteristic bursting pattern, 
where a bunch of spikes are generated in 
a clump. 
And at a different depolarization, those 
bursts almost disappear, and you get 
single spikes, more like, more like in 
the case of the cortical neuron. 
And finally, here's a motor neuron. 
So in this case, you see very regular 
firing. 
Motor neurons tend to fire very 
regularly, and the noise leads only to 
small deviations in the regular timing of 
spikes. 
So, we see that neurons can have a wide 
range of firing patterns, which come 
about partly because of the nature of 
their dynamics, and partly because of the 
nature of their inputs. 
Let's look at some potential examples of 
firing patterns. 
Imagine that a neuron fired regularly 
like this. 
And to a second input, it also fires 
regularly, but with a different, with a 
different spiking interval. 
So one might feel comfortable thinking 
about this neuron's behavior as 
expressing a rate code. 
The spike frequency signals the input. 
What, though, if we now had this case? 
Here, the mean frequency is the same, but 
now the firing times of spikes are 
shifted slightly. 
So, we might imagine that these little 
changes in local frequency and code 
stimulus information, may be like 
frequency modulator or FM signals. 
In the next case here, the main firing 
rate might still be important. 
But there's so much variability in timing 
that, that suggests that precise spiked 
times might mean something distinct about 
the input. 
And what about this final case? 
Here now you see that there are perhaps 
two distinct symbols in the code. 
This looks like the bursting that we saw 
in this thalamic neuron, are these single 
spikes signalling something different 
than these, than these groups of spikes, 
or these bursts. 
So, neurons are capable of firing with 
these many different kinds of outputs. 
And if we're trying to come up with a 
reduced model, we'd like to aim for one 
that would allow us to represent these 
different behaviors. 
So try to keep this range of different 
behaviors in mind as we go through 
different ideas about how to make reduced 
or simplified model neurons. 
Let's start with the simplest case. 
Let's just try to write down an equation 
from V that does something like what a 
neuron does. 
So we have a differential equation that 
looks like this, and and our task is to 
find a good function f of V that makes 
[INAUDIBLE] do what we want it to. 
So as we observe, the behavior of the 
neuron can be quite close to linear as 
long as it's not near spiking. 
So how bad would it be to assume that we 
simply have a linear neuron. 
That is, an equation such as we've found 
for the passive membrane. 
So note, from now on, that I'll set the 
capacitance equal to 1, so we don't have 
to carry constants arounds. 
So I'm drawing this case above. 
So here, f of V is simply minus a minus V 
naught. 
So how did the dynamics of such a neuron 
look. 
So here's our equation for the voltage. 
Let's, for now leave aside our input. 
So f of V is minus a V minus V naught. 
We have a fixed point at dV dT equals 
zero. 
That is, that V equals V not. 
Now, how do the dynamics look above and 
below that fixed point? 
If you have a voltage, which is on this 
side of the fixed point. 
So let's add dV dT for this value of the 
voltage is positive. 
So voltage increases, and that's true 
everywhere along this part of the line. 
On this side of the fixed point however 
the voltage dV dT is negative. 
And so anything that's out here moves 
back toward V naught. 
That's what makes V naught a stable fix 
point. 
But how do we get a neuron like this to 
fire a spike. 
We need to add in a couple of things. 
So for one thing we need to say that 
there's some threshold. 
So as I move around in V, although I'm 
always being drawn back to this fixed 
point, if I happen, because of the 
addition of some input, to be pushed up 
to some threshold voltage, what I'm 
going to do is, set this equal to the 
time of a spike, and just jump myself up 
to a maximum. 
And so, if we just plot what that looks 
like in time, we have some voltage that's 
varying along. 
It hits this value of the threshold, V 
thres, and instantaneously we're going to 
set that equal to the maximum of the 
spike. 
And the next thing we're going to do is 
take that voltage and reset it. 
We're going to take it back to some V 
reset out here, and now the input will 
continue to push it around, but starting 
at that new reset value. 
And so you can see that this mimics 
pretty well what spike trains look like. 
Let's have a look at that directly. 
So this is like the passive membrane. 
It, remember the equation that we wrote 
down earlier for the passive membrane. 
This captures that linear behavior. 
It has the additional rule that when V 
reaches the threshold, a spike is fired. 
And then, it's reset. 
And V naught is just the resting 
potential of the cell. 
So here's an example of how the 
integrating fire model works in, with, in 
response to a particular input. 
It might be hard to distinguish that from 
a real, a real spike tree. 
So while the integrating fire model has a 
lot of advantages and suddenly captures 
some basic properties of neurons, one can 
come a lot closer to the true dynamics of 
neurons. 
So, in the integrating fire model, we had 
to paste on the spike to make it 
excitable. 
How can we make this model intrinsically 
excitable? 
So what we need to do is to add on some 
more stuff to our f ov V. 
What we need to do is to give f of V a 
range where the voltage can, in fact, 
increase. 
So now, what have we done here? 
We've added in another fixed point. 
So we have here, stable fixed point. 
Now, what's up with this fixed point? 
So remember, that here, the voltage heads 
toward the fixed point. 
What's going to happen as we cross this 
fixed point. 
So now with voltages larger than, than 
this value, you can see that this dV dt 
is now positive and we're going to start 
heading out to larger and larger values. 
And so in response to dynamics like this, 
what's going to happen is that as one 
crosses this effective threshold. 
So now if you have some effective input 
that takes you above this value, now the 
voltage is just going to, now the voltage 
is just going to increase. 
So that means we still need a couple of 
extra pieces as we needed for the 
integrating of firing neuron. 
We're going to add a maximal voltage, not 
a threshold, now the threshold is 
determined intrinsically by the crossing 
by this unstable fix point of f of V. 
But now we need some maximal voltage 
beyond which the spike can not continue 
to increase. 
And when we reach that voltage, we're 
going to reset again back to some reset 
value. 
One example of form of a f of V that 
works quite well, is simply a quadratic 
function. 
So another example of a choice of f of V 
that's being shown to fit cortical 
neurons very well is the exponential 
entry of fine neuron. 
Now here, we can choose f of V, so that 
has an exponential piece. 
So that part of the dynamics sub 
threshold are linear and part have this 
exponentially increasing part that mimics 
the rapid rise of the, of the spike. 
And again we have to add a maximum and 
reset. 
So this model has an important parameter, 
delta, which governs how sharply 
increasing the nonlinearity is. 
So here's a strongly related example of a 
one dimensional model that gets a lot of 
use. 
This is called the theta neuron. 
And in the theta neuron, the voltage is 
thought of as a phase, theta. 
When the phase reaches pi, here, we call 
that a spike. 
So what's neat about using a phase 
instead of a continuous variable, like 
voltage as before, is that as soon as you 
pass through pi, you wrap around to minus 
pi and that gives you a built-in reset, 
so you don't need to add that extra part 
into the dynamics. 
So the dynamics given by, by this 
equation here. 
This is being shown to actually be 
equivalent to the one dimensional voltage 
model with a quadratic nonlinearity. 
This model also has a fixed point, V 
rest, and an unstable point, V thresh, 
which acts like a threshold. 
Now, because this model fires regularly, 
even without input, so now, let's imagine 
that It is zero. 
You can see that these dynamics are still 
regularly firing. 
They'll continue to oscillate, the theta 
neuron is often used to model 
periodically firing neurons. 
So aesthetically, lets say, we're still a 
little pained by this construction of the 
maximum and the reset, or even the reset 
on the, on the phase variable. 
Is there anything else we can do to 
improve this simple model? 
How might we prevent our spike from 
increasing to infinity, apart from 
putting some maximum on it? 
So, let's try falling. 
So what does that do? 
Now, there's another fixed point, here. 
So that we still have our stable fixed 
point. 
We have an unstable fixed point, which 
acts as our threshold. 
And now, we have antother fixed point. 
Now, is it stable or unstable? 
Let's just, let's just check it. 
So here we're increasing. 
There we're decreasing. 
Here we're increasing. 
And here we're moving back toward that 
fixed point, this is a stable fixed 
point. 
Hopefully you will sort of intuitive by 
now that you can tell whether a fixed 
point in this one-dimensional 
representation is stable or unstable, by 
just looking at the slope of f of V at 
that point. 
Whenever the slope is negative, that's 
the stable fixed point. 
And if the slope is positive, it's an 
unstable fixed point. 
So now we have this fixed point. 
What's the dynamics? 
Now once we get above our threshold, we 
increase. 
And instead of increasing without bounds, 
we go to this fixed point. 
So that's great. 
However, the problem is that it stays 
there. 
The system is called bi-stable. 
In order to allow the dynamics to come 
back from that stable fixed point, let's 
remember what happened in Hodgin-Huxley. 
Actually, two separate mechanisms helped 
to restore the voltage back to rest. 
One was that, this, one was that the 
sodium, switching of the drive toward 
this sodium equilibrium potential. 
And the other was set that potassium 
channel activated pulling the voltage 
back to what the potassium equilibrium 
potential. 
So here we need to do something similar 
to pull the voltage back toward rest. 
And that is to include a second variable 
to take care of inactivation. 
So that's done here by including this 
second variable, u. 
So u here decays linearly, but it also 
has a coupling with V. 
So this function of voltage means that 
when the voltage gets large, u is also 
driven to be large. 
Then we couple inactivation variable into 
the voltage equation. 
One would want the function G(u) to be 
negative, so that a large u, pulls V down 
again. 
So this leads us to the consideration of 
models that have two dynamical variables. 
Now, instead of drawing my f of V against 
V we need a new kind of plot called a 
phase plane diagram. 
The phase plane is just the plane to find 
by a dynamical variables V and a. 
Now, our understanding of how the model 
behaves is organized not just by 
identifying the fixed points as we were 
doing so far, but looking at the entire 
line of points, where either one or the 
other variables has zero derivative. 
So, we can define these nullclines, 
here's the V nullcline, as the line in 
which In which dV dt equal zero. 
So we set this equation equal to zero. 
That's going to give us a function of u 
with respect to V, if we solve this 
equation. 
And here, here it is. 
Similarly, there's a u nullcline, at 
which du dt equals zero, and that defines 
this other curve. 
For most neural models, nullclines have 
shapes something like I've drawn here. 
In this particular case, there's one 
fixed point that is a true fixed point, 
where both dV dt equals zero and du dt 
equals zero. 
And that's here. 
This is the resting state. 
So now we can think about what happens if 
we start out at some particular value of 
V and u. 
We're going to head out in a trajectory 
that has a velocity that has a component 
in the V direction, given by dV dt, dV dt 
evaluated at V and u. 
And at component in the u direction, 
which is going to be given by du dt. 
At v and u. 
The nice thing about these nullclines is 
they give us a sense of how trajectories 
in this two-dimensional plane will work. 
So this green curve divides parts of the 
plane in which the voltage is either 
increasing, down here on this side of the 
green curve and decreasing here. 
Whereas, the red curve divides regions of 
the plane in which u is either increasing 
or decreasing on this side of the red red 
curve. 
So if we start near rest, with an input 
that now takes us out into some larger 
voltage range, the nonlinearity in 
voltage now says that we start to move 
quickly in V. 
We're now going to undergo what will look 
like a spike. 
And now that we've crossed that green 
line, now remember that the direction of 
the voltage now changes now we're 
going to come backward. 
Wrap around this direction, move that 
way, but we still need to increase in u 
in this half of the plane. 
Now we've crossed that red line. 
Now we start to decrease and we come back 
this way. 
And so we have a spiking trajectory. 
If we now plot that, so now the voltage 
as a function of time, small and rapidly 
increases. 
Now, depending on how quickly it moves 
along this part of the no claim, it will 
gradually come back again. 
So there's an enormous amount or richness 
and fun to be had by analyzing your 
[UNKNOWN] dynamics. 
Likes in the phase plane this is a very 
simple example that can be multiple fix 
points, limit cycles, all different kinds 
of bifurcations and dynamics as the input 
changes. 
Since there's no way we can do justice to 
this in this course, I'm not going to go 
into any of this, would actually be a 
great point to branch out an entire 
second course. 
Some [UNKNOWN] dynamics in phase plane 
analysis. 
So luckily for you there's a great book 
available by Eugene Izhikevich if you 
want to explore this direction. 
The reference is posted on the website. 
There's also a lot of resources online 
by, by scholars like Wulfram Gerstner, 
Bard Ermentrout, our white knight of the 
previous slide. 
And also generally perusing through 
scholarpedia. 
What I will do, however, is introduce you 
to one final model that's inspired by all 
this richness. 
And that's the so called simple model. 
Izhikevich and others have noted, that if 
you zoom in here, to the, to this part of 
the phase plane, you can pick off the 
important dynamics that generate a lot of 
the nice behavior of real neurons. 
The voltage dynamics are approximated by 
a quadratic function, that's the minimal 
nonlinearity we need in order to, to have 
excitability. 
And there's simply a linear coupling to 
you. 
The u dynamics are taken to be linear, 
the linear both in V and in u. 
So what this would do is as V gets larger 
then that's going to drive u to get 
large. 
The coupling here is now going to 
decrease the voltage as u gets large. 
So that forms the basic role of 
inactivation. 
So this reduced model is certainly not 
complete. 
So like in the cases that we've just 
left, we've thrown away the higher order 
dynamics in voltage that allow it to 
restore itself from a spike. 
So we have to go back to putting in a 
maximum and a reset. 
So these are parameters of the model. 
The u variable also needs a reset. 
So one is left with one, two, three, four 
parameters and these four parameters 
determine the decay rate of u. 
To determine the sensitivity of u to 
changes in V and they also determine the 
reset of V and u. 
So here's a range of very different kinds 
of firing patterns from very different 
kinds of neurons, sort of being generated 
by different choices of these four 
parameters. 
So these are all model fits to different 
kinds of, of real neurons and they've 
been fit using just those four 
parameters, so you can see that you can 
get a simple regular spiking dynamics 
that you can get integrating the firing 
neuron. 
You can also get neurons that do 
intrinsic bursting that have these very 
rapid sequences of spikes. 
You see bust that are punctuated by these 
large inactive periods. 
You see fast spiking, low, low threshold 
spiking. 
You see spike frequency adaptation that 
the firing of this neuron starts off 
rapid and gets slower and slower. 
This [UNKNOWN] cortical neuron that has a 
burst of spikes and then no firing. 
And here you see something nice that you 
actually can't get it off from the 
integrated firing like neuron. 
You see subthreshold resonance. 
That is the propensity of the neuron to 
oscillate in response to an input. 
This is something that can only be 
achieved with two variable systems, 
because the two variables can play off 
against each other. 
This can't be caputred by a regular 
integrated [INAUDIBLE]. 
That was a painfully brief and partial 
overview of the world of simplified 
models. 
As you can imagine this is something of a 
mathematician's playground. 
So there's lot to be found out there if 
you'd like to do more reading. 
We're going to continue in the next part 
of today's lecture to go in the other 
direction to look at the, the Gory 
reality of neurons and try to understand 
how one can model complicated dendrytic 
algebras/g. 

In this final part of the lecture, we're 
going to continue to go beyond 
Hodgkin-Huxley, but this time, in this 
direction. 
We're going to be going back to 
biophysical reality and addressing the 
issue of geometry. 
How do the complexities of neuronal shape 
and structure affect our computation? 
In the first lecture we invested heavily 
in understanding the spike generation 
process in a patch of membre here at the 
easy end. 
But its a little embarrassing to zoom out 
and look at real neurons which have a 
truly extraordinary range of beauty and 
complexity in the geometry of the 
dendritic abras. 
So for moving toward building 
biophysically realistic models of neuro 
processing. 
It would be good to know how these 
structures can contribute to the 
processing of information. 
So, here's what we learnt to model, a 
compact cell, and here's what real 
neurons really look like. 
Here's even quite a simple version. 
So, as with the complexities of ion 
channel dynamics, what is the appropriate 
level of description of a single neuron 
that's necessary to understand brain 
operation? 
Because we don't yet know the answer to 
this, and there probably is not one 
answer to this, it's important to pursue 
models with many different approaches. 
So, here I'll be introducing you to the 
techniques that one can use to handle 
dendrites, and some ideas about what they 
may contribute to computation. 
So let's start by looking to what extent 
dendrites feel what's going on in the 
soma. 
So here, this is an impulse point that's 
being put in at the soma. 
Let's see what that input looks like when 
it reaches the dendrite. 
You can see that it's both delayed, it's 
reduced in amplitude, and it's broader 
Similarly if we put an input here, add in 
the dendrite, and we look at what happens 
at the soma in response to that input, 
you also see that it's much reduced in 
size and it's broadened out. 
Furthermore, how thin the dendrite is 
affects how big a voltage change you 
could make with a given amount of current 
input. 
The thinner the dendrite the larger the 
voltage change but generally the further 
away the the more that input gets 
filtered and attenuated. 
This tells us the inputs that come along 
different parts of the dendrite can have 
very different effects and very different 
influence on firing at the soma. 
As you can image this can have a 
tremendous impact on the information that 
is integrated and representated by the 
receiving neurons The theoretical basis 
for understanding voltage propogation in 
dendrites and axons is cable theory, 
which was developed by Kelvin in quite a 
different context. 
The voltage, V, is now a function of both 
space and time, which means that we're 
now dealing with partial, rather than 
ordinary differential equations. 
So here's the setup. 
We now think about a tube of membrane 
with sides have the same properties as 
our membrane patch. 
They have both capacitance and 
resistance. 
So we see little elements that look a lot 
like our, like our previous patch model, 
but now they distributed down a cable. 
There's additionally the resistance of 
the cable interior. 
Current can flow along the cable as well 
as through it. 
So generally we're not going to worry 
about the external medium here. 
We'll just take it to be infinitely 
conducting with a resistance of zero. 
So the cable equation for a passive 
membrane, we're not going to deal with 
ion channels for now, is derived by 
considering the changes in current as a 
function of space. 
The current down the cable will be driven 
by steps in voltage as a function of x. 
So, if we have a voltage difference 
between two points in the membrane, 
that's going to drive a current down the 
membrane. 
Of course, current is also passing out of 
the membrane. 
That's the im that we modeled previously. 
Now when one puts those 2 things togteher 
dealing with the way current flows out of 
the membrane and the way that it flows 
down the membrane 1 obtains an equation 
that is actually 2nd order in space so it 
has a 2nd derivative with respect to 
space. 
So this half of the equation you 'll 
recognize that we've seen before of the 
passive membrane now we have an 
additional term that, that includes a 
spacial derivative. 
So, some of you will recognize that this 
equation is not unlike the equation that 
describes diffusion or heat propagation, 
but it has this additional term, so this 
part looks like diffusiion, has this 
additional term that's linear in v. 
You might remember that when we rewrote 
the RC second equation for the passive 
membrane to find the time constant of the 
membrane that gave us sort of the 
fundamental time scale of its dynamics. 
So there's something very similar in this 
case too we can rewrite the equation in 
this form where we bring together all the 
dimensional quantities. 
This will ask us to read off the natural 
time scale so going with this time 
derivative there's a a time. 
Constant which is our tow M and now when 
we look at the, the spacial derivative 
this has units of 1 over space squared 
and there's a space constant out the 
front lambda that carries the typical 
spacial scale from the coefficient of the 
space derivative. 
That's given by the square root of the 
ratio of the membrane resistance divided 
by the internal resistance. 
Why is that. 
So what stops a signal from propagating 
is leakage through the cable. 
The higher the membrane resistance, the 
more current stays inside the cable and 
the further the signal can propagate. 
Not surprising the ether space constant 
also depends on the inverse of the 
internal resistance. 
The less internal resistance, the further 
the signal can go and so this 
combination, and so this combination of 
resistances gives us a typical scale over 
which the current is able to propagate. 
So now let's imagine that we're a synapse 
on a dendrite, trying to get a message 
down to the soma, so our message can help 
trigger a spike and then get handed on to 
the next guy. 
Let's see how far down the dendrite our 
message can travel. 
So here we are. 
We're trying to inject a signal into our 
dendrite at position x equals zero. 
Even if we pipe in a constant amount of 
current, we just keep shouting that 
signal, the voltage change that it causes 
in that cable will only propagate about 
as far as the length constant. 
Now, you see here, it decays 
exponentially over a scale given by the 
space constant lamda. 
Now let's put in a brief pause and see 
how that behaves at T equals 0 we put in 
a spike of input at X equals 0. 
Here's what happens to it, the input gets 
broader, spreads out spatially and also 
decays in time. 
This is a lot like diffusion. 
If you spray a pulse of perfume, 
somewhere in a room, and were able to 
watch what happened to it, it would do 
something similar. 
It would spread out with the same spatial 
profile. 
But for the perfume, there's always the 
same amount of perfume. 
If you were to add up all the molecules 
of perfume in the smeared out blob, it 
would be the same as you started with. 
For the voltage, that's not the case. 
The total voltage signal is decaying away 
in time, because of that first order part 
in the differential equation. 
That is, the total voltage is decaying 
exponentially, just as it did in the RC 
circuit. 
So what are we going to see if we sit 
some distance away, and observe the 
change in voltage? 
That's what's shown in this figure. 
So these are different curves that plot 
the voltage that's observed at different 
locations. 
X equals 0, x equals .5, x equals 1. 
These are all in units of the space 
constant of the membrane. 
At different times, and so sitting at x 
equal 0, we see an exponential decay. 
As we sit a little further away we're 
going to see that. 
That, that pulse of voltage change first 
rise, then seal the decay away again. 
So you can see how rapidly the signal 
decays as a function of space. 
So at about, at one space constant you 
still see a reasonably large Deflection 
in voltage that's caused by that pulse. 
But at two space constance away, the 
signal, the size of the, of the pulse 
that we see there is down to five percent 
of the original. 
You can also track the peak of this 
potential pulse and ask how rapidly does 
this signal propagate down the cable? 
It turns out that that gives us a 
velocity of propagation of the signal 
which goes like, see if you can guess, 
so, what would you expect to be the 
propagation rate down a cable? 
Let's call it c. 
It turns out that it's equal to two times 
the space constant over the time 
constant. 
So, what's the typical spacial scale? 
It's just lamda. 
Typical timescale is tow and the 
propigation velcoity turns out to be the 
ratio of those two of those two 
quantities. 
So for some of you who like to see 
general solutions here's how the voltage 
responds to a pulse of input at time T 
equals zero and position X equals zero 
looks at position X and time T. 
We can see that this solution is made up 
of two parts. 
So here's the diffusive spread for this 
Gaussian profile. 
That's very similar to the way things 
spread diffusively. 
And mutliplying that, there's this 
pre-factor that has an exponential decay. 
So, knowing the solution, one could take 
some arbitrary pattern of inputs, 
decompose it into pulses, as we, as we 
put in here, at different central 
locations. 
Say, T prime and X prime, and add 
together a weighted sum of this solution 
form. 
We've centered up those differnt 
locations and times. 
So now we know how to find solutions for 
a very long pass of cable with a fixed 
radius. 
In fact doesn't get us very far in 
dealing with the real neuron, because of 
two things, the inter current branching 
of dendrites and the fact that many 
dendrites are not passive but are active. 
That is, they have ion channels in them. 
So it quickly becomes very difficult to 
solve anything analytically. 
The path forward is by dividing the 
dendritic arbor into what's called 
compartments. 
So here's an example, one can approximate 
the dendritic arbor as a coupled set of 
compartments. 
So these, we're going to break this 
dendrite into little sub regions In which 
the radius and the ion channel density is 
taken to be constant. 
Each compartment will then have an 
equation that only depends on the time 
derivative of the voltage, and not on x. 
The spatial dependence is incorporated by 
coupling each compartment together. 
So, so [INAUDIBLE] Rall device is a 
helpful way to approximate complex 
dendritic trees of. 
Let's consider a branch that divides into 
2 daughters. 
It turns out that if the diameters of 
these two branches have this particular 
relationship to the diameter of their 
mother so if the if the diameter of 1 of 
them raises to 3 halves plus the diameter 
of the other also raised to the 3 halves 
is equal to the diameter of the mother 
raised to the 3 halves. 
What that means is that these two 
branches are impedance matched to this 
branch, and one can simply accumulate 
them all together into one long branch of 
the same, of the same diameter as the 
mother. 
So one needs to thus compute the 
effective electrotonic length of these 
two additional branch elements. 
And extend the original cable by that 
much. 
So you can see that one can continue to 
do this iteratively. 
If the same property holds here, lennox 
can extend that branch out to an 
effective branch of this, of D2 diameter. 
And then one can agormate those two 
together. 
Until one eventually has a single, a 
single cable coming out of the soma. 
So it turns out that this condition on 
the diameters it approximately satisfied 
by real dendrites and even when it's not 
exact the resulting approximation is 
often quiet accurate. 
So the role models are very useful for 
passive membranes But it doesn't address 
the issue of ion channels which make the 
problem nonlinear. 
Furthermore ion channels densities often 
very along dendrites which can lead to a 
lot of interesting effects that one might 
like to explore. 
so here's the full approach, given the 
geometry and the ion channel density of 
the dendritic tree One can divide it 
where the properties are approximately 
constant. 
One can then write down equations for the 
membrane potential in each compartment 
individually. 
So let's say compartment 1 we represent 
here in terms of a, a similar second 
model that we saw for the passive 
membrane we're going to give that the 
voltage V1 and now right down an equation 
for V1. 
We can do the same for compartment two 
and compartment three, here. 
These equations will be similar to the 
passive membrane equations we looked at 
for Hodgkin-Huxley, but with the 
individual ion conductances, membrane 
resistance and capacitance set 
appropriately for each piece of the 
cable. 
Furthermore, there'll be also two tons 
that couple the compartment with it's 
neighbors. 
The current input from the neighboring 
compartments. 
Which depends on the voltage difference 
between the two compartments and a fixed 
coupling conductance. 
So here for example, let me write down an 
equation for for v two. 
We're going to need to include a current 
that's coming from, from compartment one 
that's going to go like g one, two 
multiply it by V1 minus v2. 
And similarly, there'll be a current that 
comes into compartment one from 
compartment two that's going to have a 
different coupling conductance and the 
opposite voltage difference. 
These fixed turns, these coupling 
conductances, depend on the area of the 
connection, and whether or not the 
compartments straddle a branching point. 
So if you notice, that in general, these 
coupling conductances are not necessarily 
symmetric, which is why there are 2 
values at each of these connections. 
So there are many models like these that 
have been built from microscopic 
reconstructions of single neurons. 
And a great many have been made publicly 
available on the ModelDB site maintained 
at Yale. 
So if you're in the mood to go explore a 
dendritic forest, there's plenty out 
there, so don't forget your adventure 
hat. 
So what do den-, so what do dendrites add 
to neuronal computation? 
There are many proposals for ways in 
which the filtering and active properties 
of dendrites can work, to shift the way 
in which incoming information is Clearly, 
where an input arrives on the tree can 
influence the effective strength of the 
input because of the passer properties. 
Interestingly, it's been found that in 
the hippocampus, neuronal dendrites have 
solved this problem. 
So that when inputs arrive at the suma/g, 
they have a very similar shape no matter 
where they come in. 
This amazing property is known as 
synaptic scaling. 
Filtering through the dendritic tree on 
the way to the sonar also determines 
whether a sequence of successive inputs 
is integrated to build up to potentially 
drive the spike or not. 
Where two different inputs enter the 
dendritic tree can also make a huge 
difference in how they interact with each 
other. 
For instance, if two inputs come in on 
separate branches. 
They contribute independently. 
While if they are on the same branch, 
they can sum either sublinearly or super 
linearly which leads to amplification. 
Another very important property is that 
thanks to their ion channels, dendrites 
can generate spikes, generally calcium 
spikes. 
[INAUDIBLE] This leads to the possibility 
that one could use coincidence of inputs, 
driving spikes in the dendrites. 
Along with back propagating spikes from 
the soma back to the dendrites to drive 
synaptic plasticity. 
This is a topic you'll be hearing much 
more about in the next lectures. 
So, let's close out by looking at two 
ideas for how dendrites might perform a 
computational role. 
The experimental evidence supporting 
these mechanisms is somewhat mixed, but 
the fundamental ideas stand as 
interesting conceptual models. 
First, here's a wonderful example where 
the propagation of signals through cables 
is thought to help out in carrying out a 
computation. 
Nuclei in the auditory brain stem are 
responsible for sound localization, the 
ability that we all have to locate where 
a sound is coming from. 
The que that's thought to be used is the 
timing difference in the arrival of a 
sound at our two years. 
The sound arise at the two ears at 
slightly different times, and the signal 
those travels through the left and right 
auditory pathways at slightly different 
times. 
Imagine that these two signals are piped 
into the population of neurons The 
thresholds are set such that they can 
only fire, when coincidence signals from 
two different inputs arrive at the same 
time. 
Each neuron receives the two inputs with 
a delay caused by traveling different 
distances along the dendrites. 
The neuron that fires the most is the one 
for which the relative timing delays, due 
to the timing difference between the two 
ears Is compensated for by the dendritic 
delay. 
This mechanism turns a tiny difference 
into a place code where by the label of 
the neuron that fires, indicates the 
timing delay, which can then be 
translated into the spatial location of 
the sound. 
Here's a final example Neurons in the 
retina show direction cell activity. 
That is, they respond to individual 
stimulus moves in one direction, and is 
suppressed when it moves in the other. 
So how might such direction cell activity 
begin constructed at the single neuron 
level? 
Imagine that inputs from different 
spatial locations are coming in to a 
dendrite at locations along the dendrite, 
arranged as in space. 
As the dendrite receives a sequence of 
activations, you can see that if they 
receive that input first, at the far end 
of the dendrite, and it begins to travel 
toward the soma, then another input comes 
in, closer to the soma. 
Then the influence of these two inputs 
can sum and build up as more and more 
inputs arrive. 
So that the net input crosses some 
threshold. 
On the other hand if the nearby location 
is stimulated first, then the next one 
and then the next one and then the first 
inputs will die away by the time the 
later ones arrived. 
And you can see how, how that, how that 
voltage signal at the soma might behave. 
So this idea was first proposed by Rall. 
While it may not fully explain direction 
selectivity and retinal ganglion cells, 
the general idea does predict that the 
firing probability of the neuron should 
be sensitive to the sequence of inputs 
along the dendrite. 
Let's say these inputs occur in some 
sequence. 
One could then scramble the order, and 
see whether the firing of the neuron can 
distinguish those inputs. 
Michael Hausser's lab carried out this 
experiment by simulating sequences of 
synaptic inputs into single neurons and 
found that indeed different input 
sequences were discriminable. 
Okay so here's where we wrap up my 
section of the course. 
I hope you've enjoyed this brief 
introduction into the electrical basis of 
neural signaling in the brain. 
Roshesh is going to take it from here, to 
guide you through the ways in which these 
basic cellular components get wired up 
together, to produce the amazing variety 
amount of behaviors that I know the 
systems are capable of, through 
experience and through learning. 
Have fun. 

[MUSIC]. 
I'm happy to introduce our next guest 
lecturer. 
That's Eric Shea-Brown of the Department 
of Applied Mathematics here at the 
University of Washington. 
So Eric did his PhD at Princeton, with 
Jonathan Cohen, and with Phil Holmes. 
On the Neurodynamics of Cognitive 
Control, and after that, he did a 
post-doc at NYU under the mentorship of 
John Rendall/g. 
Eric and his wife are avid skiers and 
hikers. 
They're great cooks and they have an 
adorable one year old son. 
Eric's work and that of his group 
concerns the relationship between neural 
dynamics and coding. 
And they're particularly interested in 
issues like decision making, chaotic 
dynamics and neural circuits and also 
correlations. 
And correlations is the topic he'll be 
talking to you about today. 
Thank you, Eric. 
 >> Thank you very much for the 
introduction. 
So I'm going to talk to you about 
representation of information in large 
Neural Populations. 
The title is Correlations and Synchrony. 
So when we think about how representation 
of a signal, say something that's in the 
sensory environment, in the spike 
responses of single cells. 
A picture like that which you see on the 
screen here, comes to mind. 
This is from famous nobel prize winning 
work, in this 60s. 
the idea is there's something in the 
sensory environment. 
Again, here, the orientation of a visual 
signal. 
You can see that changing there on the 
left. 
You record form a signal cell, here a 
cartoon of what you might see from the 
visual cortex. 
And as that feature of the sensory 
environment changes, something about the 
way the spikes you see on the right 
changes, what is that something? 
Famously, that's the rate at which the 
spikes are produced, the simplest 
statistic perhaps, that you could 
imagine. 
And there's been enormous progress in the 
field from quantifying being this change 
in rates via something called a tuning 
curve. 
So here, in the bottom you can see firing 
rates as a function of the sensory 
variable. 
itself the angle, of this particular 
visual stimulus of varying in some 
systematic way. 
Okay, so that's the first statistic, that 
matters in terms of how cells respond. 
In terms of representing information. 
We gave an example from visual 
neuroscience. 
But we see these type of tuning curves 
and the covariation of firing rates with. 
Again, something that you might imagine 
the nervous system wanting to encode 
information about. 
in a wide variety of different settings. 
One almost 100 years old in terms of 
proprioception. 
motor neuroscience and other examples 
listed at the bottom of the screen. 
So here we go, we're off. 
We're talking about statistics of 
responses of cells. 
and again, how those represent signals. 
What other statistics beyond the rate 
might we be interested in? 
Well, first, if we give some sort of 
stimulus which elicits, on average, say, 
a 10 hertz response or so, we count 
spikes in half a second. 
On one trial we might see indeed the 
average occurring 5 spikes occurring, but 
on other trials we'll see very different 
responses. 
This is not a clockwork type of system. 
Looks a lot more like popcorn or a Geiger 
Counter. 
Variability or variance as represented 
say in a pluson process. 
So, what do we have so far? 
We've got a bunch of neurons, we look at 
one of them. 
We have a tuning curve, that's the mean 
response as a sensory variable changes. 
We also have variability or variance 
around those mean, that mean. 
two statistics so far, on our journey 
towards describing population responses. 
We can do that for one cell, we can do it 
for another. 
Let's grab this blue one over here, and 
we can see that we can quantify similar 
statistics, firing rates and variance and 
function of the stimulus. 
Is that all there is to it? 
Or can we just repeat this procedure 
going one cell at a time, describing 
possibly different properties of mean and 
variance in their responses. 
Or is there something more there, in the 
neural population, beyond that which we 
could deign by looking at one cell at a 
time. 
Well the simplest way to get at that 
question is to look at pairs of cells at 
once and ask whether again, in these 
paired responses. 
There's more there than you see in just 
one cell at a time. 
Quantify it as follows, put down some 
time window of length T, measure from a 
couple of cells, upper cell here. 
Another cell, grab it, produce the 
response, down there. 
Close that window. 
and simultaneously measure how many 
spikes these two cells produce, right? 
So here in our window of time the first 
cell produced two spikes, second cell 
three. 
Slide that window of time along see what 
happened, okay? 
Label these spike counts n1 and n2 for 
the two cells and ask, well, again, is 
there more there in those two cells 
responses than I could have seen one cell 
at a time? 
How do I do that? 
Measure something called the correlation 
coefficient or the Pearson's correlation 
coefficient. 
That's just that covariance of these two 
spike counts, divided by their variance. 
And you ask, well, are these cells 
covarying or not? 
What is this number, is it zero or 
something nonzero? 
By now we have many studies which 
indicate that this correlation 
coefficient is significantly nonzero. 
Now, there are some interesting cases in 
which these correlation coefficients do 
seem to be 0. 
but by and large, again, there are a 
large number of examples all the way from 
the input and of the nervous system to 
the output. 
Where we see significant departures from 
independents of the cells, again, 
quantified by non 0 correlation 
coefficients row. 
Okay, so that it looks like we need to 
keep going in our effort to describe what 
neural populations do. 
We can't just look at one cell at a time, 
there's more in the joint or co-bearing 
activity of these two cells. 
That could be discovered by looking at 
one at a time. 
But what we haven't yet established is 
whether that's just some factoid about 
the way cells fire. 
Fine, they happen to go at the same time, 
spike at the same time, with a little bit 
more prevalence than you might expect by 
chance. 
but that does that actually matter for 
the way that they encode information? 
Encode, for example, the type of simple 
sensory variables that we have been 
looking at so far. 
That's the question, who cares, at the 
bottom of the screen. 
So, there, this has been, studied. 
And also, reviewed, you see a review 
paper here. 
Averbeck et al, Nature Reviews 
Neuroscience '06. 
and studied and reviewed in the context 
of, of neuroscience by a large number of 
authors. 
And I want to give you a sense of, what 
the type of results that have been 
established. 
seem to be pointing to, so, let's look 
again at the responses of two of our 
cells, our friends, the blue and green 
neuron from before. 
in response to a particular sensory 
variable, so a drifting grading, say, 
with an orientation that you see is is 
diagonal. 
As indicated by my lollipop in the bottom 
of the screen. 
Okay, so let's talk about the mean 
responses, that these two cells produce. 
they're both firing at some reasonable 
rate. 
the blue and the green cell together. 
And if you would make some sort of plot 
where on one axis we have the spike count 
coming out of cell one. 
The other the spike count coming out of 
two cells cell two, we would get some 
sort of a point on average in this. 
in this two-dimensional space for the 
mean responses of these two cells. 
Now if we on top of that, were to make, 
we know it's probably wrong 
But we were to make the assumption that 
these cells are statistically independent 
of one another. 
Then their variability would be spread 
around that mean in some some roughly 
circular way, okay? 
Fine, so this would be a picture of the 
cloud of responses that I get out of 
these two cells. 
Under the assumption that they are 
independent of one another, not 
correlated, okay? 
Now, I present another stimulus, so my 
lollipop has moved over by a little bit. 
And my stimulus is now a little bit more 
horizontal. 
What's going on both of these cells 
respond with a slightly higher firing 
rate right? 
So, the mean of the distribution has 
moved up in this two dimensional space. 
But we're still assuming in an in a 
roughly independent way so the response 
distributioner cloud is still roughly 
circular, okay? 
So those are my two clouds of responses 
elicited by stimulus one and stimulus two 
in my pair of neurons. 
Under the assumption that these spike in 
an independent way. 
Hm, now, what if these cells were not 
independent of one another and they 
tended to be correlated in a positive 
way? 
In other words, their responses tended to 
covary. 
They're still variable, this cloud is 
extended, okay? 
But what happens to occur, is that both 
cells tend to fluctuate or tend to do 
about the same thing. 
They tend to have similar correlated 
noise, or correlated variability. 
That means that these responses cluster 
towards the diagonal. 
Again, where cell 1 and cell 2 are doing 
approximately the same thing, okay? 
So, under that correlation assumption, 
right, my response distribution has gone 
from a European football to an American 
football. 
It is more concentrated, more elongated. 
My response distribution is going to look 
something like this for stimulus 1, and 
for stimulus 2, same thing, right? 
The mean, once again, shifted up. 
R axis in both directions, but we 
maintain our correlation, so that the 
response distribution again is expanded 
or elongated like an American football. 
Fine, so that's my picture, do I care? 
Well, let's take the organism's 
perspective, as the saying goes in the 
research literature and think about 
trying to look at the responses of these 
two neurons. 
And determine or decode which sensory 
stimulus was given. 
Was it the more diagonal one or the 
flatter one? 
Well you can certainly tell that that 
task is going to be much more difficult 
in the presence of these correlations. 
Because these two response distributions 
overlap more. 
The conclusion? 
Correlations can degrade the encoding of 
neural signals, okay? 
So, we saw this result for two cells. 
Now it turns out, this is not just a 
finding auh, that holds for pairs of 
neurons. 
If I look at large groups of cells, say, 
M cells with identical tuning curves, 
there's a famous argument. 
over a paper of Zohary, Shadlen, and 
Nethor, Newsome that makes the following 
point. 
Let's compute the signal-to-noise ratio 
of the output of all M cells at once. 
What's that? 
That's just a mean response divided by 
the variance of that response. 
Okay, so this signal-to-noise ratio is 
going to grow with M as I include a more 
and more cells into the population. 
Let's be careful there. 
should I be the mean, divided by the 
variance or the mean divided by the 
standard deviation 
That will grow with M if we have M 
independent cells, then the mean will 
grow with M, and the variance will also 
grow with m. 
So this is going to be something which 
grows in time, is going to be the mean. 
That's where it grows with the number of 
neurons you include in the population 
will be the will be the mean divided by 
the standard deviation. 
So there's a typo on the slide. 
Okay, anyway, we have some measure of the 
signal-to-noise ratio. 
This is growing with am, include more 
cells in the population that are signal 
noise ratio. 
Does this make sense? 
absolutely this makes sense. 
it's just like doing an experiment over 
and over again, or flipping a coin even, 
over and over again. 
The more times you do this, if you take a 
look at the aggregate response, it will 
have a smaller ratio of the size of the 
fluctuations. 
As opposed to the, again, the aggragate 
response, the mean response. 
Repeat an experiment many times, 
aggregate the data. 
You get a more accurate result, okay? 
So, this is the type of thing you see if 
all of the cells are statistically 
independent of one another. 
Do more, include more, get more 
information out. 
But what do you see, as you include 
correlation among these variables? 
So, here's our friend the correlation 
coefficient row again, before it was 
zero, all these cells were independent of 
one another. 
Now we increase this correlation 
coefficient, it goes from 0 up to 0.1. 
And you see something quite interesting 
happening to this signal noise ratio. 
Looks like it saturates even with a 
relatively wimpy correlation coefficient 
of one part in ten. 
So this is the same picture. 
This is code fluctuation or commonality 
in the response. 
in the responses of these cells, giving 
us a noise term that cannot be averaged 
away as I include more and more cells in 
the population. 
The consequence of this is a limitation 
on the signal, a noise ratio. 
A reinforcement of our overall point that 
we already saw in these perhaps easier to 
understand bubble pictures up at the top. 
Positive correlation giving you more 
overlapping responses, giving you less 
information, a bad news story. 
Now, some in the audience probably 
already thinking about this option. 
Is this bad new story the only one we can 
ever read? 
And the answer is no. 
What if I have my friends the blue and 
the green cells arranged as follows. 
Still the same two stimuli are presented. 
But now these cells have less similar 
tuning curves. 
So, that, notice please, when you go from 
stimulus one to stimulus cell, the green 
cell displays a lower firing rate. 
But when you go from stimulus one to 
stimulus two, the blue cell displays a 
higher firing rate. 
Well, What are my clouds of response 
distributions going to look like? 
Well, in this case, one of the cells has 
a higher firing rate, but the other cell 
has a lower firing rate as I go from one 
stimulus to the next. 
And my two response distributions will be 
arranged across the main diagonal like 
this. 
Now, you can guess what's going to happen 
when you introduce positive noise 
correlations. 
There we go, these two responses become 
more elliptical, exactly as before. 
But in becoming more elliptical they now 
become less overlapping or easier to 
discriminate. 
The conclusion's in the box here. 
Correlation can have a good news effect, 
as well. 
So if we sum up what we learned here, 
right? 
These are the two examples. 
and when we were trying to answer the 
question of, who cares? 
about the fact that I see positive 
correlation or nonzero correlation, I 
should say, in many places in the nervous 
system. 
We saw that there were a number of 
different options. 
There was this bad news story, right, as 
highlighted by this famous paper in the 
l, talking about large group of m cells. 
Or in our simple lips picture here, a 
decrease in information when cells tend 
to be more homogeneous or have similar 
response properties in their means. 
A good news story where if the cells are 
sufficiently heterogeneous with respect 
to one another. 
The presence of these correlations could 
increase the detectability of the two 
different signals, the discriminability 
of the two different signals. 
Now, it's also there's another possible 
angle on this good news story. 
And that's that the neuropopulations 
could be correlated in a way that 
covaries with the stimulus. 
Say stimulus A gives you uncorrelated 
response, stimulus B gives you correlated 
responses in an extreme limit. 
With nothing else about the single cell 
responses changing at all. 
Then the presence of the correlations, or 
the synchrony, could be actually carrying 
information. 
That's another channel, you can say, in a 
very informal way by which a signal could 
be carrying. 
It's an attractive idea that's been put 
forward in more sophisticated ways than I 
offered in the literature. 
And there could be a no news story as 
well, right? 
With these varied impacts of correlations 
on encoded information, and you can 
imagine multiple of these different 
impacts being present in different 
In different competing ways at once so 
that there's not much of an impact. 
or perhaps the effects themselves do to 
the correlation coefficients being less 
extreme than I've demonstrated in my 
ellipse pictures. 
Or due to the population sizes being 
small the effects in the cells being 
smaller rather insignificant. 
So the full range of different marquee 
headlines can occur when we think about 
what the possible roles are of 
corelations. 
at the paralyze level here in terms of 
representing, representation of 
information around populations. 
And this is still an area in active 
study, and under active debate. 
Okay that's the debate in blue. 
Now I'd just like to close by, mentioning 
an area in which many parts of the field 
the research field are moving now. 
and that's best done by summing upward 
come so far. 
So, we've seen for many cases, not all of 
them, many cases in the literature, we 
can't describe responses of a neural 
ensemble here. 
It's just an ensemble of two cells. 
By thinking about the two cells 
independently, we have to think about 
these two cells as a unit, as a possibly 
correlated unit. 
Well hang on, that's just, thinking about 
2 cells at once, but what happens when I 
think about 3 cells at once. 
It's something that's really different 
there? 
Is there an analog of the American 
football being different from the 
European, or the International, football. 
that occurs when you go from three two 
cells to three. 
And how about from three cells to four? 
And when is this story ever going to 
stop? 
Now, this is a question that's been 
around in neuroscience for a long time. 
Here are some of the references, and the 
ideas go back, not surprisingly, even 
before that. 
But these type of questions have really 
come to the floor even more strongly with 
an increase in this scope and scale of 
array type of recordings. 
Here's a famous, pe, paper from the 
research group of E.J Chichilnisky. 
at the Salk Institute in which a ballpark 
of 100 or so cells are recorded 
simultaneously. 
We really have to think about the 
statistical scale, at which we would 
describe those cell populations, when 
we're faced with these type of data. 
An exciting question. 
How are you going to do it? 
How are you going to describe the 
response of this entire ensemble. 
How are you going to build up the 
probability distribution and not just 
over n1 and n2, as we had in my blue mare 
example before. 
But over that which contains all n cells. 
And this is more than just an academic 
question as these authors at the bottom 
of the page for example have emphasized. 
when we think about simply the practical 
process of doing this and trying to build 
up this probability distribution over n 
cells. 
Think about it, how many different firing 
rates are first order statistics? 
How many different tuning curvers would I 
have to describe? 
Well, that's going to be n, right? 
Because, we've got n cells. 
But, how many different pairwise 
combinations are there n squared? 
Again, these are really the arguments of 
these authors down here, Schneidman and 
Shlens, they're colleagues. 
How many triplet interactions are there? 
Well, n cubed, quartuplet, on and on and 
on. 
And if N is set at reasonably large, this 
is the appropriate time to make some sort 
of a galactic metaphor, but you get the 
picture. 
We need a intelligent way of doing this, 
of thinking about these population-wide 
statistics which is not a brute force 
enumeration of all the joint statistics. 
There are too many of them to write down 
let alone, 
Other problems that come about with 
thinking about such a complicated 
probability distribution. 
How are you going to do this? 
There is one, there are many, different 
approaches, I should say. 
There's just one I want to close with. 
This is my very last slide. 
It's this, here's an approach. 
Let's talk about this full probability 
distribution over n cells is what 
actually happens. 
What if I tried to based on this complete 
description of all of these cells. 
Build up my best possible estimate of 
what happened in all of those n cells by 
pretending that I could only observe 
pairs of cells at once, right? 
So I'm saying look, I went through that 
whole first part of that tog. 
I got what this guy was saying about 
these paralyze correlations and these 
ellipses. 
Let's just try to extend this type of 
description to the population as a whole. 
What I would get is some model which 
we'll call P2. 
Again, the best possible description 
based on just looking at pairs of cells 
at a time. 
Okay? 
So, again, if I look at just, at most, 
two cells at once all I know is how 
quickly they fire and how correlated all 
of those individual pairs are. 
Mm, okay? 
And then I minimize any further 
assumptions about the way those cells are 
interacting with one another. 
That's equivalent to something called 
maximize the entropy, this is absolutely 
not my idea. 
This goes back to Jaynes and perhaps even 
further. 
And has been advanced in the neuroscience 
literature by all of these other authors 
who you see listed at the bottom of the 
page as well as many others. 
But one idea is, again, to build up this 
P2 under that assumption. 
Under this minimal assumptions model. 
That leads to a particular probability 
distribution across the whole ensemble. 
A P2 that looks like this, it has the 
following special form. 
We're obviously not going to derive that. 
The references are here, it's a 
reasonably doable, but also a more 
advanced topic. 
but the bottom line is there's something 
concrete to compare with in answering the 
question, is there more there, than what 
is present at the level of pairs? 
The answers in research community are 
mixed. 
and interesting, this is a contemporary 
area on the frontier and I'm looking 
forward to seeing what we all learn, as 
the field moves in these and other 
complementary directions in the future. 
We'll stop there. 

[MUSIC]. 
A big hello to all of you from sunny 
Seattle. 
This is week six of Computational Neuro 
Science. 
This week, we learn how to model on a 
computer, those connections in our brains 
called synapses. 
We'll also learn how to model networks. 
Of connected neurons, but before we do 
that let's make a pit stop and reflect on 
some of the highlights of our journey so 
far. 
At the very beginning of our journey, we 
learned about neurons, synapses and brain 
regions. 
This was in week number one, when we did 
our neuroscience review. 
Adrian then told you about a class of 
descriptive models known as neural 
encoding models. 
And you learned about the spike triggered 
average as well as covariance analysis. 
And you also learned about the Poisson 
model of spiking which describes how 
neurons fire in a stochastic manner. 
The following weeks we covered Neural 
decoding methods. 
Which allowed you to discriminate between 
stimuli based on neural activities as 
well as decode stimuli from populations 
of neurons. 
And, we learned about Information Theory 
and how it's related to neural coding. 
In the previous week, we shifted gears 
and Got into mechanistic models and 
particularly we looked at single neuron 
models. 
And we covered concepts such as the RC 
Circuit model of a membrane as well as 
the famous, Hodgkin-Huxley model of how, 
the action potential is genrated in 
neurons. 
And we ended with simplified neuron 
models such as the Integrate and fire 
model. 
Of a neuron this leads us to the question 
of how can we model neurons that are 
connected to eaach other in networks. 
So how do enurons connect to form 
networks, you know the answer,they use 
synapses in particular we are going to 
focus on. 
Chemical synapses because there the most 
common type of synapse found in the 
brain. 
What do these chemical synapses do? 
Well, as you know, when there is spike 
that arrives from the first neuron. 
So we're going to call this the 
pre-synaptic neuron, and this the 
post-synaptic neurons. 
The spike causes some chemical to be 
released into the space known as the 
synaptic cleft and this chemical are in 
turn are going to bind with some 
receptors on the post synaptic membrane. 
And that in turn is going to cause either 
an increase or a decrease in the membrane 
potential of this. 
Postsynaptic neuron how does it happen? 
Let's first review what happens in the 
case of a excitatory synapse. 
So in the case of an excitatory synapse, 
when you have an input spike you get the 
neurotransmitter release. 
In this case it would be glutamate, which 
binds to receptors in the post-synaptic 
membrane, and that in turns causes ion 
channels to open. 
So you could have ion channels that open, 
which in turn cause positive ions such as 
sodium to come inside the cell. 
And that in turn is going to cause a deep 
polarization which basically means you 
have an increase in the local membrane 
potential of the neurons. 
We have an increase in the local membrane 
potential, and that excites the cell. 
On the other hand, in the case of an 
inhibitory synapse, you have the input 
spike releasing neurotransmitters into 
the synaptic cleft and this could be a 
neurotransmitter such as GABA A or GABA 
B. 
And this binds to receptors, again, in 
the post-synaptic membrane. 
And that in turn causes some ion channel 
to open, and this could result in either 
chloride coming in, or you might have 
positive ions such as potassium leading 
to cell. 
And that in turn cause hyper polarization 
or a decrease in the local membrane 
potential given by these negative signs 
over here. 
And so that's the effect of an inhibitory 
synapse. 
Now, what we want to do is 
computationally model the effects of a 
synapse on the membrane potential V of a 
neuron. 
So, here's a cartoon of what we want to 
do. 
Here's a synapse. 
And, we would like to model the effects 
of input spikes. 
As they're transmitted by the synapse on 
to the membrane potential V of a neuron. 
So, how do we do that? 
I'll let you think about that for a 
couple of seconds. 
Let's start by looking at the RC circuit 
model of the membrane, which you heard 
about last week's lecture. 
As you recall we were modeling the 
membrane in terms of a resistance and a 
capacitance so here 's the membrane 
voltage as you recall there is a net 
negative charge on the inside compared to 
the outside of the meembrane. 
And we were also allowign for some 
current I sub E to be injected into this 
ball, which is approximating a neuron. 
Now here's the circuit diagram for the 
same situation and you have both the 
membrane capacitance and the membrane 
resistance shown here, along with the 
equilibrium potential of the neuron 
denoted by e sub l. 
Now how do we model such a circuit? 
Well if you go back to your physics class 
in high school, you will recall that the 
charge held by a capacitor is given by q 
equals cm, in this case the membrane 
capacitance. 
Times the voltage across the capacitance 
so Q equals CmV now if we take the 
derivative of this equation with respect 
to time 2 dq dt that is nothing but the 
current coming into the cell and that is 
given now by Cm dv dt.Now this equation. 
Cm dv dt equals i can be written in this 
particular manner by using the fact that 
we have an input current i sub e divided 
by a, which is the input current per unit 
area as well as the current due to the 
leakage of ions. 
This is the current due to ion pumps if 
you recall that maintains this 
equilibrium potential E sub L. 
So the E sub L if you recall the 
equilibrium potential was something 
around minus 70 millivolts also called 
the resting potential of the neuron Now 
given this equation, Cm, dv/dt equals 
this current that's coming into the cell. 
You can now multiply both sides by the 
resistant R sub m. 
This is also called the specific membrane 
resistant, and this little C sub m, is 
the specific membrane capacitance. 
Then the equation that you'll get, looks 
something like this. 
So now what we have is the product RM 
times CM and that is something called the 
membrane time constant, tal sub M, and 
that in turn is also equal to the total 
membrane resistance times the total 
membrane capacitance big RM times the big 
CM. 
And they related to each other in this 
particular manner by the surface area of 
the cell. 
And so this equation here is describing 
how the membrane behaves as a function of 
time as you inject some input current 
into the cell. 
Now what is this equation really telling 
us about the membrane? 
Well, here is time, and here is the 
volatge as a function of time. 
And so if you, start out, at some 
particular value, let's say at 
equilibrium, so that is given by EL. 
Then if you inject some current into this 
neuron. 
This equation tells you that the voltage 
is going to raise to some particular 
level and it will stabilze at some 
partiuclar level as long as you are 
injecting the same current and that value 
is going to be the steady state value so 
thats Vs s at some particular value here. 
And that Vss, the steady state voltage, 
is going to be equal to whatever is the 
value that we gets when you said dv/dt 
equal to 0. 
So let's set dv/dt equal to 0 and what 
you're going to get then is minus V minus 
e l, plus i e. 
Rm is equal to zero and if you solve for 
V you're going to get EL plus IeRm as the 
voltage that the cell converges to, 
that's the steady state voltage of the 
cell. 
Now if you turn off [SOUND]. 
The input currents, set that equal to 0. 
Then what are you going to get? 
Well, you're going to get an exponential 
detail back to the equillibrium 
potential, EL of the cell. 
the membrane times constant tau, and 
plays an important role in determining. 
How quickly the cell reacts to changes in 
the input. 
So, for example, if tau m is very large, 
then the cell will take a long time to 
converge to the steady-state value. 
And, similarly, when you turn off the 
input, it will take a long time to 
converge back to the equilibrium 
potential. 
On the other hand, if you have a small 
time constant for the membrane, then the 
cell will react quickly to inputs, and 
it'll converge quickly to the steady 
state value. 
And when you turn that input off, it'll 
quickly converge back to the equilibrium 
potential. 
It might be fun to make an analogy here. 
When you wake up in the morning, you 
might find yourself a bit sluggish and 
slow. 
And a bit slow to react to new inputs and 
that's when we could say you have a large 
time constant. 
But after you've had your first few cups 
of morning coffee, you might find 
yourself alert and fast. 
And one could then say that you have 
changed your time constant to being the 
tiny value. 
Okay, so perhaps that analogy was bit 
corny. 
Well, in any case how do we model the 
effects of a synapse on the membrane 
potential V, Now that we know how to 
model the membrane potential using the RC 
circuit model. 
So what do synapses do? 
We know that synapses release 
neurotransmitters which in turn cause 
ionic channels to open or close and that 
in turn changes the membrane potential of 
this whole synaptic cell. 
So what we really need to do is to be 
able to model the opening and closing of 
ionic channels. 
On the membrane. 
So given that we have a model of the 
membrane potential, how do we model the 
opening and closing of ionic channels? 
Well here's a hint, remember the 
Hodgkin-Huxley model? 
So in the Hodgkin-Huxley model you had to 
model the opening and closing of 
potassium and sodiaum channels. 
And you did that by adding these 
additional conductance to model the 
opening and closing of sodium and 
potassium channels. 
So can you do something similar for 
synapses, which in effect also open and 
close certain channels? 
The answer as you might have guessed is 
yes, we can model the effects of a 
synapse on the membrane potential by 
using a synaptic conductance. 
And that is given by g s. 
And the other component of the synapse 
model, besides the conductance g s, is 
the reversal potential or the equilibrium 
potential of the synapse. 
And so here is the equation again, so we 
have tau m dv dt equals the first term is 
the league term as in the previous slides 
but then here is the new term that is the 
input coming in from the synapse. 
So we have the term corresponding to the 
difference between the current voltage 
and the equilibrium. 
Potential of the synapse as well as the 
conductance which is going to change as a 
function of the inputs being received by 
the synapse and finally of course we have 
the input current which is optional so if 
we have input current we can model that 
by adding this additional term so the 
important point here is that. 
For the synapse model we have these two 
components the Gs and as well as the Vs 
and so for an excitatory synapse yu can 
imagine the Es is going to be a value 
that is higher than the equilibrium 
potential of the cell which is going to 
excite the cell on the other hand for an 
inhibitory synapse. 
The ES is going to be a value lesser than 
the equalibrium potential and that's in 
turn going to decrease the membrane 
potential. 
So how does the synaptic conductors GS, 
change as a function of the inputs 
received by the synapse. 
So you could have these spikes coming in, 
and that in turn is going to change the 
synaptic conductance. 
So how do we model the effects of input 
spikes on the synaptic conductance? 
Here's the equation for the synaptic 
conductance. 
It's a product of three different factors 
which together capture the function of 
the synapse. 
The first factor, g max, is the maximum 
conductance associated with that 
particular synapse. 
And that for example is associated with 
the number of channels that one might 
find on the post synaptic neuron. 
So the more the number of channels, the 
larger the value for G max. 
The second term, P release is the 
probability of release of 
neurotransmitter, given that you have an 
input spike. 
So once you have an input spike, what is 
the probability that neurotransmitters 
are going to be released into the 
synaptic cleft. 
And the last term, Ps is the probability 
of post synaptic channels opening, so, 
what are the probabilities that these 
channels on the post synaptic side are 
going to be open given that you have 
neurotransmitters being released. 
And that in turn also corresponds to the 
fraction of channels that are opened at 
any point in time. 
Lets see if we can come up with the 
simple model of how a synapse behaves. 
Here is the equation for the synaptic 
conductance from the previous slide. 
Now lets first make the simplifying 
function that p release. 
The probability that in your transmitters 
release given a spike is equal to 1, what 
does that mean, it means that when there 
is a spike at the presynaptic neuron it 
will always cause the release of 
neurotransmitters in to synaptic cleft. 
Now this does not have to be true in a 
biological neuron. 
A spike might sometimes fail to release 
neurotransmitters into the synaptic 
cleft. 
But for the case of our simple model, 
we'll assume that p release is equal to 
one. 
Now what we want to do is model the 
effect of a single spike on ps. 
Which is the fraction of channels that 
are opened on the postsynaptic side. 
The way that we'll model the opening and 
closing of channels is based on a kinetic 
model, and this is something that you 
might have seen in a chemistry class in 
high school or in college when you were 
modeling chemical reactions. 
So what we will do is we will assume that 
channels that are closed are opening at 
particular rate alpha s and similarly 
channels that are open are closing at a 
particular raet given by beta s and so we 
characterize the rate at which Ps is 
changing where Ps is the fraction of open 
channels. 
By these two terms, it's the difference 
between the first term and the second 
term. 
The first term is essentially the product 
of the opening rate times the fraction of 
channels that are closed at the current 
point in time. 
And that's just one minus PS. 
Minus. 
The product of the closing rate beta s 
times times the fraction of channels open 
the current point in time. 
Now may be you thought that the 
differential equation for Ps looked a bit 
intimidating or may be you thought that 
was a bit confusing but here's what Ps 
really looks like as a function of time 
given as spike on the y axis we are 
plotting Ps. 
Which has been normalized to have a 
maximum value of 1 and on the x axis we 
have time measured in milliseconds and 
what we are showing is biological data 
from three different kind of synapses. 
The AMBA synapse, GABA a synapse and the 
NMDA synapse. 
What you'll notice is that for the ampa 
synapse, the way that ps behaves can be 
modeled quite well by using an 
exponential function. 
Which we're calling kt. 
On the other hand, for the gaba a and the 
nmda synapse, the way that ps behaves is 
fit better by something called the alpha 
function which has. 
A peak that is after, the spike has 
occurred. 
So there's some amount of delay before 
the peak occurs and that's given by the 
alpha function as shown, down here. 
So, this is the equation for the alpha 
function and it has a parameter Tal peak. 
Which allows you to fit the particular 
data by shifting the peak from the time 
that the input spike occurred. 
So the spike occurred at time zero, and 
the peak might occur slightly later as 
determined by tal peak. 
What if you had more than one input 
spike? 
How would you model the effect of 
multiple spike on the synaptic 
conductance? 
Well, this is where the linear filter 
model makes an appearance again in this 
course, just when you thought you are 
done with linear filtering hearing it is 
again, wearing its ugly head. 
What supose that you have an input spike 
train so here is a set of spikes so this 
is called input spike train. 
Coming in, from some neuron to a 
particular synapse which we're calling 
synapse B. 
We can categorize the input spike train 
in terms of what is known as the response 
function, Roby. 
So that's given by the summation over all 
the times at which. 
A spike occured. 
So some, some or all the I of delta, T 
minus TI. 
So this is basically the delta function. 
So everytime you have a psike you put in 
this delta function which is essentially 
an infinite pulse at that location of the 
spike. 
Now why would you really want to do that? 
Well it turns out that when we do an 
integral for the filtering, it turns out 
to be quite convenient to have the spike 
train as one of these summations of delta 
functions. 
So basically this is a technical detail, 
so don't get so worried about it right 
now. 
So suppose that we have a spike train and 
we would like to model the effect of all 
the spikes on this particular neuron. 
How'd we do that? 
Well let's first select what kind of 
synapse this particular synapse is. 
So suppose it's something like an ampa 
synapse, as we discussed in the previous 
slide. 
The ampa synapse behaves as of it is an 
exponential function, so we have 
something that looks like this. 
This is k and this is. 
T is a function of time. 
And so, this can be used as a filter to 
act as the effect of an input spike on 
the postsynaptic neuron. 
So, now we have a filter. 
So here is the filtering equation that 
will model how the synaptic conductance 
changes on the post-synaptic neuron side. 
So basically what we are saying is that 
gb which is the synaptic conductance at b 
is essentially just nothing but the 
maximum conductance times basically 
summation of all of these exponential 
functions added together, and if you like 
integrals as the summation here is the 
linear filtering equation. 
And here is your favorite function, the 
rho b, the neural response function, 
where you have these delta functions 
summed up at the locations where you have 
spikes. 
Now, if you're still confused about this, 
actually, there's a very easy way to 
interpret the summation or this integral. 
So, here is The spike train and here's 
what the synaptic conductance GB is going 
to look like. 
So every time you have a spike, you put 
in one of your K functions, your synaptic 
filter. 
And then when you have another input 
spike such as this one, you simply add a 
copy of the synaptic filter, and you do 
so for each of these input spike. 
And so you're going to get a synaptic 
conductance that looks something like 
this. 
So this is what GBT looks like for this 
particular. 
Input spike train. 
So that wasn't really too hard, was it? 
The moral of the story here of course was 
that don't be too intimidated by these 
types of complex equations. 
So are you ready now to put everything 
that you have learned so far to create a 
network model? 
Now let's do it, so here is a simple 
example, let's just take a two neurons, 
neuron 1 and neuron 2 lets connect them 
together with excitatory synapses. 
Neuron 1 cnnects with neuron 2 with this 
excitatory synapse and neuron 2 connects 
with neuron 1 with this excitaory 
synapse. 
Now each of this neurons is given by our. 
Favorite equation here is the equation 
for how the membrane potential changes as 
a function of time. 
Here's the time constant for the 
membrane. 
And we're going to model these two 
neurons as Integrate-and-Fire neurons. 
So, this is something you heard about in 
Adrian's lecture in a previous week. 
And so the Integrate-and-Fire neuron 
essentially models the membrane potential 
and then when there is A particular 
threshold that is reached. 
So, here's the threshold. 
Then the neuron has a spike. 
So, the neuron spikes here and then is 
reset back to a particular value. 
So the particular value in this case is 
minus 80 millivolts. 
And the synapses are going to be modeled 
as Alpha synapses. 
So, you're going to use an Alpha function 
which has Essentially as we saw before, 
it peaked just lightly after zero and 
then it decayed down to zero. 
And so, we're going to first look at what 
happens if we model excited rate 
synapses, so neuron one excites neuron 
two and neuron two excites neuron one. 
And here is what the behavior of the 
network looks like for these two neurons 
when they're exciting each other. 
So you can see that neuron one fires 
first in this case and then neuron two 
fires after and so on. 
So they basically alternate firing from 
one to the other. 
Now what will happen if we change the 
synapses from excited rate to inhibit 
rate? 
So here's something surprising that 
happens. 
So if we change the synapses to 
inhibitory synapses, so we can do that by 
changing the equilibrium potential, also 
called the reversal potential of the 
synapse, to minus 80 millivolts. 
So that's less than the resting potential 
neuron given by minus 70 millivolts. 
So you can see that when you change the 
synapses to be inhibitory then we get 
synchrony which means the two neurons 
start firing at the same times so this 
synchronized wti each other and that's a 
really interesting property that people 
have been looking at also in certain 
brain regions so. 
Here's an example where a simple model of 
just two neurons either exciting each 
other or inhibiting each other give rise 
to sudden interesting behaviors that 
might be of relevance to people trying to 
model particular circuits in the brain. 
Okay great, that wraps up this particular 
lecture segment. 
In the next lecture. 
We look at how we can go from spiking 
networks to networks based on firing 
rates. 
And this, as we'll see, makes it much 
easier to simulate large networks of 
neurons. 
So, until then, goodbye and ciao. 

Hello neuro musketeers from around the 
world. 
In the previous lecture, we learned that 
neurons don't use Facebook or Linkedin to 
connect amongst themselves, rather they 
use synapses. 
We learned how to model a synapse on a 
computer, and we were even entertained by 
two neurons, that performed a ballroom 
dance by following each other, and then 
danced in synchrony by inhabiting each 
other. 
In this lecture, we will learn how to 
model larger networks of neurons. 
When modeling network of neurons, one of 
the choices you'll have to make as a 
computational neuroscientist, is deciding 
whether you should use spiking models of 
neurons, or firing rate based of neurons. 
Lets look at the advantages and 
disadvantages of these two choices. 
If you decide to use spiking neurons for 
your network model, the advantages that 
you get include the fact that you can now 
model computation and learning based on 
spike timing. 
So you can model phenomena such as 
synchrony between neurons. 
Which you would not be able to model, if 
you did not have spikes as the output of 
neurons. 
On the other hand, the disadvantages 
include the fact that now you have to 
model these differential equations. 
And so if you have a very, very large 
network of these neurons, then it might 
be computationally expensive, or in some 
cases, maybe even impossible to simulate 
very, very large networks of perhaps 
millions and millions of neurons, because 
you have to simulate these differential 
equations on your computer. 
On the other hand, you might decide to 
use neurons with firing rate outputs. 
So this means that the output of the 
neurons is not zero, or one or a spike, 
but real valued outputs denoting the 
firing rate of the neuron. 
The advantages include the fact that now 
you can have greater efficiency, and so 
you will be able to simulate, for 
example, very large networks of neurons. 
On the other hand, since you are using 
firing rates as the output, you will not 
be able to model phenomena based on spike 
timing, or synchrony. 
The question that we can ask now, is how 
are these two approaches related? 
The firing rate model can be related to 
the spiking model by using the linear 
filter model of a synapse, that we 
covered in the last lecture. 
If you recall, we had a synapse b, that 
was receiving an input spike train that 
we called RHO b. 
And the RHO b, was just a representation 
of a spike train, such as this one. 
And if you can model the synapse b using 
a filter, which we called K(t). 
So for example, K(t) could be an 
exponential filter, such as this. 
Then, the synaptic conductance at b could 
be modeled using a linear filter equation 
that is given down here. 
And so, this particular equation just 
implements the filtering operation. 
So, you simply replace each spike by the 
filter for the synapse, and you get a 
conductance that looks something like 
this. 
We can now go from the basic synapse 
model that we had on the previous slide, 
to the model for multiple synapses. 
So here's a cartoon of a neuron receiving 
inputs from N different other neurons. 
And so each of these synapses has a 
synaptic weight, that they're calling W1, 
W2, all the way up to wN, and each of 
these synapses now gets spike trains 
given by RHO1, RHO2, all the way up to 
RHO N. 
And if you assume that there are no 
nonlinear interactions between these 
synapses, then the total synaptic 
current, the total input to the neuron, 
is given by just the summation of each of 
the individual inputs, coming in from 
each of these synapses. 
And so, the total input to the neuron 
then is just the summation, given by this 
particular equation. 
So, are we ready to make that leap from 
the spiking model to the firing rate 
model? 
Some people say its almost a leap of 
faith, and they compare it to converting 
from one religion to the other. 
Well, I think that's a little bit 
extreme. 
In any case, here's our network, and here 
are the spike trains from the input 
neurons, and here is the equation that we 
had in the previous slide. 
Which characterizes the total amount of 
input that the neuron is getting from all 
its synapses. 
Now the leap that we make is going from 
these spike train functions, the RHO b 
that we have, at each of the individual 
synapses, to the instantaneous firing 
rate, that we have for each of the input 
neurons. 
Now when can this replacement fail? 
Well this would fail, if for example, 
there are correlations between all the 
input neurons, or when there's synchrony, 
for example, between pairs or more than 
pairs of neurons. 
So in those cases, we cannot make this 
replacement. 
But it turns out that in many, many 
cases, it is quite possible to replace 
the spike train function Rho b, with the 
firing rate function, the instantaneous 
firing rate for the individual input 
neurons. 
Now, I know many people don't like 
integrals in their equations. 
And some people even get nightmares when 
they have integrals such as this one, in 
an equation. 
So for those integral-phobes among you, 
is there any way to simplify the input 
current equation, so it does not contain 
this integral? 
Well it turns out that if you choose the 
synaptic filter to be an exponential 
filter, then when you substitute this 
value for K(t), in this particular 
exponential expression, for K(t) in our 
equation for the input current. 
And when you take the derivative of this 
equation on both sides, then you have an 
equation that does not contain an 
integral. 
So here's how the input current Is 
changes as a function of time. 
And you will notice, its very similar in 
form to the equation we had for the RC 
circuit, except that now this equation 
tells you how the input current changes 
as a function of time, and there is a 
particular time constant tau s that 
determines how rapidly the input current 
reacts to changes in the input. 
And the input component itself is nothing 
but just a linear weighted sum of all the 
inputs, weighted by the corresponding 
synaptic weight. 
And if you want to simplify this even 
further, you can write this weighted sum 
as just the dot product of the weight 
vector with the input vector. 
Okay, it's time to uncork that champagne 
bottle. 
We've arrived at our first 
firing-rate-based network model. 
Here's a picture of the network we're 
going to look at. 
The network has a single output neuron. 
The firing rate of the output neuron is 
given by v. 
The neuron receives inputs from these 
input neurons, whose firing rate are 
denoted by this vector u, and the 
synaptic connections between the input 
neurons and the single up and neuron, is 
given by the weight vector, the synaptic 
weight vector, w. 
We're going to assume that the firing 
rate of the output neuron, follows an 
equation that is quite similar to the 
equation we had for the membrane 
potential, from the previous lecture. 
The output firing rate, has a particular 
time constant tau r, which determines how 
rapidly the firing rate is able to follow 
inputs, that the neuron is getting from 
another neurons. 
And there's an optional non-linear 
function F, that we can pass the input 
current through. 
And finally, the input current due to the 
synaptic inputs, is given by the same 
equation as we had on the previous slide. 
Now let's look at how this network 
behaves when you compare the magnitude of 
tau s, with the magnitude for tau r. 
If tau s is much smaller than tau r, this 
means that the synaptic input converges 
quickly. 
Which means that we're going to have the 
input current be equal to just w.u, as 
opposed to the network dynamics which 
takes a long time to converge. 
And so the equation that we have then, 
would basically replace the Is that we 
have here with w.u. 
And so, the equation we have then for the 
network, is just tau r dvdt for the 
change of the firing rate as a function 
of time, is equal to minus v plus, the 
potential non-linear function, and then 
just F of w.u. 
On the other hand, if the synapses are 
slow compared to the output dynamics, in 
other words, if tau r is much lesser than 
tau s, then we can set v equal to F of 
Is(t). 
Because the upper dynamics is much faster 
compared to how the synaptic current is 
changing over time. 
And so we have an equation where the 
output firing rate is equal to some 
potentially non-linear function of the 
input synaptic current. 
Where the synaptic current is given by 
this differential equation. 
And, finally, in the case of static 
inputs. 
And by static, we mean that the input 
does not change as a function of time, 
for long periods of time. 
Then, we can look at the steady-state 
output of the network. 
So, v s s denotes the steady-state output 
of the network. 
So, how do we get the steady-state output 
of the network. 
Well you can set dvdt equal to 0, as well 
dIsdt equal to 0. 
So when we set dvdt equal to 0, we get v 
steady-state is equal to F of Is. 
And since Is is also not changing as a 
function of time, Is is equal to w.u. 
So those of you who are aficionados of 
artificial neural networks, you should 
find this equation quite familiar. 
This is in fact, the equation that one 
uses in artificial neural networks, to 
model neurons, where we replace F with a 
threshold function, or a sigmoid 
function. 
And now you know that this equation that 
people in the artificial intelligence 
community have been using for a very long 
time as, in fact, a simplification of the 
rich dynamics that one has in the 
synaptic current, as well as the dynamics 
of the output firing rate. 
Let's move to the case where there are 
multiple output neurons. 
So in the case of a single output neuron, 
as in the previous slide, we used an 
equation that looked just like this, to 
model how the output firing rate changes 
as a function of time. 
And if we make the assumption that the 
synapses are relatively fast, then we can 
simply set the input current due to the 
synapses, to just w d.u, so weighted sum 
of the inputs. 
And so this equation captures how the 
output firing rate changes, as a function 
of the inputs. 
Now, what do we do if we want to extend 
this to multiple output neurons? 
So, here's what the network looks like. 
And in this case instead of having a 
single output, we have an output vector. 
And so, the equation then looks something 
like this. 
So sort of a weight vector W, we now have 
a wait matrix, W. 
And the equation is now a differential 
equation that includes a vector output. 
So v is now a vector, and this product 
instead of being w.u, is now just the 
matrix W multiplied with the input 
vector. 
The matrix W has the components Wij. 
Wij denotes the synaptic weight from 
neuron j to neuron i. 
So if this is neuron j and if this is 
neuron i, then this connection here would 
have a weight that is given by wij. 
And the synaptic weight matrix W would 
capture all of these connections from the 
input layer of neurons, to the output 
layer of neurons. 
We have so far been considering networks 
that take inputs and feed them to a layer 
of output neurons. 
These types of networks are called 
feedforward networks. 
Now if our brain had only feedforward 
networks, then we will be constantly 
reacting to stimuli and saying things 
that we're not suppose to say. 
Luckily, the networks in our brains have 
vigorant feedback connection. 
This means that for any particular layer 
of neurons or neurons in a brain area, 
the neurons make connections with each 
other. 
So for example, in this network, we have 
an output layer of neurons. 
And we can allow connections between the 
output layer neurons, and we can 
characterize the strength of these 
connections between the output layer of 
neurons, using a set of synaptic weights, 
which we're going to call, M. 
So this matrix M denotes the strength of 
the connections between the layer of 
output neurons. 
So now we have a new equation that 
characterizes how the firing rates of the 
output neurons change as a function of 
time. 
We have as before, a time constant, and 
as before we have a feedforward input, 
given by W times u. 
But now we also have the feedback given 
by M times v. 
So the matrix M is multiplied by the 
vector v, to give you a new vector that 
characterizes the feedback from past 
activities of the output neurons. 
You'll notice that if you set the matrix 
M to a matrix of zeros, which means that 
there are no feedback connections, then 
we have an equation where you don't have 
the M times v component, and that is 
equal into the equation for a feedforward 
network that you saw in the previous 
slide. 
Okay, let's begin a journey into the land 
of networks, by looking at a simple 
linear feedforward network. 
So here's the equation for a linear 
feedforward network. 
And for the sake of simplicity, let's 
assume that the input doesn't change as a 
function of time. 
So we have a static input, u, and so we 
can look at the steady-state value of the 
output. 
And so that means we just set dv dt equal 
to 0, and so we get the equation for 
steady-state output of the network as Vss 
equal to W times u. 
Now, suppose the feedforward weights are 
given by this matrix W, and suppose I 
give you the input u, to be this vector 
here. 
What is the output of this linear 
feedforward network? 
I'll give you a couple of seconds to 
think about that, or maybe you want to 
pause the video at this point. 
Okay I hope you found the answer. 
Here it is, so what we had here was a 6 
by 5 matrix, multiplied by a 5 by 1 
vector, and as you might expect, you'll 
get a 6 by 1 vector that looks like this. 
Now the question that I would like to ask 
you is, what do you think this network is 
doing? 
In other words, how is it transforming 
the input u, to the output Vss? 
What is the function that this network is 
implementing? 
As you might have guessed, this network 
is performing linear filtering, in order 
to detect edges in the input. 
So what we mean by detect edges? 
Well if you look at carefully at the 
matrix W, you will notice that the rows 
of the matrix W contain shifted versions 
of this particular filter, and the filter 
looks something like this. 
And if you take the input, so this was 
our input, 1, 2, 2, 2 and 1. 
Here is a picture of what the input looks 
like. 
The operation of multiplying the matrix W 
with the input, gives you an output that 
looks like this, and you'll notice that 
the output has detected, wherever there 
are transitions. 
From in this case 1 to 2, and then back 
from 2 to 1. 
And so it is detected the sudden change 
in the input. 
And so if you imagine that we have an 
input image, than the filter is 
essentially detecting changes in the 
brightness values of the image. 
In this case going from a brightness 
value or a pixel value of 1, to a pixel 
value of 2, and then again from a pixel 
value of 2, back to a pixel value of 1. 
Now we can also apply the linear 
filtering network to 2-dimensional 
images, such as the one shown here. 
So how would you transform a 
2-dimensional image to a 1-dimensional 
vector U, that you feed to the linear 
filtering network? 
One way you can do that is by collapsing 
each row of this 2-dimensional image. 
And so if this is row number one, row 
number two, row number three and so one, 
you would simply collapse all the rows, 
to form one very big long vector, where 
the first part of the vector is row 
number one, the second part of the vector 
is row number two, and so on. 
And so, there you have your input vector 
U. 
And here is an example output. 
So, in this case we're not using the same 
filter as the one in the previous slide, 
but something that is used in this 
particular website. 
And so, you can see how the input has 
been tranformed to enhance or detect the 
edges of this particular image. 
At this point you might be wondering, 
does the brain detect edges? 
And you already know the answer to this 
because in week one, we discussed the 
concept of receptive fields. 
And we noted that cells in the primary 
visual cortex, or V1, have receptive 
fields that look something like this. 
And this type of receptive field is 
indicative of edge detecting neurons, for 
example, this receptive field suggests 
that the neuron detects or gets excited 
by a transition from a dark to a bright 
edge, in the visual image. 
Whereas, this neuron here would be 
sensitive to dark bars embedded within a 
bright background. 
The brain is in fact not just a detecting 
edges, it is actually doing calculus. 
The brain was doing calculus even before 
Newton and Leibniz invented calculus. 
Sorry Newton and Leibniz, here's what I 
mean when I say that the brain is doing 
calculus. 
Here is the receptor field of a VI 
neuron, and this, as you recall, is the 
linear filter that we used in our feet 
forward matrix W. 
So if, this is some particular location 
x, this would be the next location, x 
plus one in the image. 
And now, if you look at the definition of 
a derivative of a function f, with 
respect to the value x, then here is the 
definition of the derivative from you 
calculus text book. 
And if you look at it discrete 
approximation of this derivative, you'll 
find that it amounts to just a difference 
between f of x plus 1, minus f of x. 
And if you look at our feedforward 
matrix, it's simply multiplying w with u, 
and so if u is the image, the f of x, 
then w, which is essentially shifted 
versions of this filter, performs exactly 
this particular operation of subtracting, 
neighboring pixel values at the location 
x and x plus 1. 
Now here's something that's even more 
interesting. 
If we look at the other type of receptive 
field for V1 neurons, we call this the 
bar detector type receptive field. 
We can implement this type of a receptive 
field by using a linear filter that looks 
something like this. 
So if this is location x, this would be 
location x minus 1. 
This would be location x plus 1, in the 
image. 
And we would use a matrix W in our 
feedforward network, that would have 
shifted versions of this linear filter. 
Now let's look at the definition of the 
second order derivative of a function f. 
So, d squared f over dx squared is given 
by this expression. 
If we approximate this expression using a 
discrete approximation, we would have 
something that looks like this. 
Now you'll notice that this 
approximation, so f of x plus 1 minus 2 
times f of x plus f of x minus 1, has the 
same coefficients of our linear filter. 
So 1 minus 2 and 1, here's 1 minus 2 and 
1. 
And so when you multiply the feedforward 
matrix with the input u, using this 
linear filter, what are you going to get? 
Well, you're going to get an 
approximation to the second order 
derivative of the image, if u represents 
an image. 
And since the image is 2-dimensional, the 
brain computes derivatives along multiple 
directions than the image, using oriented 
receptive fields such as these. 
That wraps up our first expedition into 
the world of networks. 
In the next lecture, we'll meet those 
wild and crazy creatures, called the 
recurrent networks. 
And we'll also enjoy the company of our 
little friends, eigenvectors and 
eigenvalues. 
Until then ciao and alvida. 

Hello there! 
In this last leg of our journey through 
the land of networks, we will visit the 
tricky, but fascinating, world of 
recurrent networks. 
To understand recurrent networks, we will 
get help from our favorite frenemies, the 
eigenvectors and the eigenvalues of a 
matrix. 
But to negotiate those treacherous 
eigenwaters, It is best to have your 
eigen hat on. 
I have mine on, and I'll give you a 
couple of seconds to grab yours. 
Let's begin by asking what can a linear 
recurrent network do? 
Here's the equation for a linear 
recurrent network. 
If there are N output neurons, then the 
output vector v is going to be N by 1, 
and the feedforward input to these ouput 
neurons is given by W times U, and that's 
again going to be an N by 1 vector. 
And we can call this N by 1 vector h, so 
we don't have to write W times U each 
time. 
And the feedback to the output neurons, 
is given by M x v, where M is the 
recurrent connection matrix. 
What we want to find out, is how the 
output of the network v(t) behaves for 
different values of M, the recurrent 
connection matrix M. 
This is where eigenvectors come to our 
rescue. 
Here is the differential equation that we 
are trying to solve, to understand how 
v{t} behaves. 
And this equation, as you can see, 
contains a mix of vectors and this matrix 
times a vector. 
So that's a pretty complicated equation 
to solve. 
Fortunately, we can use, eigenvectors to 
solve this particular differential 
equation. 
How do we do that? 
Well, suppose the connection matrix, the 
N by N vector and connection matrix, is 
symmetric. 
What does that mean? 
It means that for any particular pair of 
output neurons, so if this is neuron 
number one and this is neuron number two. 
Then the fact that the recurrent 
connection matrix is symmetric, just 
means that if one connects to two with 
some particular value or strength A, then 
two connects to one also, with the same 
value array. 
So in other words, M 1, 2 is equal to M 
2, 1, is equal to the value A. 
And that's what it means for this matrix 
M, to be symmetric. 
Now why is it useful to have the 
connection matrix M symmetric? 
Well it turns out that if M is symmetric, 
then M has N different orthogonal 
eigenvectors, and corresponding 
eigenvalues would satisfy the standard 
eigenvector, eigenvalue equation shown 
here. 
Now what does it mean for these 
eigenvectors to be orthogonal? 
Well, if you take any two of these 
eigenvectors ei and ej, as long as i is 
not equal to j, the fact that they're 
orthogonal just means that he dot product 
of these two eigenvectors is going to be, 
you guessed it, 0. 
Now we can further make these 
eigenvectors orthonormal, so, orthonormal 
means, that these eigenvectors are not 
only orthogonal, but also, they have a 
length of 1. 
And, we can do that, by dividing each of 
these item vectors by their length, then 
we have the fact that ei.ei is going to 
equal 1. 
And if that's satisfied, then we say that 
we have a set of vectors, these 
eigenvectors, which are orthonormal to 
each other. 
Why is it useful to have these 
eigenvectors of them, which are 
orthonormal to each other? 
Well, it turns out that we can now write 
any n-dimensional vector, including our 
output vector, v{t}, as simply a linear 
combination of our orthonormal 
eigenvectors. 
So these eigenvectors now form a new 
basis or a new coordinate system for 
expressing n-dimensional vectors such as 
v{t}. 
To drive home the point let's look at the 
special case of a three dimensional 
space. 
So here's x, y, and z, and lets suppose 
that this is our vector v{t}. 
All we're doing now is expressing this 
vector vt in a new coordinate system 
given by our orthonormal eigenvectors e1, 
e2, and e3. 
And in the xyz system we were writing 
v(t), as simply the linear combination of 
the first component of v times 1,0,0. 
This was our vector for x. 
And v2 times 0,1,0, this is our y 
component. 
And finally for the z component v3 times 
0,0,1. 
So all we are doing now is instead of 
expressing vt in the coordinate system 
given by the x, y, and z vectors, we are 
now writing v as a different linear 
combination, c1 times e1, plus c2 times 
e2, plus c3 times e3. 
Now why go through all this trouble? 
Well it turns out that if you substitute 
the equation for v(t) in terms of the 
ei's into the differential equation for 
v, and then further we use the 
eigenvector equation, as well as the 
orthonormality of ei, then we can solve 
for ci as a function of time. 
And so here is the equation for ci as a 
function of time, and once you have a 
closed form expression for ci as the 
function of time, we can substitute that 
value for ci into our equation for v. 
And therefore, we have solved the 
differential equation, and we now have a 
complete expression, that characterizes 
how v changes as a function of time. 
And if you want to get into all the 
mathematical detail of how we derived 
this expression for ci(t), I would 
encourage you to go to the supplementary 
materials on the course website. 
We can now show that the eigenvalues of 
the recurrent connection matrix, 
determine whether the network is stable 
or not. 
To see this, suppose one of lambda I is 
bigger than 1. 
Well, what happens to the output of the 
network, given by v(t), which is a linear 
combination of the item vectors weighted 
by these coefficient ci? 
Well if one of the lambda I's is bigger 
than 1, lets say that this lambda I here 
is equal to 2, which is bigger than 1. 
Then this term ends up being an 
exponential function of time. 
And so as time goes on you're going to 
have this term becoming larger and 
larger, and therefore ci of t is also 
going to become larger and larger. 
And so the output of the network then, 
also grows without any bound, which means 
that v(t) explodes, and so what you end 
up getting is an unstable network. 
On the other hand if all the eigenvalues 
are less than 1, then you should be able 
to convince yourself, by plugging in 
values of lambda I less than 1, in our 
equation for ci(t), that the network is 
stable because v(t) is going to converge 
to some steady state value. 
Which is given simply by the linear 
combination of all of these coefficients 
which are conversed now to this 
particular value, multiplied by each of 
the corresponding eigenvectors. 
Now we can answer the question that we 
posed earlier in the lecture. 
What can a recurrent network do? 
One thing that a linear recurrent network 
can do, is amplify its inputs. 
To see this, suppose that all the lambda 
I, the eigenvalues are less than 1. 
So we showed in the previous slide that 
the output of the network in the steady 
state is going to look like this. 
And if one of these eigenvalues, let's 
say lambda 1 is very close to 1, and all 
the other eigenvalues are much much 
smaller. 
Then the lambda 1 term, is going to 
dominate the sum, and so the steady state 
output of the network, is going to be 
basically the projection of the input 
onto the, first item vector, divided by 
1, minus lambda 1, multiplied by e 1. 
So, what we have then, is a network that 
is amplifying it's input projection. 
So, if lambda 1, for example, is equal to 
0.9, which is close to 1, then 1 over 1 
minus lambda 1 is going to be 10. 
And so, we have an amplification factor 
of this projection of the input on to e1 
of 10. 
Now let's look an example of a Linear 
Recurrent Network. 
So, let's assume that each of these 
output neurons codes for some angle 
between minus 180 degrees to plus 180 
degrees. 
So instead of labeling these neurons with 
1, 2, 3, 4 and 5, we can label them 
according to some angles. 
So for example, this could be minus 180 
degrees, this neuron could be minus 90. 
This neuron could be labeled with 0, this 
with plus 90, and this with 180. 
Now, why are we labeling neurons with 
angles? 
It's because we can now define the 
connection matrix M, as a cosine 
function, for example, of the relative 
angle labeling the neurons. 
So in other words, m of theta, theta 
prime, could be proportional to cosine of 
theta minus theta prime. 
What does this type of connectivity look 
like? 
Well it results in neurons exciting other 
neurons that are nearby, and inhibiting 
other neurons that are further away. 
And here's a graphical depiction of the 
cosine based connectivity function. 
So, for neurons that are close to any 
given neuron, you have excitation, and 
for neurons that are further away, you 
have inhibition. 
Now let's ask the question, isn't M, 
defined by such a connectivity function, 
symmetric? 
In other words, is M theta, theta prime 
equal to M theta prime theta? 
Well, that's the same as asking whether 
cosine of x is equal to cosine of minus 
x, which we know is true. 
Which means that yes, the connectivity 
matrix is indeed symmetric. 
Now this type of a connectivity 
function's interesting because there's 
some evidence that such connectivity is 
also found in the cerebral cortex. 
Neurons in the cerebral cortex tend to 
excite other neurons that are near them, 
and inhibit neurons that are further 
away. 
Now suppose we choose the connectivity 
matrix of a linear recurrent network to 
be proportional to the cosin function, 
such that all the eigenvalues are 0 
except one eigenvalue, which is equal to 
0.9. 
Then as we showed earlier, we would 
expect to see amplification. 
And we'd expect to see an amplification 
of the input by a factor of 10. 
So, let's see if that really happens when 
we simulate such a network. 
And, not surprisingly, the answer is yes. 
When we present the network with a noisy 
input, we do get an output that is an 
amplified version of the input, where the 
peak of this noisy input has been 
amplified, and the smaller peaks have 
been suppressed. 
So what else can a linear recurrent 
network do? 
Well the earlier remark that if all the 
eigenvalues are less than 1, then the 
network is stable. 
Now suppose one of these eigenvalues, 
lets say Lambda 1 is exactly equal to 1. 
In that case, we can show that we have a 
different kind of equation for how the 
coefficient for c1 evolves. 
It's given by this differential equation. 
And here's something interesting that 
happens. 
So suppose that the input was initially 
0, and then it was turned on and then it 
was turned off. 
So we have the input h, which was 
initially 0, and then it was turned on to 
some value and then turned off again. 
Then here's what happens, even after the 
input has been turned off, so even after 
h is equal to 0, the network maintains an 
output. 
So the network now maintains a memory of 
the integral of the past inputs, as given 
by this integral shown here. 
Interestingly there's evidence for 
integrator neurons in the brain. 
In particular in the medial vestibular 
nucleus, there are these neurons that 
maintain a memory for eye position. 
So when the input to these neurons comes 
in the form of bursts, so here's one 
burst spikes that changes the eye 
position. 
Here's another burst of spikes from a 
different neuron, that decreases the eye 
position. 
We note that the integrated neuron 
maintains persistent activity, or a 
memory of the I position by changing its 
firing rate. 
And this is very similar to what we had 
in the previous slide. 
Where we had the neuron maintaining a 
memory of the integral of past inputs. 
So what this goes to show, once again, is 
that the brain can do calculus. 
In this case, we've shown that it can do 
integration. 
And we already showed that it can do 
differentiation in the previous lecture. 
So once again, sorry Newton and Liebowitz 
,looks like the brain has beaten you to 
the punch. 
Let's conclude our tour of recurrent 
networks, by looking at nonlinear 
recurrent networks. 
And we can make the network nonlinear by 
applying a nonlinear function F to the 
sum of the input and recurrent feedback. 
And perhaps the simplest kind of 
non-linearity is the rectification 
non-linearity, which takes any input x 
and sets it equal to x, if x is greater 
than 0 and sets it equal to 0 otherwise. 
This non-linearity is quite useful 
because if you recall, the vector v 
represents the firing rates of neurons. 
And so the rectification non-linearity 
makes sure that the firing rates never go 
below 0. 
So what can non-linear recurrent networks 
do? 
They can perform amplification, similar 
to linear recurrent networks. 
So here is the input to the non-linear 
network. 
Which is a noisy input with a peak near 
0. 
And here is the output of the nonlinear 
network. 
And you can see how the network has 
amplified the input, but it has also 
cleaned up the input, and it has 
suppressed some of the other peaks in the 
input. 
Now the interesting thing here is that 
the recurring connections, although they 
were again the cosine type recurrent 
connections, with excitation nearby and 
inhibition further away, the eigenvalues, 
in this case are all 0, but one of the 
eigenvalues was actually bigger than 1. 
So lambda 1 was actually 1.9. 
So in the linear recurrent network case, 
this would have led to an unstable 
network. 
But since we have the rectification 
non-linearity, it saves the day, and the 
network is in fact, stable and gives us 
this kind of amplification. 
Now here's something else that the 
non-linear recurrent network can do. 
It can perform selective attention, which 
is it can select one part of the input, 
and suppress the other part. 
So here's an input that contains 2 peaks. 
And if you look at the output of the 
non-linear network, it has essentially 
focused only on the peak at minus 90 
degrees, and it has suppressed the other 
peak. 
So the network is performing a type of 
winner takes all input selection. 
Some might say that the network is 
implementing the capitalist credo, of the 
rich get richer, and the poor get poorer. 
And some people might even say that the 
moral of the story here, is that you have 
to be non-linear to be a capitalist. 
But I think we digress. 
The same non-linear network can also 
perform something called gain modulation. 
What does that mean? 
Well if the inputs look like this. 
Where you're adding a constant amount to 
a particular input. 
Which basically means you're shifting the 
input additively from one level to the 
other. 
The effect on the output is 
multiplicative. 
So the change in the input multiplies the 
output, and so you get this type of 
modulation. 
Also called, Gain Modulation, of the 
output firing rate of the neuron. 
Now, this is interesting because, this 
type of Gain modulation of neuro 
responses, has also been observed in the 
brain. 
Specifically in area 7A of the parietal 
cortex. 
Finally, the same non-linear network also 
maintains a memory of past inputs, just 
like the linear recurrent network that we 
considered a while ago. 
Here is the input for the non-linear 
network, it's basically a bump center 
around 0. 
That's the local input, along with some 
background input, which is about 0. 
The output of the network, as you might 
expect, is just an amplified version of 
the input, with the background 
suppressed. 
What happens to this output, when we turn 
off the local input? 
Here's what we get. 
So when the local input is turned off, 
you still have an output in this network, 
and the output has a peak at 0, which is 
exactly where the peak of the local input 
was. 
So this memory of the input, is being 
maintained in this network by rigorant 
activity. 
So what we have here then, is a network 
that maintains a memory of past activity, 
when the input has been turned off. 
And this is quite similar to the short 
term memory or working memory of past 
inputs, that is maintained by neurons in 
the pre-frontal cortex in the brain. 
We have been so far looking at networks 
with symmetric recurrent connections, 
what about non-symmetric recurrent 
networks? 
Well the simplest form of non-symmetric 
recurrent networks, would be a network of 
excitatory and inhibitory neurons. 
So for example, if you had one excitatory 
neuron, and one inhibitory neuron. 
You could have the excitatory neuron 
exciting the inhibitory neuron, and the 
inhibitory neuron then inhibiting, the 
excitatory neuron. 
And perhaps there is also connection from 
the neuron onto itself. 
These are called autapses, and so this 
will again be excititory, this will be 
inhibitory. 
So you can see why, the connections 
cannot be symmetric, because you cannot 
have excititory connection be plus, and 
the inhibitory connection also be plus. 
It has to be a negative, or an inhibitory 
connection. 
Here are the differential equations for 
our two neurons. 
So here is the differential equation for 
the firing rate of the excitatory neuron, 
here is the differential equation for the 
firing rate of the inhibitory neuron. 
And these are all the different 
parameters. 
The Excitatory connection from the neuron 
onto itself. 
Here is the connection from the 
inhibitory neuron onto the excitatory 
neuron, and so on. 
And you also see that we've added these 
parameters for thresholds that we apply 
And then that in turn is passed through a 
non-linearity, which is the rectification 
non-linearity. 
And just to make things concrete, let's 
assign some values. 
So these are some values for each of 
these parameters, for the connections and 
the threshold. 
And then finally we will leave one 
particular parameter. 
We're calling that tau i. 
That is the time constant for the 
inhibitory neuron. 
We will leave that unassigned, and we 
will vary this parameter to study the 
behavior of this non-linear and 
non-symmetric recurrent network. 
So how do we analyze the dynamics of such 
a non-linear and non-symmetric network? 
Well, hold on to your eigenhats, because 
we're going to need to use eigenvectors 
and eigenvalues again. 
To understand the dynamic behavior of 
this network, we can perform linear 
stability analysis. 
What does that mean? 
It means we can how stable the network is 
near a fixed point. 
The fixed point is basically obtained by 
looking at one of the values for vE and 
vI that make DvEdt and DvIDT go to 0. 
So, when both of these are 0, then we 
have values for vE and vI which are 
fixed, and which do not change as the 
function of time, and that would give you 
a fixed point for this network. 
So, how do we perform Linear Stability 
Analysis? 
Well we take the derivatives of the 
right-hand side of both of these 
equations, with respect to vE and vI. 
What we get then is a matrix, which is 
called the stability matrix, or if you 
want to be cool, you can call it the 
Jacobian matrix. 
Since the Jacobian matrix is not 
symmetric, the eigenvalues of the matrix 
can have both real and imaginary parts. 
So the eigenvalues can be complex, and 
these real and imaginary parts of the 
eigenvalues, in turn, determine the 
dynamics of the nonlinear network near a 
fixed point. 
So they determine whether the network is 
stable or not. 
Now we've assigned values for all of the 
parameters except for tau I. 
So what we can do now is choose different 
values for tau I, and this will in turn 
cause different eigenvalues for J. 
And then we can look at the effect of the 
different eigenvalues for J, on the 
stability and the behavior of this 
nonlinear network. 
First, let's look at what happens when we 
set tau I equal to 30 milliseconds. 
This makes the real part of the 2 
eigenvalues for the stability matrix, 
negative. 
And, as we show in the supplementary 
materials for this lecture, on the course 
website, the real part being negative 
causes the network to be stable near the 
fixed point. 
So here's a pictorial depiction of what 
happens when we set tau I equal to 30 
milliseconds. 
So the x axis is vE, the y axis is vI. 
And so if we start out at some particular 
location, which is some particular value 
for vE and vI. 
Then the network essentially converges to 
the fixed point, which is the point at 
which dve dt equal to 0, and dv1 dt equal 
to 0. 
So both vE and vI are not changing at 
this location, in this particular plot. 
Now if we look at what's happening as a 
function of time, you can see that both 
vE and vI oscillate. 
And the oscillations are damped, and 
eventually the oscillations are no longer 
there, and the network has converged to a 
specific value for vE, and a specific 
value for vI, and that is the stable 
fixed point of the network. 
This stable fixed point is also called a 
point attractor in the terminology of 
dynamical systems. 
Now look at what happens when you choose 
tau I to be 50 milliseconds. 
That makes the real part of the 
eigenvalues for the stability matrix 
positive. 
And as we show in the supplementary 
materials for this lecture, when the real 
part of the eigenvalues turn out to be 
positive, then the network is unstable. 
And so if you start out, in this plot of 
vE and vI at some location, near the 
fixed points, so here is the fixed point. 
And if you start out here with some value 
for vE and vI, then the network moves 
away from the fixed point, and so the 
network is unstable, and diverges away 
from the fixed point. 
But luckily, the rectification of 
linearity comes to the rescue. 
How is that? 
Well, as the value for vE tends to go 
negative, the rectification on linearity 
stops it from going negative, and it puts 
it back on track. 
And so we have the network looping around 
on this limit cycle. 
Here's another way to look at this limit 
cycle. 
So if you plug vE and vI as a function of 
time, then you'll observe that initially 
the vE and vI values start to increase. 
But then, once you hit this rectification 
non-linearity, then you have a stable 
oscillation. 
So, both vE and vI start to oscillate in 
a stable manner, and that corresponds to 
a going around on this limit cycle. 
So let's summarize what we saw in the 
previous slide and in this slide. 
So when you change the parameter tau I 
from 30 to 50 milliseconds, the nonlinear 
network made a transition, from having a 
stable fixed point, to becoming unstable 
and resulting in a limit cycle. 
In dynamical systems theory, such a 
transition is known as a half 
bifurcation. 
Well, I think it's time now for our own 
half bifurcation. 
That wraps up our journey into the land 
of networks. 
Next week, we learn about how the brain 
learns, by changing the connections 
between neurons in its networks. 
Until then, adios and goodbye. 

[MUSIC]. 
Hello neuro explorers. 
Last week, we learned how neurons can be 
connected to form feed forward and 
recurrent networks. 
This week, we learn how these connections 
can be adapted using synaptic elasticity, 
allowing the brain to learn about the 
world from its inputs. 
Now, what better way to start this 
journey than to gaze upon this beautiful 
drawing of the hippocampus by the great 
Romon y Cayal from a 100 years ago. 
it was in the hippocampus that some of 
the first results on synaptic plasticity 
were obtained. 
One type of synaptic plasticity that's 
observed in the brain is long term 
potentiation or LTP. 
An LTPs defined as an experimentally 
observed increase in the synaptic 
strength from some neuron A to another 
neuron B that can last for several hours 
or even days. 
And the way you would induce LTP is by 
causing some neuron A to fire a burst of 
spikes and if the neuron A is connected 
to some other neuron B where B is also 
excited, it's depolarized or it's firing 
some spikes. 
Then what you would see is an increase in 
the size of the excitatory postsynaptic 
potential. 
So that means that for the same input 
initially you might have a small EPSP. 
But after you pair neuron A with neuron B 
several times then what you'd observe is 
an increase in the size of the EPSP which 
indicates that the strength of the 
connection from neuron A to neuron B has 
been increased. 
The counterpart to Long-Term Potentiation 
or LTP is Long-Term Depression or LTD. 
And LTD corresponds to an experimentally 
observed decrease in the synaptic strand 
that lasts for hours or days and you can 
obtain LTD when you have the following 
situation. 
So if the neuron A firing some spikes but 
neuron B does not fire any spikes or it 
does not depolarize. 
So in this situation, when you have some 
input from neuron A but no output coming 
from neuron B then what you observe is a 
decrease in the EPSB size. 
So if the initial EPSB for a single input 
was as shown at the very top here. 
And as the pairing occurs where you have 
some input from A but no output from B, 
you would expect to see a decrease in the 
size of EPSP. 
And what that implies is that the 
connection strength from neuron A to 
neuron B has been decreased. 
Now here's something that's interesting. 
Even before LTP and LTD were discovered 
in the brain, a Canadian psychologist 
named Donald Hebb predicted that 
something like LTB should occur in the 
brain. 
He suggested a learning rule for how 
neurons in the brain should adapt the 
connections among themselves and this 
learning rule has been called Hebb's 
Learning Rule or Hebbian Learning Rule 
and here's what it says. 
If a neuron A repeatedly takes part in 
firing another neuron B, then the synapse 
from A to B should be strengthened. 
And here is a cartoon of what this 
learning rule implies, if we have a 
neuron A that is firing and that intern 
is participating in the firing of another 
neurons. 
So the neuron B produces for example one 
or a few spikes. 
Now if this situation occurs, Hebb's 
Learning Rule predicts that one ought to 
increase the strength of the connection 
from neuron A to neuron B, because neuron 
A is participating in the firing of 
neuron B. 
And so what we then get is for the same 
input from neuron A now and the input 
from other neurons you have an increase 
in the activity. 
You have more spikes from neuron B. 
And so another way of phrasing Hebbs 
learning rule is through that famous 
mantra that you already heard during the 
first week of lectures and that is that 
neurons that fire together wire together. 
Now mantras are great for chanting but 
they're hard to implement on a computer. 
Now let's see if we can formalize Hebb's 
rule as a mathematical model. 
So, let's start with a linear feet 
forward neuron. 
So here's the neuron with an outward v 
and it's receiving some inputs we're 
calling the input vector u and the 
synaptic weights from the inputs to the 
output neuron are given by a synaptic 
weight vector w. 
So this is very similar to the feed 
forward networks that we considered in 
the previous set of lectures last week. 
Now, if we assume that the dynamics of 
this network of the firing fate is fast, 
then we can look at the steady state 
output. 
And that's given by this equation. 
So the output firing rate of the neuron 
is nothing but just the dot product of 
the weights, the synaptic weights with 
the inputs and you can write it as a dot 
product. 
Or you can write it as w transpose u or 
you can write it as u transpose w. 
Now, here's how you can write Hebb's rule 
mathematically. 
You can use a differential equation to 
capture how the rates from the input 
neurons to the output neuron change as a 
function of time. 
So there is some time constant tau sub w 
that governs how fast the weights are 
changing. 
And we set tau sub w dw dt to be equal to 
the product of the input firing rates and 
the output firing rate. 
So how does this capture the intusion 
behind Hebb's rule? 
But remember that in Hebb's rule the 
increase the strength of the connection 
from an input neuron A to an output 
neuron B, if there is both activity from 
neuron A as well as activity from neuron 
B. 
And this product of the input firing 
rates with the output firing rate 
captures that intuition. 
Now in order to implement this 
differential equation on a computer, you 
need to discretize it. 
And so if you look at the discrete 
implementation of this differential 
equation, then this leads you to a eight 
update rule. 
And the weight update rule is shown here. 
So this is how you update the weights 
given inputs. 
And so the weight update rule tells you 
that the weights at time step i plus 1 is 
given by the weights at time step i plus 
some epsilon some positive constant. 
So this is called the learning rate. 
And that is multiplied by u times v. 
Or another way of expressing this 
equation is to say that the change in the 
weight. 
So delta w is equal to the learning rate 
epsilon times uv. 
In order to understand the Hebb rule, it 
is useful to look at the average effect 
of this rule on the synaptic weights w. 
So here is the Hebb rule from the 
previous slide and if you want to look at 
the average effect of this rule, then we 
can take the average of the right-hand 
side with respect to all the inputs u. 
So these brackets over here denote the 
average. 
And if we now substitute the value for v 
from the previous slide again. 
Then what we find is that the Hebb rule 
modifies the weight w according to the 
input correlation matrix, where the 
correlation matrix, as you might know, is 
given by simply the average of uu 
transpose. 
So what does this mean? 
What does it mean to change the weight w 
according to the input correlation 
matrix? 
Well think about that for a minute. 
We will answer this question towards the 
end of the lecture. 
Well the Hebb rule that we've been 
discussing so far only increases synaptic 
weights and this models a phenomenon of 
LTP or long term potentiation in the 
brain. 
But as we discussed earlier the brain 
also exhibits LTD or long term 
depression, which involves decreasing the 
strength of the connection from one 
neuron to another. 
Now can we model both LTP and LTD using a 
single learning rule? 
In other words can we derive a learning 
rule that can both increase or decrease 
the strength of a synaptic connection? 
One rule that incorporates both LTP and 
LTD is the covariance rule and we'll come 
to why it's called that in just a minute. 
Here is the differential equation for the 
covariance rule and you'll notice that it 
is again a product of the input firing 
rate with the output firing rate. 
Except that now the output firing rate as 
a difference term that includes the 
difference between the output firing rate 
and the average of the output firing rate 
so what is the effect this difference 
term? 
Well consider the case when the output 
firing rate is bigger than the average 
output firing rate. 
So in this case you're going to have a 
positive quantity here, which means that 
when you multiply the input firing rates 
with a positive quantity you're going to 
have an increase in the synaptic 
strength. 
And that is going to result in LTP. 
On other other hand, if the output firing 
rate is low, so for example. 
It is less than the average output firing 
rate or even the case where there is no 
output so v could be 0. 
In that case what you're going to get is 
a negative quantity here and so when you 
multiply the input firing rates with a 
negative quantity you're going to get a 
decrease in the synaptic weight. 
And so that results in LTD. 
So what does the Covariance Rule do? 
Well, just as we did with the Hebb rule 
we can look at the average effect of this 
rule. 
And that means taking the average of the 
right hand side of the rule with respect 
to all the inputs u. 
And if you substitute the value for v and 
you simplify these expressions then what 
you get is the fact that the. 
Covariance rule is changing the weight 
vector w according to surprise, surprise 
the input covariance matrix. 
So here's the input covariance matrix. 
It's simply u u transposed, the average 
of that minus the average of u with the 
average of u transposed. 
At this point I would like you to think 
about what it means for w to be changed 
according to the input covariance matrix. 
What do you think w would converge to 
when it's modified according to this 
equation? 
We will answer that question towards the 
end of the lecture. 
Now let's ask the question are these 
learning rules stable? 
In other words does w converge to a 
stable value or does it explode? 
Now how do we answer this question? 
Well one could look at the length of w as 
a function of time and see if the length 
of w remains bounded or if the length of 
w grows without any bounds. 
Let's first look at the Hebb rule. 
So here is the Hebb rule and let's look 
at how the length of w squared changes as 
a function of time. 
So let's take the derivative of the 
length of w squared with respect to time 
and when we do that, we get this 
expression here. 
And if we substitute the value for dw dt 
according to the Hebb rule We have this 
expression. 
And note that w transpose u here is 
nothing but the output firing rate v. 
And if we substitute that value here, we 
get this expression. 
Now unless v is always equal to 0, this 
expression is going to be positive. 
And so what we then have is the fact that 
the derivative of the length of w squared 
with respect to time, is always positive. 
What does that mean? 
It means that the length of w is going to 
keep increasing, which means that w grows 
without bound. 
Well, you might be thinking that's not 
too surprising, because the Hebb rule 
only increases synaptic waves. 
It only models LTP and so perhaps that's 
why w grows without bound. 
Well if that's the case then what about 
the covariance rule? 
So as we discussed, the covariance rule 
incorporates both LTP and LTD. 
And therefore it can both increase 
synaptic ways as well as decrease 
synaptic ways and perhaps that makes the 
covariance rule stable. 
What do you think? 
Do you think it's stable? 
Well, here's the answer and I'm sorry to 
say that it's not good news. 
If you take the derivative of the length 
of w squared with respect to time as 
before and we simplify the resulting 
expression then, if you further take the 
average of the right hand side of that 
expression what you find is that the. 
Derivative of the length of w squared 
with respect to time is always positive. 
And what that means is that the length of 
w when changed according to the 
covariance rule grows without any bound, 
which means that w grows without any 
bound. 
So, how do we stabilize the Hebb rule and 
the covariance rule? 
Well one in which you can do that is by 
forcing a constraint. 
On the synaptic weight vector w. 
So what kind of a constraint can we 
impose? 
Well, you could impose the constraint 
that the length of w should always be 
equal to 1 and how do we do that? 
Well, each time that you update the 
weight vector according to a new input, 
we simply divide the resulting weight 
vector with the length of that weight 
vector. 
And this ensures that the length of the 
weight vector always equals 1. 
Now this seems like a hack and perhaps 
it's not even biologically plausible. 
So is there a more elegant way of 
imposing a constraint on the length of 
the weight vector. 
Now let's look at the last of our Hebbian 
learning rules and this one's called 
Oja's rule named after its discoverer. 
And Oja's rule is similar to the Hebb 
rule in that we again multiply the input 
firing rates with the output firing rate 
except that now we subtract a term alpha 
v squared w from u times v and alpha is 
some positive value. 
Now the question is, is Oja's rule 
stable? 
What do you think? 
Well, let's do what we did before, which 
is take the derivative of the length of w 
squared with respect to time. 
So when we do that, we get this 
differential equation for the length of w 
squared. 
So, looking at this differential 
equation, do you think that the length of 
w squared converges to a particular value 
or do you think that the length of w 
squared grows without bound. 
Well, here's the answer. 
So length of w squared in fact does 
converge to a particular value and it 
converges to the value 1 or alpha. 
And you can see that by setting the 
derivative equal to zeros in that case 
unless v is equal to 0 we have the fact 
that the length of w squared is equal to 
1 over alpha because this term over here 
has to be equal to 0. 
And if that's the case then the length of 
w itself must be equal to 1 over square 
root of alpha. 
So what this tells us is that w for Oja's 
Rule does not grow without bound, which 
means that the rule is stable. 
Okay, let's summarize what we've learned 
so far about Hebbian learning. 
The basic Hebb rule involves multiplying 
the input firing rates with the output 
firing rate and this models the 
phenomenon of LTP in the brain. 
We found out that this learning rule is 
unstable unless we impose a constraint on 
the length of w after each weight update. 
The covariance rule involves multiplying 
u with v minus the average value of v, 
which means that we can now model both 
LTP and LTD. 
But we found out that that's not 
sufficient to make the learning rules 
stable. 
So this learning rule is also unstable 
unless we impose a constraint on the 
length of w. 
And finally we considered Oja's rule and 
we found out that Oja's rules in fact 
stable and the length of the weight 
vector converges to the value 1 over 
square root of alpha. 
Okay, we've arrived at the finale of the 
lecture we going to answer the question 
what does Hebbian Learning do anyway. 
We going to start with the averaged Hebb 
rule so as you recall the averaged Hebb 
rule is given by this differential 
equation where Q is the input correlation 
matrix. 
And what we would like to do is solve 
this differential equation defined wt. 
So what is w as the function of time when 
its being changed according to this 
differential equation. 
So how do we solve this equation? 
Any ideas? 
Well, if you guessed eigenvetors, you 
would be right. 
We can always rely on our dear friends, 
the eigenvectors. 
So, as before, let's write our vector wt 
in terms of the eigenvectors of the 
correlation matrix. 
Now recall that the input correlation 
matrix is going to be a real and symetric 
matrix which means that the eigenvectors 
are going to be orthonormal, which means 
that we can write any vector including 
the vector wt. 
As a linear combination of the 
eigenvectors. 
Now if we substitute our expression for 
wt in the differential equation for the 
average Hebb rule, then we can simplify 
as before and we can get this 
differential equation for the 
coefficients. 
And when we solve the differential 
equation for the coefficient, let's say 
ci, then we have this solution. 
And when we substitute this solution into 
our expression for wt, then we get this 
solution for the weight vector as a 
function of time. 
So, what is this equation telling us 
about the synaptic weight vector w as a 
function of time? 
It's telling us that the synaptic weight 
vector w is a linear combination of the 
eigenvectors of the input correlation 
matrix. 
And furthermore, it's telling us that the 
coefficients for these eigenvectors have 
terms that are exponentially dependent on 
the eigenvalues of the correlation 
matrix. 
So what do you think will happen to w as 
time goes on? 
So when t becomes very large, what do you 
think will happen to w? 
When t becomes large, the largest 
eigenvalue terms so that one that has the 
largest eigenvalue. 
Lets say it's the eigen value lamba 1 is 
the largest eigen value then that term 
dominates this linearly combition so what 
we get. 
Then is the result that the rate vector 
turns out to be proportional to the first 
eigenvector or the principal eigenvector 
of the input correlation matrix. 
And furthermore, if we're using Oja's 
rule as you know, the length of the 
weight vector then converges to 1 square 
root of alpha. 
So in that case, the weight vector 
approaches the value e1 divided by square 
root of alpha. 
We've actually shown something very 
exciting. 
We've shown that the brain can actually 
do statistics and that's in addition to 
what we showed last week which was that 
the brain can do calculus. 
There seems to be no stopping the brain. 
Well, let's look at why we think the 
brain does statistics. 
So it turns out the Hebbian learning rule 
that we just analyzed implements the same 
thing as the statistical technique of 
principal component analysis or PCA. 
So to understand what principal component 
analysis is all about let's look at a 
simple example. 
So here is some two dimensional data. 
We have these points which represent the 
values you want and u2, which comprise 
the input vector u. 
And if we start the Hebb rule with an 
initial weight vector that's given by 
this dashed line, then the Hebb rule 
rotates this initial weight vector to 
align itself with the direction of 
maximum variance. 
So here is the. 
Cloud of data and the final weight vector 
is going to be parallel to this line 
which is the direction of maximum 
variance. 
Now when we apply the Hebb rule to some 
data that has been shifted so the data 
from here can be shifted to a different 
location. 
Let's say with input mean. 
Two and two. 
So, in that case we find that the Hebb 
rule does not do what we want it to do. 
which is, it finds this direction as the 
direction of maximum variance going 
through the origin of this two 
dimensional plot. 
And that is really not the direction of 
maximum variance, the direction of 
maximum variance is given again by. 
This direction but luckily when we apply 
the equal variance rule we find that it 
does indeed find the direction of maximum 
variance. 
So it's taken care of the fact that the 
input mean is no longer 00 but it's 2 and 
2 and that is accounted for by the equal 
variance rule. 
So the equal variance based Hebb rule is 
able to find again the direction of 
maximum variance. 
So in summary what we have shown is that 
Hebbian learning learns a weight vector 
that is aligned with the principal 
eigenvector of the input correlation or 
the input covariance matrix. 
In other words, it finds the direction of 
maximum variance in the input data. 
And that is precisely what principal 
component analysis does but now why is 
that interesting? 
Well, principal component analysis is a 
very important technique used in a 
variety of fields for tasks such as 
damage [INAUDIBLE] reduction. 
So for example here what we've done is 
we've shown that this two-dimensional 
data can be compressed to just one 
dimension by projecting each of these two 
dimensional points onto their 
corresponding locations along this 
particular line. 
And so we now have a compression from 2 d 
to the 1 d location along this particular 
line and that's an example of 
dimensionality reduction or compression. 
And you can imagine that when we have a 
very large input dimension, such as the 
number of pixels in an image. 
Then this type of technique where we find 
the directions of maximum variance in 
natural images or natural movies is 
indeed going to be extremely useful. 
Because you can compress a very high 
dimensional space such as the space of 
the input image or the space of the input 
video to may be a very small number of 
principle eigenvectors the dominant 
eigenvectors of the input covariance 
matrix. 
Well that's great but what if we give a 
neuron this data, what do you think the 
weight vector for the neuron will 
converge to if we apply the covariance 
learning rule? 
As you might have guessed the covariance 
rule ends up finding the weight vector 
that is aligned with the direction of 
maximum variance in this data set. 
Now unfortunately as many of you will 
agree this data set seems to consist of 
two clusters of data points. 
So here's one cluster and here's the 
other. 
And so it appears that this particular 
data set is not correctly modeled by 
principal component analysis. 
So just finding the directional maximum 
variance through these two clusters 
doesn't seem to provide us with a very 
satisfying model of this particular 
dataset. 
So the question that I would like to 
leave you with is what should a network 
of neurons learn from such data? 
This will be the topic of our next 
lecture. 
And we will encounter the interesting 
alogrithm known as competitive learning 
and this will allow us to segue into 
generative models. 
And this will in turn lead us into the 
exciting field known as unsupervised 
learning. 
So until then, hasta la vista and 
goodbye. 

Welcome back In the previous lecture, we 
considered this dataset and we asked the 
question, can a network of neurons learn 
to represent such data? 
When we applied the co-variance rule as 
you'll recall, to this dataset, it ended 
up finding a weight vector, that was 
aligned with the direction of maximum 
variance of this dataset. 
But we noted at the time, that this was 
not a very satisfying model of this data 
set, because the input data appear to be 
consist of these two clusters of data 
points. 
So lets ask the question, can neurons 
learn to represent such clusters? 
Here is one way in which we can use 
neurons to represent clusters. 
So lets use a feedforward network with 
two output neurons, neuron A and neuron 
B. 
And lets use neuron A to represent 
cluster A, and neuron B to represent 
cluster B. 
So we can do that by making the weight 
vector WA, be the center of cluster A, 
and the weight vector WB, be the center 
of cluster B. 
And so now since this is the feedforward 
network, so here is the input component 1 
and input component 2. 
So U1 and U2 together comprise the vector 
for the input U. 
Here is the output of each of these two 
neurons, so it's just the dot product 
between the weight vector and the input 
vector. 
So the question that I would like to ask 
you is, if I give you a particular input 
such as this one here, which neuron do 
you think will fire the most? 
In other words, which neuron will have a 
higher output firing rate? 
Is it neuron A or neuron B, for this 
particular input? 
Notice that this particular input is 
closer to neuron B. 
So the distance from here to the center 
of the cluster, so that is the distance 
from this input to WB, seems to be 
shorter than the distance from this input 
to the weight vector WA, which is the 
center of cluster A. 
So, which neuron do you think will have a 
higher activity? 
Neuron A or neuron B? 
If you answered neuron B you would be 
correct. 
The most active neuron in the network, is 
going to be the one whose weight vector 
is closest to an input. 
So in this case, for this particular 
input U, the closest weight vector is WB, 
and therefore, the most active neuron is 
also going to be the neuron B. 
And we can show that by looking at the 
Euclidean distance between the input and 
each of these two weight vectors, WA or 
WB. 
And so the square of the Euclidean 
distance turns out to be equal, if you 
simplify all these terms. 
It turns out to be equal to be to the 
square of the length of the input vector, 
plus the square of the length of the rate 
vector, minus 2 times the output activity 
of the neuron. 
And so if we assume that the length of 
the input vector has been normalized to 
have length 1, and similarly the length 
of the weight vector has been normalized 
to also have, let's say a length of 1. 
Then minimizing the squared distance 
between the input and the weight vector, 
turns out to be the same as maximizing 
the activity of that particular neuron. 
Now suppose I give you a new input, UT. 
And here's that new input. 
How will you update the weights of the 
two neurons, given this new input UT? 
Well lets think about that for a little 
bit. 
So the first thing that we need to do, is 
perhaps figure out which cluster this new 
input belongs to. 
Is it cluster A, or cluster B? 
And we can do that, by looking at the 
distance between that new data point in 
each of these centers of the clusters. 
So in this case, the distance between WA, 
and this new data point as well as WB in 
that particular data point, and it 
appears in this case that the cluster A 
is the one that this new input might 
belong to. 
Because the distance from that input to 
the center of the cluster, is the 
shortest. 
And so now that we've figured out which 
cluster this new input might belongs to, 
we can update the weight, which is now 
the center of the cluster, to now include 
this new data point. 
So how would we do that? 
One way of doing that is to set the 
weight vector to be the running average 
of all the inputs, in that particular 
cluster. 
So the running average of all the inputs 
in this cluster, including this new 
input. 
So do you remember the equation for 
computing the running average? 
Well here it is. 
So we can derive the equation for the 
running average, by starting out with the 
expression for the average, which all 
know, which is just the summation of the 
data points in this cluster A, divided by 
the number of such points, which we're 
calling T. 
And so if we express the sum now, as the 
sum of all the data points except for the 
data point T, and then we simply it. 
What we find is an equation that has both 
the weight vector before we got the new 
data point, plus this additional term, 
which includes the new data point. 
And so we can now write this weight 
update rule. 
The delta W, which is the change in the 
weight vector W for the neuron A, is 
equal to some epsilon times just the 
difference between the new input, and the 
weight vector for that particular neuron. 
Now we can epsilon to be equal to 1 over 
T, and that would make this equation 
compute the running average. 
Or you can keep epsilon as some small 
positive value, and that allows the 
method to adapt to new inputs for an 
indefinite period of time. 
So the 1 over T would make epsilon go to 
0 for very large T. 
But if you keep epsilon to be some 
constant positive value, then that allows 
the algorithm to remain adaptive to new 
inputs, for an indefinite period of time. 
Okay, we're now ready to make our 
acquaintance with the competitive 
learning algorithm. 
Which is a famous algorthim in neural 
networks thoery. 
So in competitive learning, given a input 
we pick the most active neuron in the 
network. 
And this we can do, for example, by using 
the winning takes all network that we 
discussed in our previous lecture on 
recurrent networks. 
And as we discussed earlier, the most 
active neuron also corresponds to the one 
whose weights are closest to the new 
input. 
And once we have a winning neuron, we can 
then update the weights for that neuron, 
and we do that using the weight update 
rule from the previous slide. 
Now what is the effect of this rule on 
the weights for the winning neuron? 
Well, so here is what that looks like. 
So initially the weight vector for the 
winning neuron, in this case, neuron A, 
was here in the center of the cluster. 
And the effect of this learning rule this 
weight update rule, is to move the weight 
vector slightly towards the direction of 
the new input. 
And in doing so, what we're doing is 
updating the weights to be equal to the 
average, of all the data points in this 
cluster including the new data point. 
Okay lets look at an example of 
competitive learning. 
Suppose you have these green points as 
the input, and these are 2-dimensional 
points. 
So these constitute the U1 and U2, in the 
input vector. 
And suppose you have three output 
neurons, and the output neurons have the 
weight W1, W2 and W3. 
And lets randomly assign the values for 
these weights. 
So W1 turns out to be here, W2 turns out 
to be here, and W3 down here. 
Lets look at what happens to these 
weights, as we give the network these 
green inputs one by one. 
So suppose I give the network this input 
first. 
How do you think these weights will be 
changed? 
Well, according to the competitive 
learning algorithm, the closest weight to 
this particular input is W1, and 
therefore W1 is moved to be closer to 
that particular input. 
Now, if the second input let's say is 
this one, then again the winning neuron 
is going to be neuron number 2, so W2 is 
going to be adapted, and it will move 
closer to that particular input. 
And finally, if the 3rd input lets say is 
this one, then as you would expect the 
winning neuron is going be neuron number 
3, and so it's weight is going to be 
adapted to be closer to that particular 
input. 
Now if we keep repeating this procedure 
for more than just these 3 inputs, than 
here's what the weights might look like 
after you've updated them, using the 
competitive learning rule. 
As you can see, the three neurons have 
partitioned the data set into three 
different clusters, given by the red 
points, the blue points, and the green 
points. 
And the weight vectors of the three 
neurons, W1, W2, and W3, now represent 
the centers of these three clusters. 
Competitive learning is also closely 
related to self organizing maps, also 
known as Kohonen maps after the Finnish 
professor who first proposed these maps. 
And in self organizing maps, just as in 
competitive learning, we pick the winning 
neuron given in any particular input. 
And as in competitive learning, we update 
the weights for the winning neuron. 
But unlike competitive learning, we also 
update the weights of other neurons, 
which are in the neighborhood of the 
winning neuron. 
So, what do we mean by the neighborhood 
of the winning neuron? 
Well, in self organizing maps, we have 
locations assigned to the neurons in the 
network. 
And so each of these would correspond to 
a location on a grid, such as this 
2-dimensional grid, that are assigned to 
the individual neurons. 
And so when we have some inputs such as 
the inputs shown here, in this cloud of 
data, then we not only update the weights 
of this individual neuron. 
So we would move the weights for this 
winning neuron towards that particular 
data point, but we'd also update the 
weights of the neighbors. 
So this neuron, the one here and the one 
here, we would update these weights also 
to be closer to that particular data 
point, resulting in a transformation of 
the 2-dimensional grid, to look something 
like this. 
And so this morphing of the grid is a 
result of updating the weights of the 
winning neuron and It's neighbors. 
and if we keep doing that for all the 
different points on this data cloud, so 
this could be a very high dimensional 
cloud of data, then we end up with a 
representation that looks something like 
this. 
And here you can see that we have now 
transformed a very high dimensional data 
set, a potentially high dimensional data 
set. 
We've mapped it now onto a 2-dimensional 
representation, that preserves the 
topological properties of the input 
space. 
Interestingly, in the brain, one also 
finds a 2-dimensional map that preserves 
the topological properties of the input 
space. 
In particular if you look at the primary 
visual cortex or V1, we find are called 
orientation preference maps. 
And here's an example of an orientation 
preference map, where each of these 
colors represents one particular 
orientation, that the neurons in that 
particular region of cortex prefer. 
And so if you're recording from one 
particular location, let's say in blue 
here, and you move along in a particular 
direction, you find that the neighboring 
neurons prefer similar orientations. 
And that's very similar to the fact that 
in the self organizing map also, 
neighboring neurons are going to prefer 
similar inputs. 
Okay lets summarize what we've done so 
far. 
We first considered PCA or principle 
component analysis, as implemented by the 
Hebb rule or the Covariance rule. 
And we found that when the covariance is 
applied to a dataset, such this cloud of 
points here, it ends up learning a weight 
vector W, that is aligned with the 
direction of maximum variance of the 
dataset. 
On the other hand, the competitive 
learning model, learns to represent 
clusters of data, using the weight 
vectors WA and WB in this case, of two 
output neurons. 
So, here we have two different ways of 
modelling data. 
One in terms of principal component 
analysis, and the other in terms of these 
clusters. 
This should lead you to ask the question, 
is there a principled way of learning 
models of input data? 
Well, the answer as you might have 
guessed, is a resounding yes. 
There's a whole field, called 
unsupervised learning, that addresses 
this question. 
In unsupervised learning you assume that 
your inputs, these data points u, are 
being generated by a set of hidden 
causes, that we called v. 
And the relationship between the causes 
and the data point or inputs that you're 
observing, is given by a generative 
model. 
Now the causes are selected according to 
some prior distribution. 
So the prior probability distribuation of 
the causes. 
And there's a set of parameters here G, 
that we would like to learn. 
And once you have a particular cause that 
is selected according to this 
distribution, then the generative model 
assumes that the data point u is 
generated according to some likelihood 
function, that's given by p of u given v 
and again G is some set of parameters. 
So what is the unsupervised learning 
problem then? 
Well the unsupervised learning problem 
comes down to learning these parameters 
G, that characterize both the likelihood 
function, as well as the prior property 
distribution of the causes. 
Let's look at what this means for our 
example of the two clusters. 
So, we could use a generative model in 
this case, which assumes there are two 
Gaussians that are generating these data 
points. 
And each of these two Gaussians has a 
mean and a standard deviation. 
And so the way that these points would be 
generated then, according to this 
generative model, is that you first 
select one of these two Gaussians 
according to some prior property 
distribution given by p of v. 
And given a particular Gaussian, then you 
generate one of these points within that 
cluster. 
We can now write the property 
distribution of all of our inputs, as a 
mixture of Gaussians. 
So here's the expression for the mixture 
of Gaussians. 
And you can see that it contains the term 
corresponding to the Gaussian for each of 
the Gaussians in our model, as well as 
the prior Property for that particular 
Gaussian. 
And therefore the parameters for this 
particular generative model, includes the 
mean and standard deviation for each of 
the Gaussians, as well as the prior 
property which we are calling Gamma, for 
each Gaussian. 
So, to summarize, the goal of 
unsupervised learning, is to learn a good 
generative model for the data that you're 
seeing. 
In other words, we would like to mimic 
the data generation process. 
And we find that in general, if you want 
to do that, we need to solve two 
sub-problems. 
So what are these two sub-problems? 
The first one corresponds to the problem 
of recognition. 
So we mean by recognition? 
We mean that we would like to estimate 
the causes v, given any particular input. 
So for any particulate input, we would 
like to estimate the posterior property 
of the causes given the particular input 
u. 
And once we have done that, it turns out 
we can typically use that information to 
learn the parameters G. 
So let's look at an example of how this 
approach can be applied to a specific 
unsupervised learning problem. 
Lets look at the problem of clustering. 
So as you recall, we were modelling this 
particular data set of two clusters, as a 
mixture of Gaussians. 
And therefore, the parameters that we 
would like to learn are the mean and 
standard deviation of each Gaussian, as 
well as the prior property for each 
Gaussian. 
So how does the approach from the 
previous slide apply to this problem? 
Well given a particular data point such 
as this one, the first sub problem 
involves finding out the posterior 
property of the causes, given this data 
point and the parameters g. 
And the second problem then, is given the 
posterior property distribution over 
causes, for that particular data point, 
update the parameters g. 
This is actually very similar to 
competitive learnings. 
So in competitive learning as you recall, 
you also had two steps. 
In the first step, given a data point 
such as this one here, we would assign 
the data point to a specific cluster. 
So in this case, we would assign this 
data point to cluster A. 
But now we're going to compute the 
posterior property of this data point 
belong to either cluster A, or cluster B. 
And so presumably in this case we would 
have a higher posterior property, that 
this data point belongs to cluster A. 
So p of v given u and G, would have a 
higher property when v is equal to A, 
than when v is equal to B. 
And the second step in competitive 
learning was, changing the weight only 
for the cluster A. 
Where as now, we're going to use the 
posterior property to then change the 
parameters for both the Gaussian for 
cluster A and the Gaussian for cluster B. 
So let's see how we can do that. 
The algorithm for learning the parameters 
is called the Expectation Maximization 
algorithm or EM algorithm for short. 
The EM algorithm is actually a very 
general algorithm for unsupervised 
learning, and it has its rules in 
statistics. 
It involves a trading between two steps, 
one is called the E step or the 
expectation step, and the other is called 
the M step, or the maximization step. 
So lets look at what these two steps mean 
in the case of our example of clustering. 
So here's the E step. 
And as you might expect, if you pardon 
the pun, the E step involves computing 
the posterior property distribution of 
the causes. 
Which in this case is whether the cause 
is Gaussian A or Gaussian B, and we can 
do that by using base rules. 
So you can compute the posterior property 
of the cause, given the input by using 
base rule, and we can substitute the 
normal or the Gaussian distribution, for 
each of the likelihood functions for our 
inputs. 
And so we end up with an expression that 
looks like this. 
And you can see that this expression, 
implements a form of sort competition, so 
it's not a winner takes all type of 
competition. 
It's a much democratized version of 
competition, where we assigning a 
posterior property for each cause, as 
opposed to just declaring one these 
clusters, or these causes to be the 
winner. 
And here's the M step. 
And the M step involves changing the 
parameters G, using the results from the 
E steps. 
So here are the equation for changing the 
mean, the variance, and the prior 
property for each of our Gaussians. 
And you can see how the equations used 
the posterior property for each Gaussian, 
computed in the E step. 
It's interesting to compare this 
algorithm to competitive learning. 
So in the case of competitive learning, 
we only learn the mean of each cluster. 
And so we did not have an estimate of the 
variance of each cluster, or the prior 
property of each cluster. 
The other difference with competitive 
learning is that, the EM algorithm 
assumes you have all the data points at 
once. 
And so you can use all the data that 
compute estimates such as this. 
Whereas, in the case of competitive 
learning, we got the input one by one, 
and so we can call the EM algorithm a 
batch learning algorithm. 
Whereas, the competitive learning 
algorithm was an online learning 
algorithm. 
Here is the results for applying the EM 
algorithm to our data set of two cluster. 
The dash line here, represents the 
trajectory of the mean, and the circles 
represent two standard deviations from 
the mean, for each of the Gaussian A and 
B. 
And so as you see here, with each 
iteration, the estimate of the mean gets 
better and better, as does the estimate 
of the standard deviation. 
Until we get to iteration number 50, 
where you can see that the mean of each 
distribution is quite accurately modeled, 
and the standard deviation of each of 
these clusters seems to approximate quite 
well, the standard deviation of the 
Gaussians that generated the data. 
Okay that's great but so far we've been 
looking at these toy examples of 2 
dimensional data. 
What is the data are natural images such 
as these. 
I'd like to ask you two questions. 
What kind of generative model do you 
think would be useful, for modelling 
these kinds of images? 
and second how would you learn the causes 
of such images? 
Now don't lose too much sleep over this, 
because we will be addressing these very 
questions in our next lecture. 
So until then, ciao and adios. 

Hello, Comp-neuro fans. 
In the previous two lectures, we made 
friends with HEB and learned that has 
learning rule implements principal 
component analysis. 
We then learned about unsupervised 
learning, which tells us how the brain 
can learn models of its inputs with no 
supervision at all. 
I left you with the question. 
How do we learn models of natural images? 
What does the brain do? 
Well, as the saying goes, when in doubt, 
trust your Eigenvectors. 
Well, can we use Eigenvectors or 
equivalently principal component analysis 
to represent natural images. 
Well, let's see. 
So, here is a famous example from Turk 
and Pentland. 
And what they did was, they took a bunch 
of face images, and let's say that each 
of these face images has n pixels. 
So, they took a bunch of face images and 
they computed the Eigenvectors of the 
input covariance matrix. 
And when they did that they found that 
the eigenvectors look like this, and they 
call these eigenvectors eigenfaces. 
Now, we can represent any face image such 
as this one, as a linear combination of 
all of our eigenfaces. 
So, here is the equation that captures 
this relationship. 
Now, why can we do that? 
Well, remember that the eigenfaces are 
the eigenvectors of the input covariance 
matrix. 
And since the covariance matrix is a real 
and symmetric matrix, its eigenvectors 
form an orthonormal basis with which we 
can represent these input vectors. 
Now, here's something interesting. 
You can use only the first M principal 
eigenvectors. 
So what do we mean by the first M 
principal eigenvectors? 
Well, these are the eigenvectors 
associated with the M largest eigenvalues 
of the covariance matrix. 
So, if we use only the first M principal 
eigenvectors to represent the image, then 
we get an equation that looks like this. 
And this equation just tells you that 
there are some differences between the 
reconstruction of the image using only 
the first M principal eigenvectors, and 
therefore we're going to model those 
differences between the actual image and 
the reconstructed image using a noise 
term. 
So, why is this a useful model? 
It's a useful model because you can use 
it for image compression. 
So, suppose your input images were of 
size thousand by thousand pixels, which 
means that N is going to equal 1,000,000 
pixels. 
And now if the first, let's say ten 
principal eigenvectors are sufficient. 
Which means that the first ten largest 
eigenvalues are sufficient to explain 
most of the variance in your data, then M 
is going to equal ten. 
Which means that just ten numbers are 
enough to represent an image. 
So ten of these coefficients are 
sufficient to represent any image which 
consists of 1 million pixels. 
So, what we have done is a tremendous 
dimensionality reduction or compression 
from one million pixels down to just 10 
numbers for each image. 
Now, wait a minute. 
Not so fast, eigenvectors. 
The Eigenvector representation may be 
good for compression, but it's not really 
very good if you want to extract the 
local components, or the parts of an 
image. 
So, for example, if you want to extract 
the parts of a face, such as the eyes, 
the nose, the ears. 
You're not going to get that from an 
eigenvector analysis, or equivalently, a 
principae component analysis of the face 
images. 
And likewise, you're not going to be able 
to extract the local components, such as 
edges, from natural scenes. 
Now, this is certainly a sad day for the 
course, because Eigenvectors have letters 
down for the first time. 
But maybe we can resurrect the linear 
model so beloved to the eigenvectors. 
So, here's the linear model. 
We have a natural scene, for example, 
that is represented by a linear 
combination of a set of basis vectors or 
features. 
So, these do not have to be eigenvectors 
anymore. 
And so, here is the equation again that 
captures this relationship. 
And the difference now from the case of 
Eigenvectors that we had in the previous 
slide is that we are allowing M, the 
number of these bases vectors or features 
to be larger than the number of pixels. 
So why does that make sense? 
Well, consider the fact that the number 
of parts of objects and scenes can be 
much larger than the number of pixels. 
So it does make sense to allow a larger 
value for M, the number of basis vectors 
and features, than the number of pixels. 
And here's another way of writing the 
same equation. 
So, we're replacing the summation with a 
matrix multiplication G times v where the 
columns of this matrix G are the 
different basis vectors or features. 
And the vector v has elements which are 
the coefficients for each of those basis 
vectors or features. 
So, the challenge before us now is to 
learn this matrix G, the different basis 
vectors, as well as for any given image, 
we need to be able to estimate the 
coefficients, this vector, v. 
In order to learn the basis vectors G and 
estimate the causes v, we need to specify 
a generative model for images. 
And as you recall, we can define the 
generative model by specifying a prior 
probable distribution for the causes, as 
well as a likelihood function. 
Let's first look at the likelihood 
function. 
So, we start with our linear model from 
the previous slide, and if you assume 
that the noise vector is Gaussian. 
And it's a Gaussian white noise, which 
means there are no correlations across 
the different components of the noise 
vector. 
And if you assume that the Gaussian has 
zero mean, then we can show that the 
likelihood function amounts to also a 
Gaussian distribution with a mean of g 
times v, and a covariance of just the 
identity matrix. 
And here is what this likelihood 
function, then is proportional to is its 
exponential function. 
And finally, if you take the logarithm of 
the likelihood function, we obtain the 
log likelihood which now is simply just 
this quadratic term. 
So it has just a negative one half of the 
square of the length of this vector, 
which is simply the difference between 
the input image and the reconstruction of 
the image, or the prediction of the image 
using your basis vectors. 
Now here's an interesting observation, a 
lot of algorithms in engineering and in 
machine learning attempt to minimize the 
squared reconstruction error which is 
just this term here. 
And so now you can see that when you are 
minimizing the reconstruction error is 
the same thing as maximizing the log 
likelihood function, or equivalently, 
maximizing the Likelihood of the data. 
Isn't that interesting? 
Now, let's define the prior probability 
distribution for the causes. 
So, one assumption you can make is that 
the causes are independent of each other. 
And if you make that assumption, then we 
have the result that the prior 
probability for the vector v is equal to 
just the product of the individual prior 
properties for each of the causes. 
Now, this assumption might not strictly 
hold for natural images, because some of 
the components might depend on other 
components. 
But let's start off with this simplifying 
assumption and see where it takes us. 
Now, if you take the logarithm of the 
prior probability distribution for v, 
then instead of a product we now have the 
summation of all the individual log prior 
probabilities for the causes. 
Now the question is, how do we define 
these individual prior probabilities for 
the causes. 
Now, here's one answer, we can begin with 
the observation that: for any input, we 
want only a few of these causes vi to be 
active. 
Now why does this make sense? 
Well, if we are assuming that these 
causes represent individual parts or 
components of natural scenes. 
Then for any given input which contains 
for example, a particular object only a 
few of these causes are going to be 
activated in that particular image, 
because those are the parts of that 
particular object, and then the rest of 
the vis are going to be zero. 
So, what we have then is that vi, for any 
particular i, is going to be zero most of 
the time, but is going to be high, for 
some inputs. 
And this leads to the notion of a sparse 
distribution for pvi. 
And what this means is that the 
distribution for pvi is going to have a 
peak at zero. 
So, it's going to be zero, most of the 
time, vi is going to be zero most of the 
time. 
But the distribution is going to have a 
heavy tail, which means that for some 
input it's going to have a high value. 
And this kind of a distribution is also 
called a super-Gaussian distribution. 
Now here are some examples of the 
super-Gaussian or sparse prior 
distribution. 
So, this plot here shows three 
distributions. 
All of them can be expressed as pv equals 
exponential of gv, and the dotted 
distribution here is the Gaussian 
distribution. 
And the other two distributions, the dash 
as well as the solid line here, represent 
the examples of sparse distributions. 
And if you take the log of pv, we get a 
more clear picture here of what these 
distributions look like. 
So, you can see that when gv equals minus 
the absolute value of v, then we get an 
exponential distribution. 
And when we have gv equals minus the 
logarithm of 1 plus v squared, we get 
something called the Cauchy Distribution. 
So, to summarize then, the prior 
probability pv is equal to just the 
product of these exponential functions. 
And therefore the logarithm of the prior 
probability pv is going to equal the 
summation of all these values, g v i plus 
some constant. 
Okay, after all that hard work, we've 
finally arrived at the grand mathematical 
finale of figuring out how to find v 
given any particular image and how to 
learn G. 
And we're going to use a Bayesian 
approach to do that. 
So, by Bayesian we mean that we going to 
maximize the posterior property of the 
causes, so here is p of v given u. 
And from Bayes' rule we can write p of v 
given u as just the product of the 
likelihood times the prior. 
And k here is just the normalization 
constant. 
We can maximize the posterior property by 
also maximizing the log posterior, so 
that's the same thing as maximizing the 
posterior. 
And so here's the function f, which is 
the log posterior function, and you can 
see how the function f has two terms. 
One of them is a term containing the 
reconstruction error, the other is a term 
containing sparseness constraint, and we 
can maximize this function by essentially 
doing two things. 
We have to minimize the reconstruction 
error while at the same time trying to 
maximize the sparseness constraint. 
And so, you can see how this function f 
trades off the reconstruction error with 
the sparseness constraint. 
And so, we would like our representation 
to be sparse. 
We would like only a few of these 
components to be active, while at the 
same time we would also like to preserve 
information in the images. 
And that's enforced by this 
reconstruction error term. 
One way of maximizing F with respect to v 
and G is to alternate between two steps. 
The first step is maximizing F with 
respect to v, keeping G fixed. 
And the second step is maximizing F with 
respect to G, keeping v fixed to the 
value obtained from the previous step. 
Now, this should remind you of the EM 
algorithm. 
So just as in the EM algorithm, in the E 
step, the computed the posterior property 
of v. 
Here we're computing a value for v that 
maximizes F. 
And similar to the EM algorithm where, in 
the M step, we updated the parameters 
here, we're updating the parameter G, the 
matrix G to maximize the function F. 
Now, the big question is, how do we 
maximize F with respect to v and G. 
Well, one potential answer is to use 
something called gradient ascent, which 
is that we change v, for example, 
according to the gradient of F with 
respect to v. 
So why does this make sense? 
Here's why it makes sense. 
So, let me draw F as a function of v. 
So, suppose F is this function, and you 
can see that the value of v, which 
maximizes F is some value here. 
So, let's call that v star. 
And if the current value of v is, let's 
say to the left of v start, let's say 
that this is where the current value of v 
is, you can look at the gradient of F 
with respect to v. 
So, you can see that it's the slope of 
this tangent here. 
Do you think the gradient is positive or 
negative at this particular value? 
Well, if you answered positive, you would 
be correct. 
So, it is a positive value, so what does 
that mean? 
It means that if you update v according 
to this equation, then you're going to 
move v this direction. 
So you're going to add a small positive 
value to v, and that's going to move v in 
the right direction towards the start. 
Similarly if you're on this side, let's 
say this is where your current value of v 
is. 
I'm calling that v prime. 
Then you can see that the gradient is in 
this case, you guess right it's negative. 
Which means that you're going to subtract 
a small value from your current value, 
and that's going to move the value of v 
again towards the optimal value. 
So either way Gradient ascent does the 
right thing. 
Okay, let's apply the idea of gradient 
ascent then to our problem. 
So, you would like to take the derivative 
of F with respect to v, and here's the 
expression that we get. 
So G prime here denotes the derivative of 
our function G. 
And, we can now look at the way in which 
we should update the vector v, and that's 
given by this differential equation with 
some time constant. 
And the interesting thing to note here is 
that we can interpret the differential 
equation that we have here for v as 
simply the firing rate dynamics of a 
recurrent network. 
And so, what does this network do? 
It takes the reconstruction error, and it 
uses it to update the activities of the 
recurrent network. 
And it also takes into account the 
sparseness constraint that encourages the 
output activities to be sparse. 
And here is the recurrent network that 
implements our differential equation for 
v. 
And so you can see how it has both an 
input layer of neurons and an output 
layer of neurons. 
But the interesting observation here is 
that the network makes a prediction of 
what it expects the input to be, so G 
times v is a prediction or a 
reconstruction of the input. 
And then we take an error, so u minus Gv 
is the reconstruction error or the 
prediction error. 
And that is then passed back to the 
output layer. 
And the output layer neurons then use the 
error to correct the estimates they have 
of the causes of the image as given by 
the vector v. 
For any given image, the network iterates 
by predicting and correcting, and 
eventually converges to a stable value 
for v for any given image. 
We can learn the synaptic weight matrix 
G, which contains the basis of vectors or 
the features that we are trying to learn, 
by again applying the gradient ascent. 
So, we can set dG/dt proportional to the 
gradient of F with respect to G. 
So when we take the derivative of F with 
respect to G, you're going to get an 
expression that looks like this. 
So, it's u minus Gv times v transpose, 
and so what we end up with then is this 
learning rule for updating the synaptic 
weights G. 
It has a time constant tau G, and that 
specifies the timescale at which we're 
going to update the weights G. 
And so if we set tau G to be bigger than 
the time constant we had for v, that 
ensures that v converges faster than G. 
And so, we have the desired property that 
for any given image, v will converge fast 
to some particular value. 
And then, we can use that value for v to 
then update the weights for the network. 
Now, if you look closely at the right 
hand side of the learning rule, you'll 
see that it's actually Hebbian. 
So you can see how it contains the term u 
times v. 
So, u times v transpose is basically the 
Hebbian term. 
Now, it also contains a subtractive term, 
and that actually makes this rule very 
very simila. 
In fact almost identical to the Oja rule 
for learning. 
So, if the learning rule is almost 
identical to Oja's rule, why doesn't this 
network then just compute the 
eigenvector? 
So, why isn't it just doing principal 
component analysis? 
Well, the answer lies in the fact that 
the network is actually trying to compute 
a sparse representation of the image, and 
so that ensures that the network does not 
just learn the eigenvectors of the 
covariance matrix. 
It's actually learning a set of basis 
vectors that can represent the input in a 
sparse manner. 
Okay, so here is a pop quiz question. 
If you feed your network some patches 
from natural images, what do you think 
the network will learn in its matrix G? 
What kind of bases vectors would you 
predict are learned for natural image 
patches? 
Time for the drum roll. 
The answer as first discovered Olshausen 
and Field, is that the basis vectors 
remarkably resemble the receptive fields 
in the primary visual cortex, as 
originally discovered by Hubel and 
Wiesel. 
So, each of these square images is one 
vector or one column of the matrix G. 
So, you can obtain a vector from the 
square image by collapsing each of the 
rows of the square image into one long 
vector, and that would be one column of 
the matrix G. 
So, what is this result telling us? 
It's telling us that the brain is perhaps 
optimizing its receptive fields to code 
for natural images in an efficient 
manner. 
You can look at this model as an example 
of an interpretive model. 
So, this is going back to the first week 
of our course, where we discussed the 
three different kinds of models in 
computational neuroscience. 
So this would be an example of an 
interpretive model that provides an 
ecological explanation for the receptive 
fields that one finds in the primary 
visual cortex. 
The Sparse Coding Network that we have 
been discussing so far is in fact a 
special case of a more general class of 
networks known as Predictive Coding 
Networks. 
So, here's a schematic diagram of a 
Predictive Coding Network. 
And the main idea here is to use feedback 
connections to convey predictions of the 
input, and to use the feedforward 
connections to convey the error signal 
between the prediction and the input. 
And this box labelled Predictive 
Estimator maintains an estimate of the 
hidden clauses, the vector v of the 
input. 
Now here are some more details of the 
predictive coding network. 
So as in the case of the sparse coding 
network, there are a set of feedforward 
weights and a set of feedback weights. 
But, we also can potentially include a 
set of recurrent weights, and these would 
allow the network to model time varying 
inputs. 
So, for example, if the input is not just 
a static image but natural movies, then 
we can model the dynamics of the hidden 
causes of these natural movies by 
allowing these estimates of the hidden 
causes to change over time. 
And that's modeled using a set of 
recurrent synapses, and additionally one 
can also include a component which is a 
gain on the sensory error. 
So, this allows you to model certain 
effects such as visual attention. 
Well, this brings back some fond memories 
for me, because I worked on these 
predictive coding networks as a graduate 
student and as a postdoc. 
If you're interested in more details of 
these predictive coding models, I would 
encourage you to visit the supplementary 
materials on the course website where 
you'll find some papers that I wrote as a 
graduate student and as a postdoc. 
And finally, the predictive coding model 
suggests an answer to a longstanding 
puzzle about the anatomy of the visual 
cortex. 
Here is a diagram by Gilbert and Li of 
the connections between different areas 
of the visual cortex. 
And the puzzle is this, every time you 
see a feedforward connection, such as the 
one from v1 to v2 given by the blue 
arrow, you almost always also find a 
feedback connection from the second area 
to the first. 
So in this case, from v2 back to v1. 
So, why is there always a feedback 
connection for every feedforward 
connection between two cortical areas, 
and here is this schematic depiction of 
this puzzle. 
Information from the retina, as you know, 
is passed on to the LGN or lateral 
geniculate nucleus and then the 
information is passed on to cortical area 
V1, cortical area V2 and so on. 
But for every set of feedforward 
connections, there seems to be a 
corresponding set of feedback 
connections. 
So, what could be the role of these 
feedforward and feedback connections. 
The predictive coding model suggests 
interesting functional roles for the 
feedforward and feedback connections. 
According to the predictive coding model, 
the feedback connections convey 
predictions of the activities in the 
lower cortical areas from a higher 
cortical area. 
And the feedforward connections between 
one cortical area to the next convey the 
error signal between the predictions and 
the actual activities. 
It turns out that it can explain certain 
interesting phenomena that people have 
observed in the visual cortex, known as 
contextual effects or surround 
suppression or surround effects. 
And these effects can be explained in an 
interesting manner by the hierarchical 
predictive coding model that we have 
here, when it is trained on natural 
images. 
So, I'd encourage you to go to the 
supplementary materials on the course 
website, if you're interested in more 
details. 
Okay, amigos and amigas, that wraps up 
this lecture. 
Next week, we'll learn how neurons can 
act as classifiers, and how the brain can 
learn from rewards using reinforcement 
learning. 
Until then, Adios and Goodbye. 

Welcome back. 
In the previous lecture, we learned about 
super wise learning, but humans and 
animals in general do not get exact 
supervisory signals when they're learning 
to find food in a maze or leaning to ride 
a bike or play the piano. 
We learn by trial and error and we might 
get rewards by punishments along the way. 
For example we might find food at the end 
of the maze. 
Or get praise and critisisms from the 
piano teacher or you might even get an 
amazing reward and the end of a course, a 
certificate of accomplishment. 
This leads us to the last type of 
learning that we'll consider in this 
course, reinforcement learning. 
In reinforcement learning we have an 
agent such as a rat interacting with an 
environment, such as this barn. 
The agent at any point in time t can be 
in a state denote by ut, where ut is a 
vector that could denote, for example, 
the location of the rat in the barn. 
And the agent may get a reward at any 
point in time t and this reward is 
denoted by rt. 
And rt can be a scalar value that could 
be. 
Positive or negative and the reward might 
denote for example the amount of food 
that a rat gets in a particular location 
in a barn or it could represent a 
particular nasty encounter with a cat in 
some location in the barn. 
Now the problem facing the agent or the 
rat in this case. 
Is selecting the best actions that will 
maximize the total expected future reward 
and this is the problem reinforcement 
learning. 
Perhaps the earliest results in 
reinforcemont learning were obtaied by 
Pavlov in this experiment with dogs. 
These are the classical conditioning or 
Pavlovian conditioning experiments. 
So what did Pavlov do? 
Well he rang a bell and followed the 
ringing of the bell with some food reward 
for the dog. 
And he repeated this association of bell 
followed by food reward many many times 
here's what he observed. 
He observed that every time he rang the 
bell the dog began to salivate as 
depicted by this animation here. 
So what do you conclude from this? 
You can conclude that the conditioned 
stimulus, which in this case is the bell, 
predicts the future reward, which is the 
food. 
So the problem faced by Pavlov's dog's 
brain is this. 
How do we predict rewards that are 
delivered some time after a stimulus such 
as the bell is presented. 
Well, let's see if we can formalize this 
particular problem. 
So what we are given then are many, many 
trials, each of length let's say capital 
T time steps. 
And let's denote the time within one 
particular trial using this little t. 
And let's denote the stimulus that we 
might get at any particular time step as 
ut, so for example ut might be the 
ringing of a bell or not. 
And the reward, rt is what the animal 
might get at each time step t. 
So this could mean that the animal gets a 
food reward at some particular time step 
t or maybe it doesn't get any reward at 
all so rt can be 0 for some time steps t. 
And here's what we would like. 
So we would like a neuron whose output, 
vt, predicts the expected total future 
reward. 
And so what we would like is for the 
output vt to be approximately equal to 
the average over all the trials of the 
summation of all the rewards from 
timestep t onwards until the end of the 
trial denoted by capital T. 
Here's how you can get a neuron to 
predict the expected total future reward. 
We can use a set of synaptic rates, wt 
and we can predict based on all past 
stimuli, ut. 
So here is a network that can perform 
this operation. 
We have to use what is called a tapped 
delay line to feed in all the past inputs 
into this network. 
And here is the output of the network. 
It's simply a weighted summation of all 
of the weights with the past inputs. 
And you'll notice that this is nothing 
but the equation for a discrete linear 
filter, so linear filtering strikes 
again. 
And here are out standard trick for 
learning the weights we can minimize an 
error function. 
And here's the error function it's just 
the squared difference between the total 
future reward and the prediction of the 
total future reward. 
So how do we minimize the error function 
can we for example use gradient descent 
and the delta rule as in the previous 
lecture? 
Well yes but we have a problem. 
We don't really have the future rewards. 
So if you look carefully at this sum 
you'll notice that we have the reward at 
time step t, which is rt, but then we 
don't have rt plus 1, rt plus 2. 
r t plus 3 and so on. 
So how do we solve this problem? 
How do we minimize this function which 
contains some quantities that we don't 
yet have? 
Any ideas? 
Well the key idea goes back to Richard 
Bellman and his optimization method known 
as dynamic programming. 
And the idea is to rewrite the error 
function recursively to get rid of the 
future terms that are not available at 
this time. 
So how does this apply to our problem? 
Well here's the problematic summation of 
future rewards. 
And we can rewrite that as rt. 
Plus the sum of all the future rewards. 
And here is the key jump. 
We replace the summation of all the 
future rewards with the prediction by our 
network of the expected future reward. 
Now we have an error function where we 
have all the quantities that are 
available to us. 
Between time step t and t plus 1 and so 
if you can minimize this error function 
now using our old friend gradient decent. 
When we do that we get what's called the 
temporal difference rule or TD learning 
rule and this was orginally proposed 
Sepheran Barto in the 1980's. 
And here's what the learning rule looks 
like. 
So the rates for each time step, tau, are 
updated according to these three terms. 
So there's the learning rate as before 
and then there's a prediction error term 
given by delta and there's also the 
input. 
Now why is this learning rule called 
temporal difference learning? 
Well, as you can see in this term, we 
have a temporal difference between the 
prediction at t + 1 and the predication 
at time t. 
Well if skeptical about the temperal 
difference learning rule. 
I wouldn't blame you. 
If it's unclear that replacing the sum of 
future rewards would a prediction can 
actually work in practice. 
Well hopefully this example will convince 
you. 
Suppose we take the example Pavlov's dog. 
And suppose that the bell, the stimulus 
is given at time step 100 and the reward, 
the food is given at time step 200 within 
any given trial. 
Now let's look at the situation before 
and after learning. 
So in the case of the stimulus and the 
reward there is no difference before and 
after learning. 
Because the stimulus and the reward are 
presented at time step 100 and around 
time step 200 in both of these cases. 
But look at what happens to the 
prediction. 
The prediction of the network is all 
zeroes initially but after learning. 
There is a prediction of two starting at 
the time of the stimulus at time step 
100. 
And why is it two? 
Well two is the total reward that is 
delivered around that time step 200. 
And so you can see that the network has 
learned to correctly predict the total 
reward it expects starting from the time 
of the stimulus. 
It's also interesting to note what 
happens to the delta, the prediction 
error. 
So you can see before learning, the delta 
is high around the time of the reward and 
so that's because the network is 
predicting all zeroes, whereas the reward 
is delivered at time step 200. 
And so the prediction error is now going 
to be essentially just the reward but 
look at what happens after learning. 
So after learning around the time of the 
reward, there is a delta of 0. 
So there is no error in prediction but 
the prediction error has shifted now to 
the time of the stimulus. 
And that's because what this reflects is 
just the value of the prediction. 
So v of 100 minus the value of the 
prediction at the previous time step 
which is v of 99 so this reflects the 
prediction error given by v 100 minus v 
99. 
Now this plot shows how delta changes as 
a function of trials so at the very of 
trial number 1 we have a bump. 
Around the time of the reward around 200 
and that's identical to this plot here. 
But as the network is exposed to several 
trials, this bump moves backwards in time 
until it reaches a value of 2 at the time 
of the stimulus. 
And that is exactly the situation we have 
here. 
So that is where the network now has 
learned to predict a value of 2 for the 
total reward expects from the time of the 
stimulus from time step 100. 
Now here are some intriguing results from 
Wolfun Scholtz and colleagues. 
They recorded from the ventral tegmental 
area of the midbrain of a monkey. 
And the neurons in the ventral tegmental 
area or VTA are dopaminergic, which means 
that they transmit the neurotransmitter 
dopamine to different parts of the brain. 
Now dopamine has been implicated in 
reward based learning and it is also 
involved in various addictive behaviors 
such as addiction to drugs like cocains. 
In the experiments, the monkey was 
presented with a stimulus, for example a 
sound and then the monkey had to press a 
key. 
A short while later the monkey was 
rewarded and here's what the neurons in 
the ventral tegmental area did in this 
experimental paradigm. 
Before training, the neurons in the 
ventral tegmental area had a very high 
firing rate around the time of the 
reward. 
But after training, the neurons no longer 
responded near the time of the reward. 
They started responding around the time 
of the stimulus. 
Now what does this remind you of? 
That's right these two plots look very 
similar to the plots for delta, the 
prediction error from the previous slide. 
So what this suggests is that the neurons 
in the Ventral Tegmental area may be 
encoding reward prediction error. 
And that would explain why you have a big 
response before training around the time 
of the war. 
Where as after training the response is 
very small because the reward prediction 
error now is very small since the animal 
has learned to predict the reward. 
And then the reward prediction error is 
larger around the time of the simulis 
because now that response encodes the 
prediction error vt minus vt minus 1. 
Which is similar to the error that we saw 
in the previous slide, v100 minus v99. 
Now here's an interesting question. 
What do you think will happen if we don't 
give the monkey any reward at the time 
that it expects to get the reward? 
Well the monkey is probably going to 
think it's a cruel joke but what do you 
think is going to to happen to the 
responses of neurons in the ventral 
tegmental area. 
Well that's right you would expect to see 
a negative error because the prediction 
was not fulfilled and that's indeed what 
will Schouls and colleagues observed. 
In the ventral tegmental area neurons. 
So when there was reward, you have this 
response, which is similar to what we had 
in the previous light. 
But when the reward is omitted, there's 
no reward, then you see a dip in the 
firing rate of the neurons. 
And that corresponds to a negative error 
in the temporal difference learning model 
of the dopaminergic cells NVTA. 
Now that you know how the brain might 
learn to predict rewards you might be 
asking the question how does the brain 
learn to select actions that maximize 
future rewards. 
This will be the topic of our next 
lecture. 
Until then. 
Sy Chin and good bye. 

Welcome back dear adventurers. 
In the previous lecture we learned how 
the brain can predict future rewards. 
Now it's time for action. 
How does the brain use reward information 
to select actions in any given 
circumstance. 
Lets find out. 
Well lets try to formulize this problem. 
Here's the diagram for the reinforcement 
learning frame. 
And as you would recall, we had an agent 
interacting with an environment and at 
any point in time T, the agent measures a 
state UT of the environment and also 
potentially gets a reward, RT, from the 
environment. 
And, the agent can then execute an 
action. 
80 which in turn would change the 
environment in a specific way and then 
agent gets to measure again a new state 
ut plus 1 and potentially get a reward rt 
plus 1. 
Now the phased by the agent is figuring 
out which action to take given any 
particular state. 
And this can be formalized as the problem 
of learning a state-to-action mapping, or 
a policy, as it is called in 
reinforcement learning circles. 
And the policy is typically denoted by 
the function pi. 
So pi is a function that maps states to 
actions. 
Now, what kind of a policy do we want? 
We would ideally like to have a policy 
which maximizes the expected total future 
reward. 
So here's the mathematical expression for 
that, and it's, you'll notice, exactly 
the same expression we had when we were 
discussing temporal difference learning. 
So it's the expectation of the sum of all 
the rewards we're going to get from time 
step T, all the way up to the end of the 
trial times state capital T. 
Let's try to understand the problem in 
the context of an example. 
Here's our friendly rat and let's suppose 
that the rat is in a barn. 
Given by this maze here, and different 
locations in the barn, contain different 
amounts of food reward. 
So let's suppose that this location here 
contains a food reward of five. 
This location contains a food reward of 
two and these two locations contain no 
food reward at all. 
Now, we could be mean to our rat and make 
these two numbers negative, which would 
mean that these two locations contain 
predators, such as cats, but let's be 
nice to our rat, so no predators in this 
barn. 
Now we have marked three locations in 
this barn A, B, and V. 
And the rat has to decide whether to go 
to the left or to the right at each of 
these locations. 
So, the reinforcement learning problem 
for the rat, then, is to choose an action 
at each of these locations. 
So as to maximize the expected future 
reward. 
Now in the context of the formal 
reinforcement learning framework, can you 
tell me what the states and actions are 
in this problem? 
That's right, the states are the 
locations and the actions Are the two 
actions, go left or go right. 
Here's another question that i would like 
to ask you. 
If the rat chooses to go left or right 
uniformly at random. 
In other words it's executing a random 
policy, then can you tell me what 
expected reward would be for each state? 
The expected reward for each state is 
also called the value for that state. 
So can you tell me what the value for 
each of the states A B and C would be if 
the rat was executing a random policy of 
going uniformly at random left or right 
at each of these locations. 
Well here's the answer. 
Let's first look at the value for B, so 
if you're in location B, then if you 
uniformly go at random left or right, 
then you're going to be half the time in 
the location with the 0 and half the time 
in the location with the 5, and therefore 
the expected reward is 2.5. 
Similarly if you're in location C, then 
you have an expected reward or value of 
1. 
And interestingly, if you are in location 
A you an use the values you just computed 
for B and C, because those are the next 
states following A if you go left or if 
you go right. 
And so if you go left or right uniformly 
at random then you have the expression 
half of the value of B plus half of the 
value of C. 
Adding them together gives you the 
expected reward or the value for the 
location A. 
Well, that was easy, but how can you 
learn these values in an online manner? 
Remember that the rat Is experiencing 
these locations sequentially and 
therefore, the rat has to learn these 
values, from experience, as it is going 
through each of these locations. 
How can we do that? 
The answer, as you might have guessed, is 
to use our old friend, Temporal 
Difference Learning or TD Learning. 
So let's represent the value of each 
state u, v of u by a weight w of u. 
Now we can update w of u using the 
temporal difference learning rule. 
So epsilon as you recall is the learning 
rate. 
And here is the prediction error from the 
temporal difference learning rule. 
And you can see that the prediction error 
term contains both the reward that you 
get at the state u as well as the 
prediction of the expected reward, the 
value for the state u prime. 
So u prime here is the next state that 
you get after taking an action at the 
location u. 
And vu, of course, is the prediction of 
expected reward, it's the value for the 
state, u. 
Now let's look at what this u prime is. 
So, if you are in a state, u, and you 
take an action, a, then, you're going to 
end up in a state u prime. 
So, specifically, in our example here. 
If we have a state A that we're in, and 
if we take an action go left, then the u 
prime, the next state, is going to be the 
state B. 
Here are the results of using TD Learning 
to learn values for our problem of the 
rat in the barn using a random policy. 
Each of these plots shows the value for 
the states A, B and C, as represented by 
the weights wA, wB and wC. 
The jagged lines show the values as a 
function of trial number. 
And you can see that the values for each 
of these locations A, B, and C jumps 
around a bit, but the running average, as 
represented by the dark line, converges 
to the right answer. 
So, 1.75, 2.5 and one, were the values 
that we calculated for A, B, and C on the 
previous slide. 
So, indeed The temporal difference 
learning rule appears to be learning the 
correct values for each of these 
locations. 
Now why are these values jumping around 
so much well it's because we've set the 
learning rate epsilon to a high value of 
0.5 and that speeds up the learning the 
process but it will also cause your 
estimates of your value to jump around a 
bit. 
Now why did we go through the trouble of 
finding the value of each of the states? 
Well, here is the answer, as observed by 
our astute friend, the friendly rat. 
Once you know the values for the states, 
you solve the action selection problem. 
Here's why. 
If you're given the choice between two 
different actions that lead to two 
different states, then all you have to do 
is pick the action that leads you to the 
higher valued state in the next time 
step. 
Let's see if this works in our example. 
Well, as you might have guessed, it does. 
Let's consider the action that we should 
take in the location a so we have 2 
possible action to go left or go right 
and all we have to do now is to look at 
the values associated with the next 
states if we each of these respective 
actions. 
So if we take a left we end up in the 
state B and we can look the value which 
is the expected reward we get. 
In this state B which as we computed in 
the previous slide is 2.5, now similarly 
if we take the action right then we end 
up in the state C which has as we 
computed earlier a value of one, so 
that's the expected reward that we might 
get if we move to the state C. 
So given that we have these two possible 
states that it would move to, if we take 
the action left or the action right, the 
obvious choice here then, is to choose 
the action going left, and that will make 
us go to the state b, which has the 
higher expected reward or value. 
The important point here is that we're 
using values as surrogate immediate 
rewards. 
So what do we mean by that? 
Well consider the fact that in locations 
B and C, we do not get any immediate 
rewards but we can compute the value 
which is the expected reward at B and at 
C, and we can use the value as a 
surrogate. 
For the immediate reward, and so we can 
use the value to guide our selection of 
action at location A. 
This leads us to the important result 
that a locally optimal choice here leads 
to a globally optimal policy, as long as 
we have a Markov environment. 
And by Markov we mean that the next state 
only depends on the current state and the 
current action. 
This important result, which we can 
rigorously prove, is closely related to 
the concept of dynamic programming first 
proposed by Richard Bellman. 
Okay, let's put it all together and see 
if we can come up with an algorithm For 
learning optimal policies in Markov 
environments. 
And the algorithm that we're going to 
look at is called actor-critic learning. 
And it's called actor-critic learning 
because there are two components. 
The first one is the actor, and the actor 
component selects actions and maintains 
the policy. 
The critic component maintains the value 
of each state. 
Lets first look at the critic component 
and see how it learns. 
The critic component can also be looked 
upon as performing policy evaluation 
because it's evaluating the current 
policy by finding the value of each state 
under the current policy. 
Now do we find the value of a state u 
well we can first represent it using a 
weight w of u as we did before and then 
we can apply the temporal difference 
learning rule as we did earlier in this 
lecture to find the value of a state u. 
So here is the prediction error term as 
before. 
And so the prediction error term is then 
used to update the weights wu for each 
state u and that allows the critic 
component to compute the value of each 
state u. 
Lets now look at the actor component. 
The actor component as you recall selects 
actions and maintains the policy. 
So does it select an action. 
It selects probabilistically by using 
this function also known as the soft max 
function. 
The soft max function is basically an 
exponential of a Q function which is 
basically the value of a state and action 
pair. 
So we'll come to that in just a minute 
but think of as a function very similar 
to a value of a state except that now 
we're computing the value of a state 
action pair. 
So the soft max function looks at the 
value of any given state action pair and 
it runs it through this exponential. 
Beta here is some fixed parameter so when 
you divide it by the sum of all the 
exponential your normalizing it so that 
this probability the action for any given 
state sums to 1. 
So given this set of probabilities for 
any given action in any given state we 
can now select the action according to 
this probability, and that gives us, the 
action that we execute. 
You might be wondering why we have to use 
a probabilistic method for selecting 
actions. 
Well it let's us address what is known as 
the exploration, versus the exploitation 
dilemma in reinforcement learning, and 
what is that? 
Well, consider the fact that early on at 
the very beginning of learning these Q 
values might not be very accurate because 
you have not experienced the environment 
very much. 
So what you would like to do at that 
stage is explore the environment, and so 
having such a soft maximum selection 
method lets you explore the environment 
if the beta values are small. 
And, as you've learned, better and better 
of values of Q, through exploration you 
can then increase the value of beta, and 
that will tend to then pick the actions 
that have the higher Q values. 
So then, you get more and more towards a 
deterministic action selection process 
that favors more and more, the actions 
that have the higher Q values. 
Once you've selected the action area of 
the state u, you can use this learning 
rule to update the q values for all your 
actions. 
A prime. 
This learning rule is quite similar to 
the learning rule for w, and it also uses 
the reward prediction error, except that 
now we multiply the reward prediction 
error with this term here which uses the 
direct delta function that we all know 
and love. 
And as you recall the direct delta 
function is going to be equal to a value 
of 1 when a prime is equal to a, and a 
value of zero when a prime is not equal 
to a. 
So the effect of this term is to multiple 
the reward prediction error with a 
positive number when A' is equal is A and 
a negative number when A' is not equal to 
A so the overall effect then is that the 
Q value for A is increased if the action 
A leads to a. 
Greater than expected reward and then the 
Q value is decreased if the action leads 
to a less than expected reward. 
Now the Actor-Critic Learning Algorithm 
proceeds by repeating the steps one and 
two above. 
And under reasonable assumptions, it can 
be shown to converge to the optimal 
policy, Now let's see if it finds the 
optimal policy in the example of our 
friendly rat in the barn. 
Yes it indeed finds the optimal policy as 
it turns out for our barn example. 
Here is the probability of going left. 
At each location in the bar and as you 
can the probability of going left is 
initially 0.5 for each of the locations A 
B C as is the probability of going right 
but after several trials the algorithms 
assigns a probability almost close to 1. 
For going left at location A. 
So that means that the action that's 
favored is going left at location A, and 
now at location B the algorithm assigns a 
probably that's quite close to zero if 
not equal to zero and that means that 
going right is highly favored and so that 
is the preferred action at location B. 
And now if you look at the location C, 
you can see that there is a gradual 
convergence towards assigning a high 
property or a property close to one for 
going left at C. 
Now can you tell me why is takes the 
algorithm a longer time to find that the 
best action at location C is left. 
Well that's right, it's because of the 
exploration exploitation trade off that 
we observed in the previous slide. 
And so the algorithm tends to go left 
more often at location A, and so it very 
rarely tends to go to the right. 
And so it does not get to experience the 
state or the location C. 
Very often, which is why it takes a 
longer time to learn, the value 
associated with C, and therefore it takes 
a longer time to realize that the best 
action to take, at location C, is in 
fact, going left. 
Now researchers such as Andrew Barto have 
suggested, a mapping between the 
components of the actor critic model. 
And the components of an important 
structure in the brain known as the basal 
ganglia. 
The dashed box on the left represents the 
components of the basal ganglia and the 
dashed box on the right represents the 
components of the actor-critic model. 
Note that we're using a hidden layer in 
this case to implement a multi-layered 
network for mapping. 
State estimates to values, and state 
estimates to actions. 
We can see that there's a rough 
similarity between the components on the 
left side and the right side. 
For example, the DA here on the left side 
stands for the dopamine signal. 
And we already saw in the previous 
lecture that these dopamine signals Are 
very similar to the temporal difference 
prediction area signal that we see in the 
temporal difference learning model. 
Now, this is ultimately a very abstract 
and high level model of basal ganglia 
function, but perhaps, it could serve as 
a starting point for more detailed 
models. 
For more details on these actor critic 
type models of the Basal Ganglia please 
see the supplementary materials on the 
course work site. 
I would like to end the lecture by noting 
that reinforcement learning has been 
applied to many real world problems, and 
as a grand finale for this lecture. 
I would like to show a video of 
autonomous helicopter flight based on 
some work by Andrew Eng, Peter Abel, and 
others at Stanford University. 
In this case, the reinforcement learning 
algorithm learned a policy based on a 
dynamics model for the helicopter and a 
reward function learned from human 
demonstrations. 
If you're interested in more details, I 
would encourage you to go to the website 
shown at the bottom of the slide. 
Okay, so are you ready to see what 
reinforcement learning can do? 
Here we go. 
[NOISE] Wow, that was an exciting way to 
end the lecture. 
That in fact ends the last lecture of 
this course. 
Thank you all again for joining us on 
this first online journey through the 
land of Computational Neuroscience. 
Until we meet again, happy adventures and 
goodbye. 

[MUSIC]. 
It is a great pleasure to introduce my 
colleague and friend, Dr Eb Fetz, who's a 
professor of Physiology and Biophysics 
here at the University of Washington. 
Eb got his Ph.D in Physics from MIT in 
1967. 
He's widely regarded as one of the 
leading researchers in the area of modern 
neuroscience. 
Eb is also a founding father of the field 
of brain computer interfacing. 
In 1969, he showed that monkeys can 
control the activity of single brain 
cells to move the needle of a meter to 
get food rewards. 
More recently he has proposed the concept 
of bidirectional brain-computer 
interfaces, which can both record from 
and stimulate neurons at the same time. 
To tell us more about this exciting new 
area of research, here's Eb. 
 >> Okay. 
Thank you very much. 
so I'm going to tell you about 
Bidirectional brain-computer interfaces. 
This is a new experimental paradigm 
that's illustrated schematically here. 
And basically it consists of a computer 
that's connected to electrodes that 
record activity from the brain. 
And process it in real time, and deliver 
activity dependent stimulation back to 
the brain. 
So this in a sense creates an artificial 
recurrent connection that operates 
continuously during free behavior. 
And the brain can learn to incorporate it 
into its normal activity. 
It also creates conditions that promote 
synaptic plasticity. 
These are themes that you've heard about 
in this course, and we'll take a look at 
some of these issues as we've explored 
them recently in the lab. 
So everyone is familiar with the standard 
brain computer interface, in which neural 
activity is record from let's say cells 
in the brain. 
And processed to control an external 
device such as a robotic arm. 
And the best cells for this sort of 
function are in the motor cortex, 
actually. 
Where neurons are used to controlling the 
activity of peripheral muscles through 
their connections to the spinal cord. 
The ideal recording is from neurons, but, 
as you've heard. 
They're also possibilities of recording 
not only in neural activity from a single 
neurons with invasive intracortical 
electrodes. 
But also, less invasively signals from 
the surface of the brain, the 
electrocorticogram. 
Or a surface of the scalp, even the 
electroencephalogram, EEG. 
And these signals are processed then 
through electronic circuitry to generate 
control signals for the external device. 
It could be cursor on a screen, or it 
could be the movement of a prosthetic 
arm. 
And the subject sees this output and uses 
this visual feedback to optimize the 
control of these signals by modifying 
illustional controls of neural activity. 
So that's a standard brain computer 
interface. 
the, bidirectional, computer interface 
converts this activity to control a 
stimulator. 
Which can electrically stimulate the 
periphal muscles or similarly could be 
delivered in spinal cord or they could 
actually be delivered anywhere in the 
brain. 
So this creates a recurrent loop that can 
as I said operate continuously. 
And this component of this, of the loop 
can be implemented in electronic 
circuitry that can be made small enough 
to be portable. 
And carry around and operate 
continuously. 
And we've done this in the form of a 
device we call a neurochip, so this 
neurochip is shown here. 
Basically it can consists of a printed 
circuit board that is connected to 
electrodes in the brain. 
Printed circuit board is populated with 
off the shelf components including an 
amplifier a computer chip and a 
stimulator. 
So the computer chip can be programmed to 
do a number of operations on the recorded 
activity. 
For example, to identify the occurrence 
of action potentials of a single cell at 
the recording electrode. 
And then deliver, convert those action 
potentials to deliver stimuli back into 
the brain. 
So what's all this good for? 
So the bidirectional brain computer 
interfaces basically have two general 
types of applications. 
one is to, they said create these 
artificial recurrent connections. 
And the fact that the brain can adapt to 
consistent sensorimotor conditions means 
that it could actually learn to 
incorporate this bidirectional Brain 
Computer Interface into normal behavior. 
There are obvious clinical applications 
to using this artificial recurrent 
connection to bridge lost biological 
connections. 
So the second general type of application 
is to create synaptic plasticity. 
And this occurs because spike-triggered 
stimulation produces a synchrony that's 
required to strengthen synaptic 
connections. 
Again, a clinical application would be to 
strengthen weakened connections as occur 
in a stroke. 
So, I'm going to show you two examples 
from our lab one of each of these 
applications. 
So first to look at the possibility of 
creating a recurrent connection that 
bridges a lost biological connection. 
This experiment with Chuck Moritz and 
Steve Perlmutter showed that monkeys 
could learn to use this bidirectional 
computer interface. 
To, control electrical stimulation of 
muscles triggered on neural activity in 
the brain. 
So the experiment went as follows, first, 
the monkey was controlling a cursor, and 
driving it into a target by normal, 
forces generated around the wrist. 
The hand was held in an isometric torque 
transducer, and the monkey generated 
these torques, that drove the cursor into 
the target. 
So then, the peripheral nerve was 
blocked, and this paralyzed the muscles 
temporarily. 
And then the money was rewarded, and then 
the cell activity was connected to a 
controlled cursor. 
And the monkey learned in within 10, 20 
minutes to drive the cursor into the 
target with neural activity. 
And the final step was to then connect 
the activity of the cell to electrical 
stimulation of the muscle. 
And this generated electrically evoked 
twitches that now generated forces, and 
it was the forces that drove the cursor 
into the target. 
So, the monkey was essentially 
controlling the cursor position with, 
torques that were generated by electrical 
stimulation that was muscle. 
That was controlled by neural activity. 
So, the, an example of, an operation of 
this type is shown in this, slide. 
So, in this situation, there were 
actually two cells, one drove stimulation 
to flexor muscles. 
The second row stimulation to extensor 
muscles. 
And the top trace here shows the forces 
generated through this stimulation of 
muscles. 
As the monkey tried to acquire visual 
targets that are represented by these 
rectangles. 
So here, for example, the rectangle 
queued flexing forces, the monkey 
generated the force by increasing the 
activity of cell one. 
And when cell one's activity exceeded the 
threshold, the flexor muscles were 
stimulated in, with intensities 
proportional to the difference between 
firing rate and threshold. 
And so this produce greater extension 
force. 
And then in order to get into the 
extension target the monkey activated the 
second cell. 
So he could generate alternate flexion 
extension torque's by altnernately 
activating these two cells. 
Another scenario is one in which the 
monkey actually controlled bidirectional 
movements with only one cell. 
But, control this by increases and 
decreases of the cell activity. 
Now here we've got again the same, sort 
of torque trajectories generated, when 
the extension and inflection targets are 
presented. 
So in this case the increase in activity 
above a certain threshold stimulated 
flexor muscles. 
And in order to stimulate the extensors 
this monkey had to decrease the activity 
below a certain level. 
So one can actually use the ability of 
the animal to increase and decrease 
neural activity to generate alternating 
forces. 
Now one of the interesting findings in 
the study it didn't actually turn out to 
be necessary to find cells that were 
normally related to the risk. 
Turns out that any motor cortex cell can 
be volitionally controlled. 
And during this period when the monkey 
learned to control the cell they learned 
that the control of any cell, whether it 
was related to risk or not. 
So the importance of that is that It's 
not necessary to go looking for cells 
that are normally related to risk 
movements and decode their activity. 
It's possible to have the subject learn 
to control this activity. 
and the transition to controlling the 
activity of a cell to controlling muscles 
is relatively Straight forward. 
So the paradigm is illustrated here in 
relation to controlling multiple muscles. 
So [COUGH], one can imagine multiple 
cells being directly connected to 
activate multiple muscles. 
And in principle this could work in 
practice there are challenges. 
because electrical stimulation of the 
peripheral musculature is recruiting 
motor neurons in an artificial way. 
And it's going to get complicated to be 
able to control multiple muscles in a 
synergistic way. 
So, an alternate place to stimulate is 
actually in the spinal cord, as shown 
here. 
Where neural activity can deliver stimuli 
in a spinal cord and those spinal 
stimulation activate circuitry. 
That number one ,recruits the motor units 
in a more natural way. 
And number two often activates muscles in 
synergistic ways so this is probably a 
more practical cellular stimulation. 
Now, the second example I want to show 
you is this plasticity that can be 
produced by spike-triggered stimulation. 
[COUGH]. 
In this example, used the activity of 
cells in the primate motor cortex that 
have direct connections to muscles. 
So, in certain so called cortical motor 
neuron cells, go from the cortex to the 
motor neurons. 
And these synaptic connections then 
activate the motor neurons or facilitate 
the activity of motor neurons. 
Which then produce activity in the 
peripheral muscles. 
And these cells can be identified by so 
called spike-triggered averaging of the 
EMG activity. 
An example from early experiments shown 
here, monkey is doing alternate extension 
inflection movements against the load. 
Here we have six extensory muscles that 
are coactivated with the extension. 
A cell is coactivated with these muscles 
and using the action potential of these 
cells to compute spike triggered averages 
of the EM, rectified EMG activity. 
Shows the two of these muscles showed 
this post spike facilitation. 
Post spike increase that is a signature 
of this mono synaptic connection to motor 
neurons of those two muscles. 
So this is a measure of the strength of 
the connection here. 
So here again is the circuit showing. 
The cortical cell monosynaptic connected 
to motor neuron and that produces this 
post spike facilitation. 
There are also inhibitory circuits that 
can be revealed by post spike suppression 
and are mediated by a disynapTic link 
through inhibitory inner neurons. 
So, this is a way to show what muscles 
the, a given single cortical cell 
facilities and suppresses. 
And over here on the left is a summary 
diagram of the types of cells that were 
found in these earlier studies. 
The main point here is that many of these 
cells had divergent connections to 
multiple target muscles. 
So the conditioning of this connection 
was performed in a study by Yukiyo 
Nishimura and Steve Promotor/g. 
Where the [COUGH] activity of the 
cortical cell was recorded, not any 
cortical cell but cortical monoclonal 
cell. 
So these where neurons that were 
demonstrated to have post spike effect in 
target muscles. 
And the neurochip was used to trigger 
stimulation in the spinal cord of the 
target motor neuron at various delays. 
So we implemented, in other words, 
parallel circuit through the neurochip 
that operated in parallel with the 
descending action potentials of the 
recorded cell. 
And, this, produced synchrony at the 
target site that, could change these 
connections. 
Now, here's, the, demonstration of, the 
change in these connections. 
It was measured by the magnitude of this 
post spike facilitation. 
So here's the spike triggered average 
showing post spike facilitation. 
Spike-triggered average was compiled on 
day zero when the monkey was generating 
standard movements through isometric 
forces. 
Target tracking task and after the post 
spike effect was documented the neurochip 
was connected and operated for about 22 
hours. 
Producing spike-triggered stimulation at 
the the spinal site that where this cell 
terminated. 
And then the second day, the post spike 
effects were measured again with under 
the same conditions. 
And this, post spike effect increased. 
The post spike effect magnitude is 
measured by the so called mean percent 
increase over baseline, which was 6 
before and about 10 after. 
And, this definitely, is, related to the 
magnitude of the, strength of the cortico 
mono [UNKNOWN] connection. 
This sort of experiment was performed, in 
a number of cell muscle pairs, using, 
different delays between the spike and 
the spinal stimulus. 
And this curve plots the change in this 
mean percent increase as a function of 
the delay between the spike and the 
stimulus. 
And makes the point that increases in the 
mean percent increase were obtained in a 
specific temporal window. 
That is to say, for delays between ten 
and about 25 milliseconds recorded here. 
So there was no change for longer 
intervals. 
Which is interestingly is a control for 
the effect of stimulation alone. 
It required spike-triggered stimulation 
in this interval. 
And this is consistent with the temporal 
window for spike timing dependant 
plasticity. 
Even more interesting, I think, is this 
point here, which [COUGH], represents a 
decrease in the mean percent increase of 
this post spike effect. 
And this was obtained when there was zero 
delay between the spike and the stimulus. 
What this means is that the spinal motor 
neurons were electrically activated prior 
to the arrival of the descending cortical 
spinal action potential. 
And that produced a decrease in the 
strength of the connection which is also 
consistent now with this bidirectional 
spike timing dependent plus this 
integral. 
So these two examples illustrate how one 
can use the bidirectional Brain Computer 
Interface to implement artificial 
connections. 
And also create plasticity. 
And in conclusion there are lots of 
applications for this this paradigm. 
Depending on the source of the signals 
the transform of signals and where the 
stimulation is delivered. 
So The sources include neurons either 
single cortical neurons, multi unit 
activity. 
Or it could be more easily recorded field 
potentials could be less invasively 
recorded electrocardiogram or EMG. 
And the side of recording can be pretty 
much anywhere in the brain. 
So this activity can be transformed by 
delivering stimuli that were directly 
proportional to this activity. 
or through computed function of this 
activity one can even imagine all 
networks being used to convert this 
activity to stimulation. 
And finally the stimulation can be 
delivered at multiple targets, muscles 
we've seen, the spinal cord or cortex. 
Or also another interesting target is 
subcortical reward site, in other words 
inner cranial reinforcement sites where 
the activity that triggers that 
simulation gets rewarded and it basically 
implements a operand conditioning 
paradigm that operates during free 
behavior. 
So these are just examples of some of 
these multiple applications that are 
going to provide exciting insights into 
basic neuroscience mechanisms. 
As well as promising clinical 
applications. 
Thank you. 

[MUSIC]. 
Hello, and a warm welcome to every one of 
you from around the world. 
This is Week 1 of Computation 
Neuroscience and I'm your instructor, 
Rajesh Rao. 
Let's begin our computation adventures 
with a picture. 
You've probably seen a picture like this 
before. 
Physicists tell us that this is the 
universe that we live in, but I think 
they're mistaken. 
This is the universe that we really live 
in. 
This 3-pound mass of tissue inside our 
skull is what allows us to perceive the 
world and indeed the universe. 
This amazing machine is what enables us 
to think, feel, act, and be human. 
This is what is enabling me to speak 
these words right now and allowing you to 
listen. 
And when the lecture gets boring, which 
hopefully, won't happen too often. 
You can thank the same 3-pound organ for 
enabling you to skip forward a few slides 
or maybe doze off in that chair that 
you're sitting in. 
Understanding how the brain does all of 
these things is one of the most profound 
scientific mysteries of the 21st century. 
In this course, we'll try to unravel some 
of this mystery and understand the brain 
using computational models. 
In this course, we'll cover three types 
of Computational Models. 
The first kind are Descriptive Models. 
So in this case, we're interested in 
quantifying how neurons respond to 
external stimuli. 
And what we get here is something called 
a neural encoding model, which 
quantitatively describes a particular 
neuron responds to external stimuli. 
The counterpart to encoding is decoding. 
So in this case, we're interested in 
extracting information from neurons that 
have been recorded from the brain. 
And then, using this information for 
controlling something like a prosthetic 
hand for example. 
So this problem of decoding is extremely 
important in the field of brain-computer 
interfacing and neuroprosthetics. 
The second type of model that we look at 
are called Mechanistic Models. 
So in this case, we are interested in 
simulating the behavior of a single 
neuron or a network of neurons on a 
computer. 
So you might have heard about the Human 
Brain Project that is being led by Henry 
Markram in Europe. 
And, that project is an example of a 
computer simulation of an extremely large 
network of neurons in the extreme case, 
perhaps, the entire brain on a computer. 
The last type of models that we look at 
are called Interpretive or Normative 
Models. 
So in this case, we are interested in 
understanding why brain circuits operate 
in the way that they do. 
In other words, we're interested in 
extracting some computational principles 
that underlie the function of a 
particular brain circuit. 
So we'll look at examples of all these 
three types of models in the coming 
weeks. 
Now, moving on to some course information 
and logistics. 
The length of the course is eight weeks, 
and each week, we'll give you one video 
lecture, which will be about 1 hour long. 
And we'll split this up into 10 minute 
chunks and we'll also give you one 
homework quiz each week. 
Now, these lectures and homeworks will be 
released on Fridays and the homeworks 
will be due the second Monday from the 
release date. 
So this way, you'll be able to use two 
weekends to work on each homework and the 
complete syllabus and schedule is on the 
course web page. 
Here are the two recommended textbooks 
for this course. 
They are not required, but they might be 
useful if you need additional information 
besides what's covered in the lecture 
videos and lecture slides. 
The first one is Theoretical 
Neuroscience, this is a standard textbook 
in the field and its a book written by 
Peter Dayan and Larry Abbot, two leading 
researchers in computational 
neuroscience. 
The other textbook is called Tutorial on 
Neural Systems Modelling and it's by 
another researcher in the field, Thomas 
Anastasio. 
And this book also comes with the Matlab 
code that you might find useful as you're 
exploring concepts and computational 
neuroscience. 
Now, on to homeworks and gradings. 
So the course grade will be based on six 
weekly homeworks, most of these will be 
multiple choice questions. 
Some of them will be based on the result 
of programming and Matlab or Octave. 
And, we'll allow up to submissions for 
each homework and we'll take the maximum 
score out of the three that you've 
submitted. 
The good news is there's no exams and 
there will be a certificate of completion 
if your total course grade is greater 
than 60%. 
I mentioned earlier that some of the 
homework questions will be based on 
results that you get after you've 
executed your program in Matlab. 
So what if you don't have Matlab? 
Well, no worries. 
You can use Octave which is quite similar 
to Matlab, but it's free. 
And you can download Octave from the 
website given on the slide. 
And Octave should suffice for all the 
homework questions that we'll have in 
this course. 
But what if you have no programming 
experience to begin with? 
Well, no problem. 
Perhaps you can use this opportunity to 
learn programming. 
there are several Matlab tutorials on the 
course website that you can try out, and 
also, the first homework assignment is 
actually a Matlab practice homework. 
So the submission is optional, but it 
might be really useful for those of you 
who are programming for the first time in 
Matlab to try out the homework and submit 
it and see how you did. 
So let's end with some of the goals of 
the course. 
In other words, what can we expect to 
learn in this course? 
Well, at the end of the course, you 
should be able to, first of all, 
quantitatively describe what a biological 
neuron, a network of neurons is doing 
given some experimental data that perhaps 
you got from your neuroscientist friend. 
Secondly, you would like to be able to 
simulate on a computer the behavior of 
neuron or other networks of neurons. 
And finally, you should be able to at the 
end of the course, formulate competition 
of principles that would help explain the 
operation of certain neurons or networks 
in the brain. 
So, are you ready? 
Let's begin. 

 >> Welcome back. 
Let's begin by asking what is 
computational neuroscience? 
According to Terry Sejnowski, who was one 
of the founding fathers of the field and 
was also my postdoctoral advisers, the 
goal of computation neuroscience is to 
explain in computational terms how brains 
generate behaviors. 
Now, let's dissect this definition a 
little bit. 
This leads us to a definition proposed by 
Peter Dayan and Larry Abbott. 
According to them, computational 
neuroscience is the field that provides 
us with the tools and methods for doing 
three different things. 
One is characterizing what nervous 
systems do. 
The second is Determining how they 
function. 
And finally, the third is, understanding 
why they operate in particular ways. 
Now, this actually corresponds quite 
nicely to the three types of 
computational models that we looked at 
earlier in the, in a previous lecture. 
This corresponds to descriptive models, 
mechanistic models and interpretive 
models. 
Now to understand these three types of 
models in a little bit more detail with 
an example, let's look at the concept of 
receptor fields. 
In order to understand the concept of 
receptive fields it is useful to go back 
to some early experiments performed by 
Hubel and Wiesel back in the 1960s. 
Now, they were interested in trying to 
understand the visual system of the cat. 
In order to do so, they implanted tiny 
electrodes, or tiny wires. 
Into the visual area of the cat's brain. 
So this is this an area that's in the 
very rear of the cat's brain and by using 
these electrodes, they were able to 
record some electrical signals from 
particular brain cells. 
So these electrical signals that they 
record are due to the output of the brain 
cells and these outputs are in the form 
of Tiny digital pulses. 
which are also called spikes or action 
potentials. 
We'll learn more about them in a later 
lecture. 
And in order to get these cells to 
respond, they show different types of 
stimuli to the animal. 
And on the right hand side, you see one 
of these, stimuli. 
So this is a bar of light that's oriented 
at approximately 45 degrees, and I'm 
going to show you a movie that will have 
this bar of light moving in a particular 
direction. 
And what you're going to hear are the 
responses of one particular brain cell in 
a cat's brain. 
And you're going to hear the responses 
because Hubel and Wiesel have converted 
the electrical signals that they're 
recording into sound signals. 
So are you ready? 
Here we go. 
[SOUND] So the crackling sound that 
you're hearing are the responses of a 
brain cell, a visual cortical brain cell. 
[SOUND] In the cat's brain. 
And you'll notice that this particular 
brain cell likes bars of light that are 
oriented at this 45 degree angle. 
But it doesn't like broad field 
illumination, like this big square of 
light that they are showing. 
So it doesn't really respond when this 
big square of light is being turned on. 
On or off, but it does respond to the 
edge of that square when it's oriented at 
this 45 degree angle. 
Okay, so what did we see in the previous 
slide? 
We saw that when the bar of light was 
horizontal, there was not much of a 
response from the cells so this is a way 
of representing There being not much of a 
response. 
Each of these vertical lines is a spike. 
So you were hearing the crackling noise 
that responded to one little pop for each 
of these vertical lines. 
And that's the spike from the recorded 
neuron. 
And we found that in the particular movie 
that we say in the previous slide The 
cat's cell, the one that we were 
recording from, responded the most when 
the bar of light was at a 45-degree 
angle. 
So we got a very robust response. 
So here's where the light, the bar of 
light, was in a particular location at 
that particular orientation. 
And so we got a very robust response as 
shown by. 
These vertical lines which correspond to 
the output of neuron, also called a 
spike. 
And similarly when the bar of light was 
at a different angle, you would not 
expect much of a response. 
That the response is lesser than it is 
for the 45 degree angle. 
So what this leads us to is a notion 
called A receptive field. 
So here is the definition. 
So the receptive field is defined by 
neuroscientists as comprising of all 
these specific properties of a sensory 
stimulus that generates a very robust or 
a strong response from any cell that 
you're recording from. 
So examples could be that, for example, 
you're, recording from a cell. 
In the retina, and you might find that 
the cell responds really robustly to 
spots of light that are turned on at a 
particular location. 
Similarly, as we saw in the Hubel and 
Wiesel experiments, a bar of light that 
is at a particular orientation and at a 
particular location on the retina. 
Might cause a robust response in a visual 
cortex cell in the cat's brain. 
So what we'll do now is we'll look at the 
three different types of computational 
models that we mentioned earlier, 
descriptive, mechanistic and interpretive 
models. 
And we're going to build these models for 
The concept of Receptive Fields. 
So first, let's look at Descriptive 
Models. 
So how do you build a Descriptive Model 
of a Receptive Field? 
Well let's take the case of the Retina. 
So the Retina is the layer of tissue 
that's at the back of your eyes. 
And when you for example are looking at a 
particular object, let's say this pencil, 
the inverted image is projected to the 
back of your eyes and on the retina. 
And if you're recording from a particular 
group of cells called the retinal 
ganglion cells, you will find that it is 
conveying information about the image. 
To the other areas of the brain. 
Particularly, this area called the 
lateral geniculate nucleus. 
And so you can do an experiment to try to 
understand the receptive fields of cells 
in the retina. 
So how would you do that? 
Well, you could try to flash spots of 
light that are circular, as shown by the 
yellow circle here. 
different locations on the retina. 
And what you'll find is that for any 
particular cell that you're recording 
from, let's say, this particular cell 
over here. 
You might find that the cell only 
responds when you turn on a spot of 
light. 
So when you turn on a spot of light in 
this particular location. 
And that generates a robust response, as 
shown by these spikes over here. 
And interestingly enough, when you turn 
on the spot of light in the surrounding 
area. 
In this annulus around the center. 
You might find that the cell stops 
responding. 
So it does not generate these spikes. 
This allows us to define the concept of 
center surround receptive fields in the 
retina. 
So as we saw in the previous slide, when 
you turn on a spot of light in the 
central region, you get an increase in 
the activity of the cell in the retina. 
And when you turn off a spot of light in 
the surrounding region, you also get an 
increase in the activity of the retinal 
cell. 
So this leads us to the concept of its 
On-Center, Off-Surround, Receptive Field. 
And this basically means that the cell 
responds when you turn on a spot of light 
in the center or when you turn off a spot 
of light in the surrounding region. 
Now, the counterpart to this type of a 
receptive field is the off-center. 
On-surround type receptive field. 
And so as you might expect, in this case 
the cell likes it when you turn off a 
spot of light in the center region. 
And also it will respond with increased 
activity when you turn on a spot of light 
in the surrounding region. 
So the plus indicates on, and the minus 
indicates off. 
Now the information from the retina is 
passed on to a nucleus. 
As I mentioned earlier, called the 
Lateral Geniculate Nucleus or LGN for 
short. 
And, this in turn passes information, to 
the back of your brain, to an area called 
the Primary Visual Cortex, and so one 
might ask What happens if you record from 
cells in the primary visual cortex? 
What kind of receptive fields do you 
observe in the primary visual cortex? 
Well, remember what happened in the Hubel 
and Wiesel movie that I showed you 
earlier? 
In that movie we saw that a particular 
cell responded robustly, to an oriented 
bar of light that was oriented in a 45 
degree angle. 
So this gives rise to, receptive fields, 
that look like this in the visual cortex. 
So these are called oriented receptive 
fields, because they're oriented at 
different angles. 
And the, neurons are cells in the visual 
cortex, in this particular case, the 
primary visual cortex. 
Tend to respond the best to bars, such as 
this one, bright bar in a dark 
background. 
So the black or the gray over here 
represents a dark background. 
And so we have a bar that's bright that 
is oriented at 45 degrees, and that is 
what the particular cell that we saw in 
the movie responded to best. 
And so this corresponds to a descriptive 
model. 
Of the oriented receptive field of the 
neuron, in that case in the primary 
visual cortex of a cat. 
Now, obviously these are not the only 
types of receptive fields that are found 
not just at one orientation. 
What you'll find if you recall from a 
whole bunch of cells in the primary 
visual cortex is that the receptive 
fields vary in their orientation such as 
shown over here. 
Some might be vertically oriented or at 
90 degrees, some might be horizontal, 
some might be at a different angle, such 
as the one shown here. 
But you would also find that there are 
cells that respond to dark bars such as 
one shown over here, or the one shown 
over here, it's oriented at a 45 degree 
angle but it likes dark bars. 
As opposed to this one over here, which 
likes the bright bar, oriented at 45 
degrees. 
So we'll later learn in the course how to 
quantitatively estimate these types of 
receptive fields using a technique called 
reverse correlation. 
Now, the second question we can ask is. 
We know that there are center surround 
receptive fields in the retina. 
And also, it turns out, in the LGN, or or 
lateral geniculate nucleus. 
But when you come up to the cortex, you 
find this oriented receptive fields. 
So, how do we get from center-surround 
type of receptive fields to oriented 
receptive fields? 
And that leads us to a Mechanistic Model 
of Receptive Fields. 
And that'll be covered in the next 
lecture. 
So, see you then. 

Hello and welcome back. 
In the previous lecture Hubel and Wiesel 
entertained us with their Star Wars Jedi 
Knight lightsaber-like experiment and 
introduced us to the concept of receptive 
fields. 
We learned about a descriptor model of 
receptive fields, and in particular, we 
looked at two different kinds of receptor 
fields, center-surround receptor fields 
and oriented receptive fields. 
So here are some examples of these 
oriented receptive fields. 
And, the question that we asked was, how 
are these oriented receptive fields 
constructed from the center-surround type 
receptive fields? 
In other words, what we wanted was a 
mechanistic model of how receptive fields 
are constructed using the neural 
circuitry of the visual cortex. 
So, in order to answer this question, we 
need to look at the neuroanatomy of the 
visual system. 
Here's what happens to the visual 
information after it's processed by the 
retina. 
It flows through the optic nerve to this 
nucleus, called the lateral geniculate 
nucleus. 
And from there, it flows to the primary 
visual cortex or V1. 
And as we know, in V1, we have receptor 
fields which are elongated that look 
something like this. 
Whereas the lateral geniculate nucleus, 
LGN, we have receptive fields that are 
basically centered-surround, similar to 
what you find in the retina. 
And the question that we're asking is how 
do we go from the center-surround 
receptor fields to these elongated 
receptor fields? 
And the clue to this conundrum comes from 
the anatomy. 
So if you look at the anatomy, you'll see 
that a number of LGN cells converge to 
single V1 cells. 
And so, what this means is that a single 
V1 cell receives inputs from a large 
number of LGN cells. 
And so the question then becomes, how do 
the inputs which have receptive fields 
such as this, give rise to a receptor 
field in the output in V1 that looks 
something like this? 
So I'll give you a couple of moments to 
figure out the answer. 
So are you ready? 
Let's see. 
Here is the answer. 
It's actually quite obvious when you 
think about it. 
In order to form a receptive field that 
is oriented and looks like this, all you 
have to do is arrange the inputs to have 
receptive fields that are aligned in this 
particular manner. 
And, given that, there is a feed forward 
connection or a convergence of these 
inputs onto one particular V1 cell. 
This particular cell is going to behave 
as if it has this type of a receptive 
field. 
In other words, this particular cell in 
V1, our primary visual cortex is going to 
behave just like the cell that we saw in 
the Hubel and Wiesel movie. 
It's going to respond to any bar that is 
oriented in this particular way and is 
bright and has this particular 45 degree 
orientation. 
So this particular model is a mechanistic 
model of how receptive fields in V1, of 
this particular kind are constructed. 
And this was suggested by Hubel and 
Wiesel in the 1960s. 
This model is actually quite 
controversial, in the sense that, it does 
not take into account other inputs that 
this V1 cell is receiving. 
So if you look at the neuroanatomy, in V1 
you'll find that there are a lot of 
recurrent connections from one V1 cell to 
other V1 cells. 
So each V1 cell receives inputs from its 
counterparts within V1, in addition to 
the feed forward inputs from the LGN. 
And the Hubel and Wiesel model does not 
take into account these recurrent inputs 
to V1. 
And it turns out that these frequent 
inputs also contribute to the responses 
of the V1 cell. 
We will look at both feed forward, as 
well as recurrent networks a little bit 
later in the course when we discuss 
network level models. 
Okay, great. 
Now, let's move on to the last type of 
models. 
These are called interpretive models. 
So we're going to look at an interpretive 
model of receptive fields, and the 
question that we're asking is, why are 
receptive fields in V1 shaped in this 
particular way? 
In other words, why do they look like 
this? 
Why do they have this orientation and why 
are they selective for bright or dark 
bars? 
Another way of asking the same question 
is, what are the computational advantages 
of such receptive fields? 
Now, this is the kind of question that 
perhaps an engineer or a computer 
scientist would ask. 
So why do we need to use these types of 
receptive fields? 
Do they confer any advantages? 
So, let's look at one particular 
interpretive model of receptive fields. 
The interpretive model that we look at is 
based on the efficient coding hypothesis, 
so what does this hypothesis state. 
Well, it states that the goal of the 
brain, through evolution for example, is 
to represent images as faithfully and as 
efficiently as possible using the neurons 
that it has. 
Now, these neurons have receptive fields 
RF1, RF2, and so on. 
So here are some receptive fields of 
neurons. 
And the question we're asking is, are 
these the best way, or the most efficient 
way, of representing images. 
And so, how do we represent images using 
these types of receptive fields? 
Well, as an example, I can take these 
receptive feels are of three and four so 
as take these two, and I can just add 
them. 
So imagine that they are images, so here 
is a bright region in the image and a 
dark region in the image bright region in 
the image and dark in the image so if i 
think of these as just two image patches 
so these two receptive feels I can add 
the two receptive fields. 
And what kind of an image can I 
reconstruct? 
Well, if you add these two together, 
you're going to get some linear 
combination of these regions. 
So, you have that particular shape. 
So, it's going to be a really bright 
region up here. 
Maybe a slightly bright region here and 
here and here and here. 
Because you're adding the plus is here 
with a little bit of the minuses over 
there, and you're going to get some dark 
regions over here. 
And, so, what you have is an image that 
looks something like that. 
So, given that you are adding these two 
receptive fields, you have a image that 
you reconstructed that looks something 
like, let's say, a plus sign. 
Now, if you're given a whole bunch of 
these receptive fields, you can literally 
combine them in this particular way. 
So this is just a summation sign over all 
the receptive fields, RF1, RF2 and so on 
and each of these are weighted by some 
number. 
So, these are the neural response, so a 
linear combination of them is going to 
give you some particular image. 
Now, what is the goal here? 
The goal is to find out what are the 
receptive fields, RF1, 2, and 3, and so 
on that minimize the total squared 
pixel-wise errors between a given set of 
images. 
So one of these images, perhaps the brain 
is trying to optimize its representation 
for natural images. 
And so we can look at the squared 
pixelwise errors between natural images 
and the reconstruction of those natural 
images I had. 
And we've also add an additional 
constraint, so you want them to be 
efficient and so we want these responses 
for example to be as independent as 
possible we don't want all the neurons to 
be firing at the same time for example 
and so we can add the constraint that. 
We want these coefficients or these 
responses r sub i to be as independent as 
possible. 
So given that we have now this 
optimization criteria and this particular 
idea of minimizing the total square pixel 
wise errors. 
And also keeping these responses as 
independent as possible. 
What are the receptive fields that 
achieve this objective? 
So, here's what we could do. 
We could start out with a set of random 
receptive fields. 
So, we are assuming that we don't know 
what the optimal receptive fields are. 
And we could run our efficient coding 
algorithm, which tries to minimize the 
reconstruction error. 
The error of the square pixel-wise errors 
between the reconstructed images and the 
actual images, and we can run it on 
natural image patches. 
So, why natural image patches? 
Well, we can assume that the brain has 
evolved to process natural images. 
In that case, if the brain is indeed 
trying to perform efficient coding, then. 
Perhaps it's trying to be efficient on 
these types of images, natural images of 
plants and trees et cetera. 
And so we can take these kinds of images 
and run our efficient coding algorithm on 
tiny patches. 
So here's an example of a tiny patch. 
So we can randomly sample from different 
locations on these natural images and 
then run the efficient coding algorithm. 
So what is the efficient coding 
algorithm? 
Well, there's several different kinds. 
So one is called sparse coding and this 
was suggested by Olshausen and Field. 
Another one is independent component 
analysis and this is suggested by Bell 
and Sejnowski. 
And then another algorithm called 
predictive coding. 
Was suggested by myself and Dana Ballad 
this was back when I was a graduate 
student I think I'm giving away my age 
now, but that's okay. 
And basically, these three types of 
efficient coding algorithms give rise to 
a set of receptive fields that I have 
been optimized as a less. 
Reconstruct these particular tiny image 
patches within these natural images as 
faithfully as possible. 
So, let's look at what these receptor 
fields that started out as random, but, 
then they were tuned by the efficient 
coding algorithm, what they look like 
after they've converged. 
Aha, so here they are. 
What you see here is that each of these 
is one particular receptor feel that has 
been learnt from natural images by the 
efficient coding algorithm. 
And you'll see that each of them seems to 
have remarkably structured that's quite 
similar to what we saw before in terms of 
these types of receptive feels in V1. 
In other words, they are receptive fields 
that are oriented, so there's an 
orientation here and they have both the 
white and dark regions. 
So in other words, the plus and the minus 
that we saw in the V1 receptive fields, 
you see them here also. 
And you can see that they're localized to 
different locations, so they're very 
specific to location, which is another 
characteristic of receptive fields. 
And they're oriented and they have 
different orientations that span the set 
of orientations that you might get in 
natural images. 
And so what is the conclusion that we can 
draw? 
The conclusion is that the brain maybe 
trying to find faithful and efficient 
representations of an animals, natural 
environment. 
So people have applied this principle 
also to other kinds of inputs, such as 
sounds for example. 
And they find that the auditory cortex 
representation is also quite explainable 
by this type of principle, the principle 
of efficient coding. 
Okay, great. 
So we'll explore a variety of these types 
of Descriptive, Mechanistic, and 
Interpretive models throughout the 
course. 
But before we do that, we have to do one 
thing and you might guess what that is? 
No, it's not homeworks. 
It's being introduced to neurobiology. 
So a lot of you might not have a 
background in neurobiology. 
So for those of you have, who have never 
taken a neurobiology course before, the 
next set of lectures will introduce you 
to neurons, synapses, and also brain 
regions. 
So until then, adios, amigos, and amigas. 

[MUSIC]. 
Hello and welcome to Neurobiology 101. 
In this lecture and in the next two 
lectures, we'll introduce you to neurons, 
synapses, and brain regions. 
Now you might have already taken a course 
or two in neuroscience or neurobiology in 
your careers. 
And if so, you might consider skipping 
this lecture and the next two ones. 
But on the other hand, if you do so, you 
might miss on some of the entertainment 
we've planned in these lectures. 
Okay, let's get started. 
If the brain is a stage, the star of the 
show are the Jackie Chan, Julia Roberts, 
Shah Ruck Khan and Tom Cruise are rolled 
into one. 
Would undoubtedly be the neuron. 
Now here is an example of the neuron, 
this is a neuron from the visual cortex 
and you can see that the neuron is 
extremely tiny. 
The size of the cell body is only around 
25 microns. 
That's one micron is one millionth of a 
meter. 
And you can also see that the cell has 
different branches, so the neuron has all 
these branches which are called 
dendrites. 
And there's also one slender branch that 
goes down like this and that's called the 
axon. 
And as we'll see later, that conveys the 
output of the neuron. 
Now this is not the only type of neuron 
that exists in the brain. 
Let's look at some other types. 
There is in fact, a veritable zoo of 
neurons, as shown by these wonderful 
drawings by Ramon Y Cajal from the turn 
of the last century. 
Here are some cells from the visual 
cortex. 
And you'll notice that some of them, such 
as this one and this one, have a 
triangular shaped cell body. 
These are called pyramidal neurons, 
partly because of the shape of their cell 
bodies. 
But also because their axons form what is 
known as the pyramidal track in the motor 
system. 
Now here we have some cells from the 
cerebellum. 
So these two cells over here are called 
purkinje cells. 
And you'll notice they have a very 
interesting branching structure in their 
dendrites. 
And then finally here we have some cells 
from the Optic Tectum. 
And once again we notice a wide variety 
of cells. 
And you'll notice that they have 
different branching structures depending 
on where they are in terms of the depth 
of their location. 
Now when Ramon Y Cajal made these 
drawings back in the early 1900's, there 
were some controversy in the field 
regarding the structure of the brain. 
Now the two competing hypothesis were 
that, one, that the brain is a continuous 
network and this was called the vaticular 
hypotheses. 
And the other hypotheses was that the 
brain consists of discrete cells. 
And when Ramon Y Cajal made these 
drawings and he had very clear 
observations that the cells were indeed 
discrete. 
This led to what is known as the neuron 
doctrine. 
So what is the neuron doctrine? 
The neuron doctrine states that the 
neuron is the fundamental structural and 
functional unit of the brain. 
And moreover, the neurons are discrete, 
and thereby not continuous with other 
cells. 
So there's actually some exceptions to 
that rule, but it holds true for a 
majority of the brain regions. 
And finally, the third part of the neuron 
doctrine states that, information flows 
from dendrites via the cell body to the 
axon. 
And once again, there are some exceptions 
to this rule, but it holds true for a 
majority of neurons. 
So, this leads us to what we might call 
an idealized model of a neuron. 
So let's look at what an idealized model 
of a neuron looks like. 
Here's the idealized model of a neuron. 
You can see that it has these dendrites, 
as well as a cell body and then the axon 
that conveys the output of the neuron to 
other cells. 
Now the inputs to the neuron might come 
from axons, from other neurons and how do 
these inputs look like inside the neuron? 
They look something like this. 
So each time you have an input from a 
neuron that is then translated into some 
activity inside this neuron. 
It's going to give rise to what is known 
as an EPSP or Excitatory Post-Synaptic 
Potential. 
That's one kind of input that the cell 
might receive. 
And if there's a whole bunch of them, so 
here's one, two, three, four. 
And these are all EPSP's that arrive 
let's say almost simultaneously, then 
what you get at the cell body or indeed 
in these regions. 
If the, if these arrive in neighboring 
regions, it's a summation of the EPSP's, 
and if there's enough of them, then the 
summation might reach a threshold. 
And what you will get out of that is 
something called an action potential or 
spike. 
So here is a depiction of an action 
potential. 
And so what we have here is basically a 
really simple model of inputs coming in 
from other neurons being summated in the 
cell body. 
Followed by an output, which is called a 
spike or action potential. 
And that happens if some particular 
threshold is reached. 
When you have enough inputs coming into 
the neuron. 
So that is a really simple depiction of 
what happens in a neuron. 
Now let's go into a little bit more 
detail. 
Let's start by asking what is a neuron. 
Well a neuron is nothing but a leaky bag 
of charged liquid. 
Now, I know that doesn't sound too 
attractive, but that's basically where it 
is. 
It's a leaky bag of charged liquid. 
Now, why is it a bag? 
Well, it's a bag because the contents of 
the neuron are enclosed within a cell 
membrane. 
What does the cell membrane consist of? 
It's actually a lipid bi layer. 
So, here's a depiction of that. 
The bi layer. 
And what do we mean by lipid? 
So do you know what a lipid is? 
Well, lipid's are nothing but fat. 
And so what we have here is basically a 
fatty bi layer. 
That is enclosing the contents of the 
neuron. 
Now the bi layer. 
The cell membrane is impermeable to 
charged ions such as sodium chloride and 
potassium which are dissolved on the 
outside. 
And are also present on the inside of the 
cell. 
Now, if that was all that there was in 
our brains, just a cell membrane with 
things on the inside and things on the 
outside. 
Nothing much would be happening and 
things would be pretty boring. 
So, things really get interesting because 
embedded within this, this particular sub 
membrane are what are known as ionic 
channels. 
Now these ionic channels allow ions to 
pass from the outside to the inside or 
from the inside to the outside and that's 
where all the action happens. 
And so we'll be looking at the actions of 
these ion channels in a little bit more 
detail in the next few slides. 
Now you can see that the neuron has an 
electrical personality because it 
maintains a potential difference between 
the outside and the inside. 
So the inside of the cell, the inside of 
a neuron is approximately minus 70 
millivolts compared to the outside. 
So if the outside is zero millivots, the 
inside is minus 70 millivolts. 
And this is basically called the resting 
membrane potential because this is when 
there is no spike or action potential. 
And this particular difference between 
the outside and the inside is. 
Because there is a higher concentration 
of certain ions on the outside compared 
to the ions on the inside. 
So, in particular, there's more of sodium 
and chloride on the outside, compared to 
on the inside. 
Now that's pretty interesting. 
So why should there be more sodium and 
chloride? 
And water, on the outside. 
Compared to the inside. 
Well, if you want to speak on it a little 
bit. 
Maybe it's because it's, reflect to where 
we came from as single cellular organism. 
So recall that life. 
Perhaps began in the ocean. 
And what does the ocean have? 
Well it's NACL. 
Salt, so salty water. 
So perhaps, we're still living. 
At least in our brains, in something like 
the ocean. 
Anyway, that's just a speculation. 
Now, it's not sufficient to just have a 
higher concentration of one set of ions 
and another set of ions on the inside. 
What you really need to maintain this 
difference between the outside and the 
inside is something called an Ionic pump. 
Which actively expels sodium from the 
inside to the outside and then it also 
allows potassium ions from the outside to 
the inside. 
So this takes energy. 
So a lot of the energy that we consume is 
actually devoted to maintaining this 
particular difference between the outside 
and the inside of neurons. 
Well, if it's always minus 70 millivolts 
on the inside of the neuron, then things 
are pretty boring. 
There's not much happening in the neuron. 
So how do we influence a neuron's 
electrical personality? 
Well how do we influence anyone's 
personality, whether it be electric or 
not? 
Well, we need to provide them with some 
inputs, so in the case of neurons what 
this amounts to is asking. 
How can the electrical potential be 
changed in local regions of a neuron? 
And the answer to this question, again 
comes down to, our good old friends, the 
ionic channels. 
So the ionic channels, which I earlier 
mentioned, are embedded in the cell 
membrane. 
Are nothing but proteins, and they're 
selective in that they only allow 
specific ions to pass through. 
So for example this particular ionic 
channel might only allow sodiums. 
So perhaps these blue circles depict 
sodium ions, and so only sodium ions are 
allowed to pass through. 
Whereas the potassium or chloride ions 
which might be these yellow and red 
circles are not allowed to pass through. 
Now to make things a little more 
interesting, the ion, the ionic channels 
are gated. 
So what do we mean by that? 
Well, they're gated in the sense that 
they might change their property of being 
open or closed to ions. 
According to conditions that are existing 
in the neighborhood of the ionic 
channels. 
So for example, there's three different 
types of ionic channels. 
One is voltage-gated, so in this case, 
the probability that the ionic channel is 
open depends on the local membrane 
voltage. 
So if the membrane voltage is high, for 
example, then the channel might open and 
then it might allow sodium ions to come 
through. 
And if the membrane voltage in this local 
region is low then perhaps it will close 
and then no longer will sodium ions be 
able to come inside the cell. 
Now similarly another set of ionic 
channels are chemically gated. 
So in this case the opening of the 
channels is determined by the presence of 
certain chemicals. 
And these chemicals bind to the ionic 
channel, and the binding causes the ionic 
channel to open and let inside certain 
ions. 
Or perhaps let ions from the inside go 
outside. 
We'll actually come to these chemically 
gated ionic channels later when we 
discuss synapses. 
Synapses are examples of chemically-gated 
ionic channels. 
And finally, there's another class of 
ionic channels which are called 
mechanically-gated, because they are 
sensitive to pressure or stretch. 
And so they open or close depending on if 
there is pressure or if there's a stretch 
in the, the neighborhood. 
These gated channels are what allow 
neuronal signaling and communication 
between neurons in the brain. 
So here's an example, suppose you have 
spikes that are arriving from another 
neuron, and the spikes arrive and 
terminate at these locations. 
On this neuron number two, so these 
junctions between the neurons are called 
synapses, and let's see what happens in 
this particular region here. 
So when you have the inputs from other 
neurons, in this case the input coming 
from this neuron's axon. 
You have an opening of the chemically 
gated channels at these locations. 
So, these chemically gated channels are 
at these synapses, one, two, and three, 
and when these chemically gated channels 
open. 
They are going to cause changes in the 
local membrane potential. 
So for example here, if there are some 
sodium channels they might start opening, 
and that in turn is going to cause sodium 
ions from the outside to come inside. 
So do you know why they are coming 
inside? 
Well, remember that there's more sodium 
ions on the outside, there's a higher 
concentration. 
Which means that there's going to be a 
diffusion of these sodium ions into the 
inside. 
And since sodium ions are positively 
charged, they're going to cause an 
increase in the local membrane potential. 
So what happens when you have an increase 
in the local membrane potential? 
Well, this in turn is going to cause 
opening or closing of voltage-gated 
channels. 
So remember there's a second type of 
channels called voltage-gated. 
So when the local membrane potential 
changes, is going to cause these voltage 
gated channels to also start opening and 
closing. 
And this in turn is going to result in 
either a depolarization which is, that's 
a big word. 
But what it really means is a positive 
change in the local voltage of the cell. 
And it might also, in the other case, it 
might cause a hyperpolarization. 
So that's when for example, what will be 
open here are some potassium channels, so 
remember that there is more potassium on 
the inside compared to the outside. 
And so when these potassium channels 
open, then you are going to have some of 
the positive charge. 
So potassium is positively charged, you 
are going to have those potassium ions 
diffusing to the outside. 
And that is going to result in a negative 
change in voltage. 
So in either case, you're going to have 
some changes in the local voltage of the 
different parts of the cell. 
And now if there's a strong enough 
depolarization, which is, there's a 
strong enough excitation. 
And then the excitation reaches that 
particular threshold that we talked about 
then you're going to get a spike or an 
action potential. 
So let's look at the action potential in 
a little bit more details as to how it's 
generated as a result of these voltage 
gated channels. 
So here's the action potential, and. 
The reason why we have this particular 
shape of the action potential is because 
of voltage-gated channels. 
So let's see how an action potential is 
generated. 
So when there's a strong depolarization 
or excitation in the cell then we have an 
opening of potassium channels. 
And that's because these potassium 
channels are voltage gated. 
And so when there's an increase in the 
membrane voltage you're going to have 
these potassium channels opening, and 
that in turn causes a rapid influx. 
Of these sodium ions from the outside of 
the cell into inside the cell. 
And this causes even more channels to 
open, because remember these are 
voltage-gated channels. 
And so when the membrane potential 
increases, then even more channels are 
going to open. 
And so here's a depiction of what 
happens. 
So initially we have these sodium 
channels opening, and that's given by the 
initial part of this trajectory of the 
action potential. 
And as the membrane potential increases 
you have more sodium channels opening and 
that's basically you can look at it as a 
positive feedback loop. 
And so the more these sodium channels 
open, the more the local membrane 
potential increases. 
And the more these sodium channels open 
until you get to this very top and that's 
when you have these channels 
inactivating. 
So, when these channels inactivate at the 
same time, or approximately at the same 
time, you also have the potassium 
channels opening. 
So we have the sodium channel closing, 
and the potassium channels opening. 
And as the potassium channels open, you 
know what happens? 
Well, since there's more of the potassium 
ions on the inside, you're going to have 
an outflux of the potassium ions. 
And that causes a decrease in the 
membrane potential. 
And that's responsible for this downward 
slope of the action potential. 
And finally. 
You get to the point where, at the very 
end here of this cian block you're going 
to have the potassium channel also 
closing and that completes the action 
potential. 
So you have this very stereotypical shape 
of the action potential, which is given 
by the sodium and the potassium channels 
opening and closing. 
And this particular shape is, as we 
mentioned, very stereotypical. 
So it means that there really isn't any 
information communicated between neurons 
in the shape of the action potential. 
So it's not that one neuron has a short 
action potential and another one has a 
long, tall action potential or that. 
And one neuron has a very broad action 
potential. 
Another one has a very narrow action 
potential. 
They're all going to look the same 
because you have the property that the 
action potential is given by the dynamics 
of the sodium channels opening and then 
closing along with the potassium channels 
opening and closing. 
So what happens to the action potential 
after it's generated at the initial 
segment of the axon? 
Well, it's propagated along the axon as 
depicted by this animation. 
So at each location on the axon you have 
these sodium channels opening first and 
then that causes the rising edge, the red 
part of the action potential. 
And then this is followed by those sodium 
channels closing and the potassium 
channels opening as we saw in the 
previous slide. 
And that causes the fall of the action 
potential, so essentially you have the 
action potential propagating along. 
The axon, as shown in this manner due to 
the opening and closing of the potassium 
and sodium channels along the axon. 
If you thought that was a neat trick, 
wait til you hear about this other trick 
that neurons have up their sleeve. 
It's called myelination. 
So there are cells called 
oligodendrocytes that grow these sheaths 
called myelin around the axon of cells. 
So these sheaths are insulating sheaths, 
so they don't allow charge to pass 
through. 
But they do leave open certain areas. 
These are uncovered regions of the axon. 
These are called, nodes of Ranvier. 
And so, here in this picture, the green 
are the Myelin sheaths, and the red, 
uncovered parts are called the Nodes of 
Ranvier. 
Now, why should the brain go to the 
trouble of covering parts of the axon, 
and not covering other parts of the axon. 
Well, it turns out that there is a very 
important function that the myelin 
serves. 
And in particular the myelinated axon 
allows fast long-range communicational 
spikes. 
So for example, if you want to 
communicate spikes from the brain to 
other areas of the nervous system such as 
distant parts of the spinal cord then the 
myelinated axon does the job for you. 
How does it do its job? 
Well, here's what it does. 
It allows the action potential that's 
generated near the cell body to 
essentially hop from one non-myelinated 
region which is one of the nodes of 
Ranvier to the next. 
And this is something that's called 
saltatory conduction. 
So, let's see how this works in this 
case. 
So if you have an action potential that's 
generated in this particular node of 
Ranvier then through the cytoplasm. 
There's a propagation of the charge and 
there's a buildup and another. 
Action potential that's generated at each 
node of Ranvier, and so on until it 
reaches the termination point. 
And so what you have is a fast and a 
robust method of communicating spikes 
along the axon to its destination. 
So we can call this a form of active 
wiring. 
And this allows loss less signal 
propagation. 
A very useful property to have. 
And you might have heard of a disease 
called multiple sclerosis. 
And this disease happens because there is 
an auto immune response where you 
essentially loose a lot of the myelin. 
On your axons and so in that case 
unfortunately the cell loses it's 
capacity to send these action potentials 
or spikes in a fast and robust manner. 
So it will manifest itself in various 
forms and different symptoms all caused 
by the fact that the axons are no longer 
myelemated. 
Okay, so let's summarize what we've 
learned in this lecture. 
We learned about neurons, and in 
particular, we learned that a neuron has 
a cell body. 
It has these branches, that are called 
dendrites. 
And a neuron also has a long, slender, 
fiber, called an axon. 
And the axon carries the output of a 
neuron. 
These outputs are essentially electrical 
impulse called spikes or action 
potentials. 
And these action potentials are delivered 
at these junctions between neurons and 
these junctions are called synapses. 
So what happens at the synapse. 
So what happens to the action potential, 
once it reaches the synapse. 
That is something we going to learn in 
the next lecture. 
So until then, ciao and[UNKNOWN] 

Hello again and Namaste to all of you.
The last lecture we talked about ion
channels and
how they give rise to action potentials or
spikes.
In a later lecture we will describe this
mathematically, using
a model first proposed by Hodgkin and
Huxley in 1952.
Hodgkin and Huxley got a nobel prize for
their computational model.
Now what does that mean for you?
It means that you too can could get a
Nobel prize someday by solving problems
and computational neuroscience.
But before you start making plans for that
trip to Sweden, we first need to
learn about what happens to a spike when
it reaches the end of an axon.
This is where we meet the synapse.
So what is a synapse?
A synapse is a connection or junction,
between two neurons.
There are two different kinds of synapses.
The first kind are called electrical
synapses.
They use something called, gap junctions.
The second kind of synapses are chemical
synapses, and they use chemicals known as
neurotransmitters.
Let's first look at electrical synapses.
Now here's an example of an electrical
synapse
between two neurons, Neuron A and Neuron
B.
Now, electrical synapses function in a
manner very
similar to the way that the connections
between
components in your cell phone in your
computer function.
They allow the activity from one side, in
this case neuron A,
to be directly propagated to the other
side, in this case neuron B.
So they change the voltage on one side,
given
some activity or voltage changes on the
other side.
And the way that neuron A communicates
with neuron B, or vice-versa,
is through what is known as a gap
junction, here.
And the gap junction is depicted here in
terms of these.
So these are essentially ionic channels.
So here's one, here's another and so on.
And you'll notice that these ionic
channels span the membrane, of both,
neuron A an the membrane of neuron B.
And the result of this particular
arrangement is that if you have,
excitation on one side, due to for
example, an action potential.
And maybe you have, sodium
ions on this side that are, in higher
concentration than on the other side, then
these channels allow these ions to migrate
to the other
side.
And the result of this movement of
ions from one side to the other is that
you're going to have a change
in the membrane potential of Neuron B as a
result of some activity in
Neuron A.
So these types of electrical
synapses are really useful when you want
to have fast connections between two
neurons.
And typically they're found in the case
where you need to synchronize, which is
you want to make neurons fire
simultaneously together.
It turns out to be a useful mechanism when
you want to synchronize sets of neurons.
The other case where you would like
to have these types of fast electrical
synapses
or fast connections between neurons is
when you
have to implement something like an escape
reflex.
And that's something that's found for
example in the crayfish.
So that's an example of an electrical
synapse.
Well what about chemical synapses?
So here's a depiction of a chemical
synapse.
Suppose we have a neuron A and a neuron
B, and here's the action potential, a
spike coming in.
Now, what we have on one side, on the
side of neuron A, are these bags, which
are known
as vesicles.
And these are bags of neurotransmitter
molecules.
So these are chemicals that are stored in
these bags.
And so when an action potential spike
comes in along the axon of Neuron A, it
causes these bags, these bags called
vesicles, to fuse with the membrane.
And in doing so, these bags release the
neurotransmitter molecules into the gap,
between the two neurons.
Now this gap is called a, the synaptic
cleft and so when these neurotransmitter
molecules then fuse with the receptors on
the other side.
So these receptors are nothing but the
chemically gated ionic channels that we
talked about in the previous lecture, then
these chemically gated channels start to
open.
And so you know what happens when these
channels start to open.
Well they are going to allow some ions to
either come inside, or go
outside, according to the concentration of
the ions on the inside or the outside.
So suppose that these are channels that
allow sodium ions to come in.
Then you're going to have sodium ions
coming in into this neuron.
And as a result, you're going to have an
increase in the membrane
potential of neuron B.
So you can see how a spike from neuron A
causes this cascade of chemical events.
And then that in turn causes these
channels to open
and that finally causes some changes in
the membrane potential.
So you're going from electrical activity
to chemical and back
to an electrical change again on the side
of neuron B.
Now you might ask yourself, why did
evolution go to
the trouble of, constructing such a
complex, you know, electrical,
chemical, an electrical connection when
you, you could have just,
used these, electrical connections with
gap junctions to begin with?
Any thoughts on that?
So why would, something like this, like in
a chemical
synapse be useful compared to just a
simple electrical synapse.
So here's a possible answer.
A chemical synapse allow you to change the
way that neuron B
is affected by the same spike by simply
changing the number
of ionic channels or the density of ionic
channels that you have on this side.
And so, if for example for the seams
spike, you want to decrease the amount of
excitation
in B, then you could, for example, take
out, or remove, some
of these ionic channels, and then just
leave a few of them.
And in that case, for the seam spike from
neuron
a you would have A lesser amount of
excitation in B.
But this is the reason why chemical
synapses have been suggested as being the
basis
for learning and memory in the brain as we
will look at a little bit later.
Here's a wonderful picture from the
Kennedy Lab at Cal Tech.
Now what does this picture remind you of?
Well it looks like it could be the picture
of
a city taken at night from an airplane
flying overhead perhaps.
Well actually it's the picture of a neuron
and all of its synapses.
Each of these bright spots corresponds to
a synapse.
And typically, a cortical neuron has up to
10,000
synapses on its dendrites and on the cell
body.
Now the synpases can be either excitatory
or inhabitary, so what does that mean?
An excitatory synapse is one that tends
to increase the post synaptic membrane
potential.
So what does that mean?
Well, if you have a neuron A and a neuron
B and you have a synapse between
them, then we call neuron A the
presynaptic neuron.
And we call neuron B the post synaptic
neuron.
So pre and post.
And so an excitatory synpase is one
that tends to increase the postsynaptic
membrane potential,
which means that it tends to excite the
neuron B.
On the other hand, a inhibitory synapse
is one that tends to decrease the
postsynaptic
membrane potential, which means that it
tends
to decrease the membrane potential of
neuron B.
Now how does that happen?
Let's go through the sequence of events
governing the action of an excitatory
synapse.
So first we have an input spike, so here
is the spike.
And the spike is going to cause
neurotransmitters to be released.
In the case of an excitatory synapse, the
neurotransmitter could be glutamate.
And when these neurotransmitter molecules
are released into the synaptic
cleft, they are going to bind with ion
channel receptors.
So in this case the ion channels could be
selected for sodium,
so you might have these sodium selective
channels opening.
And that in turn is going to cause, sodium
ions, to come
into the cell.
In this case, cell B, if this was cell A.
And now, what's going to happen, on the
side of cell B is, something called
depolarization.
So you might remember the word
from a previous lecture.
And that in turn, causes an EPSP, or
excitatory postsynaptic potential.
That's again something again we
encountered in the previous lecture.
And so, the sequence of events is
basically spike
and then you have a release of the
neurotransmitter glutamate.
And then the ion channels opening causing
sodium to come inside.
And that in turn increases
the membrane potential of neuron B.
Now lets look at an inhibitory synapse.
The sequence of events is quite similar to
what we had for the
excited resynapse except that now we have
a different neurotransmitter and different
ion channels.
So.
We begin again with an input spike which
causes neurotransmitters to be released.
And in
the case of an inhibitory synapses, the
neurotransmitter could be GABA.
Now that's not the famous cricket ground
down in Australia.
GABA stands for gamma-Aminobutyric acid
and it's a
kind of neurotransmitter that when
released binds with receptors.
And the Ion channels in this case, could
for example, be selective for potassium.
So when we have the ion channels
open, we'll have some potassium from the
inside of the cell B.
So this is our cell B here and here is our
cell A again.
Now since there's more potassium ions on
the inside of
the cell, we're going to have a diffusion
of the potassium
ions outside, which is going to cause a
decrease in
the membrane potential of cell B, and that
results in hyperpolarization.
So this is again a word we
encountered before, a decrease in the
membrane potential.
And, that's called hyperpolarization, and
that in
turn causes an IPSP or inhibitory
postsynaptic potential.
So that's the counterpart to the EPSP we
saw in the previous slide.
Now these chemical synapses are important
for
another function besides just
communication between two neurons.
Do you remember
what that function is?
Synapses are thought to be the basis for
memory and learning in the brain.
This is called the synapse doctrine.
This is quite deep actually.
Think about what this means.
This means that all of your memories from
the first time you rode your bike and
crashed
into your neighbors brand new car, to the
first
time you met the love of your life is
all stored in your synapses.
What's more, all of your skills from
typing, dancing,
driving to reading and writing, guess
where it's stored.
It's in your synapses.
Synapeses play such an important role in
our lives
that perhaps we should have a world
synapse day.
What do you think?
Okay.
Maybe not.
But how do synapses play such an important
role in memory and learning?
Let's find out.
Synapses allow learning in the brain
through a mechanism called synaptic
plasticity.
Let's look at one particular example of
synaptic plasticity and this
is Hebbian Plasticity, named after Donald
Hebb, who was a Canadian psychologist.
In 1949, Hebb proposed the following rule
for plasticity.
If a neuron A repeatedly takes part in
firing neuron
B, then we should strengthen the synapse
from A to B.
Now here's a diagram that represents
Hebb's rule for plasticity.
So here's neuron A.
And when neuron A fires, it participates
in firing neuron B.
So here is a single spike from neuron B.
Now, according to the rule for plasticity,
we need
to strengthen the synapse from A to B.
And that's shown by this bigger red
triangle.
So what happens?
So the next time A fires, so here's A and
here's the input from A.
You're going to get a bigger and perhaps a
faster response from neuron B.
Now why is this computational useful?
Well.
Suppose you are in a jungle and it is dark
and then you hear a growl.
Let's say the growl is conveyed by this
firing of neuron A.
Now right after that, neuron B fires, and
you see a tiger.
And you start running and barely escape
with your life.
Now if you increase the strength of the
connection between neuron
A and neuron B as we did here using
Hebbian plasticity.
Then the next time that you hear a growl
and that's given by neuron
A firing, you are going to predict that
you are going to see a tiger.
And you're going to start running even
before the tiger
appears, and you have Hebbian Plasticity
to thank for it.
Isn't that great?
Hebbian plasticity is so useful
that there's in fact a mantra that goes
along with it.
Here it is.
Neurons that fire together, wire together.
Some say that if you repeat this mantra
three times
every morning, you are sure to have a
great day.
But I'm skeptical.
Is there any evidence for Hebbian
plasticity in the brain?
In fact there is.
There's something called long term
potentiation
that researchers have observed in several
areas of the brain most prominently in the
area called the hypocampus.
So LTP is defined as an experimentally
observed increase in
the synaptic trend that lasts for hours or
even days.
And the way you can demonstrate that
experimentally is by measuring the size
of the EPSP is caused by a single input to
this neuron B.
So, if you demonstrate that the size of
this EPSP increases as you pair neuron A
with neuron B and you give lots of
stimulation to neuron A which in turn
causes
some excitation in neuron B.
Then, what you'll observe is an increase
in the size.
So here is the final size.
Here is the initial size, so you can see
that the effect of a single input
to this neuron B has increased from being
this little bump to this really big bump.
And that is taken to be evidence for an
increase
in the synaptic strength from A to B.
What about the opposite?
Is there something that, can decrease the
synaptic strength between two neurons?
And which could last for hours or days?
Well, there is something called long term
depression.
That's not the same as the kind of
depression you get when you fail an
exam or you get a rejection letter.
That's a different kind of depression.
This is an experimentally observed
decrease in the synaptic
strength from one neuron to another, that
lasts for hours or even days.
Now how do we experimentally observe long
term depression or LTD?
Well, you can do the same thing that you
did with the LTP.
You could look at the
EPSP that's generated when you stimulate
one neuron
you observe the EPSP in the second neuron
B.
Now if over time there's a decrease in the
size of the EPSP until
you get a smaller sized EPSP for the same
input, from A to B.
Then you have evidence, that there has
been a
decrease, in the synaptic strength from A
to B.
There's a recent twist to the story of
synaptic plasticity that I thought I
should mention.
And that is that synaptic plasticity
depends on spike timing.
So what we mean by spike timing?
Well, it turns out that whether you get
LTP or LTD
depends on the relative timing of the
input or output spikes.
So remember that we have a neuron A that
has
a synaptic connection to neuron B.
And so when you have an input, so we're
calling A the input, and B the output
neuron.
So when we have the situation that the
input spike is before the output
spike, which means that neuron A fired and
then you have neuron B firing.
Then, what you have is the situation such
as this, so here's the input output
pairing, so you have the input happening
before the
output spike, which is given by this spike
over here.
Then what we observe in this case is an
increase in the size of the PSP after
you have repeatedly paired the input and
output In
the sequence that the input occurs before
the output.
Now the interesting case is where you have
the reverse situation
where the input spike occurs after the
output spike.
So, you have B firing and then you have A
firing.
And in that situation, here is the
situation here, here is the pairing.
So you have the output spike here and
then you have the input spiking right
after that.
And so if you keep doing this pairing for
several repitions,
you're going to observe that the EPSP size
diminishes
from being like that to something that
looks like this.
And that's what we mean by long term
depression.
So we have both LTP and LTD.
So if you summarize this particular result
for different intervals between
the input and the output spike you get a
window of plasticity that looks like this.
So we have one portion of it has LTP, the
other LTD,
and here is the delta T.
Between the input and the output spikes.
So the input happens to be before, the
output.
So the delta t is, bigger than zero, then
you are in this portion, of this plot.
And you can see that if the input happens
before the output, you have, LTP
or an increase in the synaptic strength.
If the input occurs after the output where
the delta T is negative then you have
LTD or a decrease in the synaptic strength
from neuron A to neuron B.
So this is actually quite amazing because
you can see that there is a very
short interval of about 40 milliseconds
between the input and output.
And depending on where, the input spike
occurs with respect to the
output spike, you could get a dramatic
shift, from LTD, to LTP.
So, the neurons, in several brain areas
including the cerebral cortex,
seem to be very sensitive to, the timing,
of the input versus the output spike.
And we learn later that this type
of sensitivity to the timing of spikes is
very important for learning sequences to
make predictions.
Okay.
To summarize the last couple of lectures,
we seem to
know a lot about ionic channels, about
neurons, about synapses.
But what do we know about how networks of
neurons give rise to perception, behavior,
and maybe even consciousness?
Well, not as much.
This is actually one of the primary
motivations for the recent
large scale brain projects that have been
announced in Europe and the US.
In Europe, the human brain project, led by
Henry Markram, will
attempt to construct a large scale
computer simulation of the human brain.
While in the US, President Obama has
announced the brain initiative to map the
activities of hundreds thousands of
neurons simultaneously
in order to understand how large scale
networks of neurons give rise to
perception action and cognition.
In the next lecture, we will briefly cover
what we
do know about networks in the brain and
their function.
Until then, bye bye.

Hello neuro adventurers. 
We're nearing the end of our safari 
through the land of neurobiology. 
I have my adventure hat on. 
Do you? 
In the last two lectures, we had close 
encounters with those wild creatures 
called neurons, and their close 
companions. 
The synapses. 
In this lecture we'll stop at a few 
tourist destinations in the nervous 
system. 
The nervous system has two parts. 
The peripheral nervous system and the 
central nervous system. 
We'll make a stop first at the peripheral 
nervous system. 
The peripheral nervous system consists of 
two components, the somatic and the 
autonomic nervous systems. 
Let's first look at the somatic nervous 
system. 
The somatic system consists of nerves 
connecting to all of your voluntary 
muscles and your sensory receptors. 
Now what do we mean by nerves? 
A nerve is nothing but a bundle of axons. 
So you'll recall that axons are the 
output cables of neurons. 
And so, a collection of axons or bundle 
of axons would correspond to what we call 
a nerve. 
Now, let's take an example of where we 
see the somatic system in action. 
So, suppose you are about to shake the 
hand of one of your friends. 
Now in order to shake your friend's hand 
you have to move your arm and your hand 
towards the person's hand. 
So in doing so you're going to be using 
the somatic nervous system to send 
commands from your brain and spinal cord. 
To your hand and arm to move towards the 
person's arm and hand. 
And in return, when you shake your 
friend's hand, you're going to get the 
sensation of shaking your friend's hand 
through the sensory receptors. 
That are on your skin in your hand. 
And so, in doing so we are using two 
different kinds of nerve fibers. 
The first one is called, afferent nerve 
fibers, and these correspond to all the 
axons that are carrying sensory 
information from the periphery. 
Which in this case the hand to the 
central nervous system, which is your 
brain and your spinal cord. 
Now, in order to shake your friend's 
hand, you send some commands from your 
brain to the muscles in the hand. 
In doing so, you were using what are 
called efferent nerve fibers. 
These are the outgoing nerve fibres, or 
axons that are carrying information from 
the central nervous system, which is the 
brain and the spinal cord, outward 
towards the periphery, which in this case 
are the muscles in your, hand. 
The second component of the peripheral 
nervous system is the autonomic nervous 
system. 
And the autonomic system consists of 
nerves that connect to all of your 
internal organs such as the heart, blood 
vessels, various smooth muscles and 
glands. 
And the autonomic nervous system largely 
operates below the level of 
consciousness. 
And it regulates various vital functions 
such as your heart rate, digestion, 
respiratory rate, etc. 
And it's also responsible for some 
important functions, such as the fight or 
flight response. 
The other major part of the nervous 
system is the central nervous system, or 
CNS. 
The CNS consists of the spinal cord and 
the brain, and so let's first look at the 
spinal cord. 
Now remember the time that you were 
extremely hungry and you baked a pizza in 
your oven, and when you reached in to get 
your pizza out you accidentally touched 
the extremely hot pizza pan. 
Now even before you could say the word 
ouch. 
Your hand had already been drawn from the 
pizza pan. 
How did that happen? 
Well you can thank something called the 
reflex arc, that is implemented via the 
spinal cord, so here's a diagram of the 
reflex arc. 
And so when your hand touched the 
extremely hot pan, the information was 
conveyed to set off neurons in your 
spinal cord, and that in turn triggered 
spikes that activated the muscles that 
then withdrew the hand from the extremely 
hot pan. 
And so the spinal cord is responsible for 
such local feedback loops, that are 
called reflex arcs. 
Now if that's the only thing that the 
spinal cord did then things wouldn't be 
that interesting, and we would lead 
basically lives that are just full of 
reflexes. 
But instead the spinal cord also has. 
Information coming in from higher brain 
centers, so these are motor control 
signals coming from the brain and these 
activate spinal motor neurons in specific 
ways. 
So for example the brain could tell the 
spinal neurons to implement the procedure 
for walking and so you would be walking. 
through the help of your spinal neurons, 
while the brain could perhaps be doing 
other more interesting things such as, 
you know, talking to your best friend on 
your cell phone. 
Now, as you're doing that, you might 
actually trip over a rock that you didn't 
see. 
And so, in that case, you do need to 
commit that information up to your brain. 
And perhaps that'll interrupt your 
talking on the cell phones. 
In that case, these ascending sensory 
axons convey the information from muscles 
and the skin. 
Back to the brain so that the brain can 
take appropriate actions. 
And so, the spinal cord not only executes 
local feedback moves, but it also uses 
information from the brain to regulate 
the behaviors. 
And as well as convey information or 
feedback from the other parts of the body 
to the brain, so that the brain can take 
the appropriate actions. 
Now we've been talking a lot about the 
brain. 
What are the major regions of the brain? 
And what functions are these brain 
regions involved in. 
Let's do a quick tour. 
Let's start from the rear of the brain. 
This is the region called the hindbrain, 
and the hindbrain consists of three 
different regions. 
So, the first one is the Medulla 
Oblongata. 
And this structure controls basic 
functions such as breathing, muscle tone 
and blood pressure. 
The second structure is the pons, and 
that's the structure over here. 
And it's involved in sleep and arousal. 
And it's also connected to the 
cerebellum, which is the. 
Other major structure in the hindbrain. 
So the cerebellum is involved in a lot of 
things, including voluntary movements, 
maintaining a sense of equilibrium, and 
more recently it's also been implicated 
in language and attention. 
As we move towards the middle of the 
brain, we encounter, quite appropriately 
The midbrain and something called, the 
reticular formation. 
The midbrain controls things like, eye 
movements and your visual and auditory 
reflexes. 
So for example, if there was a loud bang 
in some particular location around you? 
You have a reflex, an auditory reflex. 
That will make you orient yourself 
towards where the loud bang came from, 
and that's being implemented by your 
midbrain. 
The reticular formation performs a lot of 
basic functions including breathing, pain 
perception, it's involved in some muscle 
reflexes. 
It also regulates your sleep and 
wakefulness, as well as, your arousal. 
Near the center of our brains, we find an 
important structure known as the 
thalamus. 
The thalamus consists of a number of 
nuclei. 
What are nuclei? 
Nuclei are essentially clusters of 
neurons that can be defined anatomically. 
The thalamus is traditionally regarded as 
a relay station for all the sensory 
information coming in from our sensory 
organs, such as the eyes and the ears. 
And the sensory information is conveyed 
to the cortex, the structure that we will 
consider in the next slide. 
Now, interestingly the sense of smell is 
not conveyed through the thalamus. 
It goes directly to the cortex. 
the thalamus also regulates sleep and 
wakefulness. 
The other structure that is right below 
the thalamus anatomically is the 
hypothalamus, and the hypothalamus 
regulates our basic needs or if you 
want to call it the four F's. 
Fighting, fleeing, feeding and mating. 
We finally reached the pinnacle of the 
brain and indeed some say the pinnacle of 
evolution. 
The cerebrum. 
The cerebrum consists of the cerebral 
cortex along with other structures, such 
as the basal ganglia, the hippocampus, 
and the amygdala. 
The cerebrum is involved in a diverse set 
of functions ranging from perception and 
action. 
Two various cognitive functions 
including, learning and memory. 
Now one of the most important structures 
in the cerebrum is the cerebral cortex. 
And the cerebral cortex plays a very 
important role in, especially, functions 
that define what it means to be human, 
such as language. 
So let's look at the cerebral cortex in a 
little bit more detail. 
The cerebral cortex is a layered sheet of 
neurons. 
It's about one eighth of an inch thick. 
The famous computational neuroscientist 
Christof Koch has likened the cerebral 
cortex to basically a 14 inch pizza that 
has been stuffed inside of your skull. 
Now here is the sheet that forms the 
cerebral cortex, and you can see that 
when you enlarge a portion of it. 
It consists of these layers. 
Of neurons. 
And the cerebral cortex contains about 30 
billion neurons. 
And each of these neurons makes 
approximately 10,000 synapses. 
And that leaves us with a staggering 
total of 300 trillion connections just in 
the cerebral cortex. 
Now here's an intriguing fact about the 
cerebral cortex. 
It has six layers of neurons, and it 
seems to be relatively uniform in 
structure across different cortical 
areas. 
So if I gave you some cortical tissue 
from that cortical area, and that 
cortical area, you'd be hard pressed to 
find. 
Any differences between this cortical 
area where it says another cortical area. 
And if you look at the way that the 
inputs and the outputs are organized. 
You again find some regularities. 
So the input which might come for example 
from the thalamus. 
Seems to always terminate in the middle 
layers, or layer four. 
And then the output back to the thalamus 
for example, seems to come always from 
layer six. 
And then the output to other subcortical 
regions, or regions that are below the 
cortex, tend to come from layer five. 
Similarly, the output to so called higher 
cortical areas. 
Or areas that process more abstract 
information. 
Tend to come from the, so called, 
superficial layers. 
Or layers two and three. 
Whereas the input or the feedback from 
these higher cortical areas. 
Come and terminate in layer one besides a 
few other layers. 
And so, the suggestion that's being made 
is that there might be a common 
computational principle that is operating 
across cortex. 
Now if that's true or not, or what is 
that computational principle. 
Is something that perhaps you as a 
computational neuroscience researcher can 
try to unravel, or we might make that a 
homework problem. 
Okay great, so now that we've learned 
about brain regions how do they interact 
to produce cognition and behavior? 
Well I'm sorry to say that we don't know 
fully yet. 
But, I'm an optimist, and I think in the 
next 30 to 50 years we might be able to 
figure this out based on a whole bunch of 
techniques that have been invented 
ranging from electrophysiological, 
optical optogenesis, molecular, 
functional imaging, psychophysical, and 
various anatomical, and connectomic. 
Studies, as well as traditional brain 
damage or lesion studies. 
Okay, so let's now do a head to head 
comparison of neural versus digital 
computing. 
Are you ready? 
Here we go. 
First, device count. 
The human brain has, as we've mentioned 
earlier, 100 billion neurons, and each of 
these neurons has up to 10,000 
connections. 
The silicon chip on the other hand has, 
you know, pretty comparable to the human 
brain in terms of the number of neurons. 
We have about 10 to the 10 transistors, 
but these transistors are very sparsely 
connected, so the brain wins in terms of 
the number of connections per component. 
What about device speed. 
So biology has up to a 100 microsecond 
temporal resolution for some neurons, but 
digital circuits are approaching now a 
100 picosecond clock or 10 gigahertz. 
And so digital circuits pretty much beat 
biology hands down when it comes to 
device speed. 
But then how does biology, the brain, 
actually outperform digital circuits in 
some particular tasks, such as perception 
of fast object recognition, speech 
recognition and so on. 
Well, the secret lies in the computing 
paradigm that the brain uses compared to 
digital computers. 
The brain uses massively parallel 
computation using networks of neurons, 
and furthermore it uses adaptive 
connectivity, as we saw in terms of 
synapses that can change the strength 
using LTP and LTD. 
Digital computers, on the other hand, for 
the most part use sequential information 
processing, using the traditional Von 
Neumann architecture with CPUs and fixed 
connectivity. 
And this, in turn, means that the two 
different types of computing systems have 
different capabilities, at least for now. 
Digital computers excel in math and 
symbol processing, whereas brains at 
least at the current point in time are 
better at solving what are called 
ill-posed problems. 
And these are problems such as speech and 
vision. 
Okay. 
So, let's summarize what we have learned. 
The structure and organization of the 
brain suggests specific computation 
analogies in terms of information 
storage. 
It appears that information in the brain 
is stored in the physical and chemical 
structure of neurons and synapses. 
In terms of information transmission, we 
know that neurons use spikes which are 
electrical, as well as they use synapses 
which are chemicals. 
So, the brain appears to be using both 
electrical and chemical modes of 
signalling. 
In terms of the primary computing 
elements, these are neurons, and finally 
what is the computational basis of the 
brain? 
Well unfortunately, we do not know the 
answer to that question, but we can be 
hopeful that perhaps at the end of the 
course, through all the discussions that 
all of you are going to have on our 
course discussion board, maybe we'll 
figure this out. 
but that still remains to be seen. 
But what we will do in the course, and 
that we will do for sure, is that we will 
try to understand computation in the 
brain through our three different kinds 
of models: descriptive, mechanistic, and 
interpretive. 
Thank you all for joining us this week. 
Hope you enjoyed the safari. 
Next week, our other guide, Adrienne 
Fairhall, will introduce you to 
descriptive models of neural encoding and 
decoding. 
I'll be back for the last three weeks of 
the course to tell you about networks and 
learning. 
Until then, this is Rajesh Rao signing 
off. 
Good luck, and don't forget to keep your 
neurons happy. 

[MUSIC]
Welcome to week two of computational
neuroscience.
I'm Adrian Fairhall.
Last week's introduction gave you a high
level overview of
some of the concepts we'll be covering in
the course.
Today we're going to start on the subject
of the
first half of the course which is neural
coding.
As you know and heard last week brains do
many, many different kinds of
things, but one of the best studied
is the representation and transformation
of sensory information.
So the first part of this course will be
an introduction to common concepts
that are used in thinking about and
quantifying the way that information is
represented
in the brain, and how we can extract that
information by monitoring brain activity.
Our ability to respond to the world and
think about it purely
inside our head requires a transformation
of information from one form to another.
Sensory signals or maybe written language
are converted into physical
processes inside our brains that contain
some version of that information.
A natural
way to talk about this is in terms of a
code and our task is
to discover the form in which information
is represented in the activity inside our
heads.
Today we'll be talking about how to go
about cracking this code.
So, first we'll discuss some techniques
for recording from the brain.
We'll then talk about tools for
discovering how the brain
represents information and models that
express our understanding of this
representation.
Next week we'll go on
to thinking about some methods for
inferring what the brain is
doing based on recordings of acts, of it,
of it's activity.
The following week we'll talk about,
information theory
which is a method to quantify neural
representations.
And finally in the fifth week we'll talk
about the biophysical
basis of how the brain processes inputs
and performs complex computations.
So the methods that we use to collect
brain activity determine the ways that
we can analyze it and also the
kinds of information that that activity
can contain.
So let's start by reviewing a few
techniques
that are currently in use to peak inside
brains.
We'll go from large scale to small scale,
starting with some methods that
allow us to record activity from people,
while those people are performing tasks.
So you've probably heard of functional
magnetic resonance
imaging, as this is also used as a
diagnostic
tool in hospitals.
This technique allows us to record from a
persons brain while they're
performing some task, as long as that task
does not require moving around.
The person is placed inside a scanner with
their head in the center of a large
magnet.
The scanner measures spatial perturbations
in the magnetic
field, which are caused by changes in
blood oxygenation.
As different parts of the brain become
active, blood
flows to those areas that support the
underlying neural activity.
This provides a measure of activity over
regions
of the scale of about a cubic millimeter.
Obviously this must represent the average
activity of millions of neurons.
Here's an example of some images that one
can collect from fMRI, showing in color
small regions
of the brains that respond differentially
to some experimental
condition, in this case the viewing of
some images.
While fMRI is a wonderful method for
discovering the approximate regions of
neural activity, as
we've said the responses that are recorded
are
averaged over many neurons and also
they're slow.
The blood flow response to changes in
neural activity
happens over timescales of seconds.
Another method that still relies on
averaging over the responses of
many neurons but has a much
faster response time is EEG
electroencephalography.
This method is faster because it captures
the changes
in the electrical fields of the underlying
neural circuits directly.
Here, one of our former grad students, Kai
Miller, is wearing
a cap covered by electrodes that are
making contact with his scalp.
The downside of EEG is it tends to be a
very noisy signal,
since there are many contributions to the
recorded signal.
Still, methods like fMRI and EEG
are very exciting because, although
they're limited,
they're non-invasive and so they can be
used on healthy, awake human subjects.
Ideally, of course, we'd like to have
access to the activity of single neurons.
In cases when we have direct access to
neural tissue, one
can use devices such as I'm showing here:
a multi-electrode array.
This one developed by physicist Alan
Litke.
Most of the device that you see consists
of electronics and
amplifiers for amplifying the tiny voltage
signals extracted from individual neurons.
At the center, down here, is the array
itself,
blown up on the picture here.
Each electron is about 10 microns across,
roughly the size of
a single neuron, and this array has 512
such electrodes spaced
60 microns apart, so one can record from
many neurons simultaneously,
as is being done here with the slice of
the hippocampus.
The multi-electrode array shown before is
great when it's possible to lay the
tissue directly on the array surface, for
example in experiments using brain slices.
But generally one would like to penetrate
into the brain to see
what neurons are doing when the organism
is carrying out normal behavior.
This rather scary looking electrode is
actually only about, about two millimeters
long.
The tip of each prong is an active
electrode.
In newer versions of such electrodes, that
are being
developed by groups at MIT and elsewhere,
these electrodes can
be moved, individually, into tissue,
allowing one to find active cells.
Other electrodes also have multiple
contact points along them
so that one can record at different depths
simultaneously.
Another beautiful technique for recording
for
many individual cells is through calcium
imaging.
Here, cells contain a calcium indicator
that changes
its florescent properties when calcium
binds to it.
Thus, the fluorescent light intensity is
an indicator
of the amount of calcium inside the cell.
Since calcium enters the cell during
action potentials this signal
acts as a record of the firing activity of
the neuron.
This technique
is being used to record, like here,
for many neurons at once, possibly even
thousands.
It's also possible to use fiber optics to
be able to
open windows like this, our neural
activity deep within the brain.
The electrical techniques mentioned so far
look at the changes in the
electric field outside the cell caused
by signals that the cells generate
internally.
It's also very interesting to look inside
the
cell to learn how these signals are being
generated.
To do this, we can use patch
electrodes, which the experimenter clamps
onto the cell
membrane, and can use to make a direct
electrical contact with the inside of the
cell.
So now we've seen some examples of the
kind of data we have to work with.
Let's move on to talk about the neural
code itself.
Let's start out discussion with some
example experimental data which we'll
take from a very important part of the
brain, the retina.
Your eyes are an outlying but very vital
part of your brain.
The retina is a sheet of cells at the back
of your eyeball that
take light that's focused through the lens
and converts those light signals into
electrical signals.
So by looking at these signals we can get
our first look at the language of the
brain.
Here's a rough cartoon of the experiment.
A retina is dissected from an eye and
placed
on top of one of those multi-electrode
arrays that
we already saw, and it's kept in some
fluid
that will help to keep it alive and
active.
A movie is projected down onto the retina
and
the neurons of the retina respond to the
movie.
So let's zoom in a little on the retina
itself,
which is an excuse, really, to show you a
beautiful image
drawn by Ramon y Cajal, a master Spanish
anatomist
who was active at the turn of last
century.
This is an example of very early
connectomics.
Well, in real life, the cells in the
retina are very densely packed, here
they're drawn schematically in this image
to get a sense of the circuit wiring.
So this image shows you the cells that
capture light,
here at the photoreceptors, both rods,
here, and columns here,
and the successive layers of cells that
accumulate and process the
information from the photoreceptors, until
they finally reach the output cells, here,
the retinal ganglion cells, whose axons
join the optic tack in
heading out of the eye and into the rest
of the brain.
Here's a typical way that we look at
responses of a single neuron.
This is called a raster plot.
Every tiny red dot here is an action
potential or a spike.
When we play the movie once, time goes
this way, the
neuron fires at some particular times,
marked by those red dots.
When we repeat the movie, we can plot the
responses in
that second repetition, and in some cases,
they're almost the same.
We can repeat
it again and again.
The responses for different repetitions
are staggered upward by a little
so that each horizontal strip represents
one repetition of the movie.
So now we're looking at the neural
activity of a group
of about 20 retinal ganglion cells while
the movie was played repeatedly.
In fact, many times, as you can see from
the
number of repetitions in one of these
individual raster plots.
So what you can see hear is that each cell
responds every time at specific sudden
points in the movie.
Sometimes the responses are strong.
Maybe here, and sometimes they're weak,
here.
There are some repeats of the movie, where
the cell doesn't fire at all.
What I hope you can see from this is
something extremely beautiful
and exciting, each neuron is encoding some
feature or features of the movie.
And each neuron is responsible for
a different set of features, sometimes
overlapping.
For example, Cell R and Cell P have a
lot of features in common, but sometimes
they're very different.
So our questions are, how do we use these
responses
to determine what in the stimulus is
making that cell fire.
Looking at this picture one also wonders
how should we think about this concerted
activity?
Does every neuron signal its own message
or is
the population responding as a whole in
some complex code?
Well, we'll not answer that question.
We will be discussing models that, that
describe single neuron and population
level responses.
So the questions we'll be addressing over
the next
few weeks involve how we read out this
code.
There are two ways of looking at it that
are of course quite related.
We can consider the end-coding problem.
How does a stimulus cause a pattern of
responses?
This leads us in the direction of building
quasi-mechanistic models
of our neural system that allow us to
predict it's response.
We can also remain agnostic about the
system
and simply ask what do the responses that
we
observe tell us about what the stimulus
was?
This is the approach one might see at work
in
say a neural prosthetic used to drive a
robot arm whose
job would be to read some measure of
neural data
and activate the arm to move in that
person's intended direction.
So our goal will be to build models of
this type.
Because neural systems are noisy or may
contain information about aspects
that we don't control, we think about the
model as inherently probabilistic.
We would like to know the probability of a
response, given a stimulus.
This is a conditional probability
distribution.
Conversely, in decoding, we'd like to know
the probability of the,
of a stimulus having been shown, given the
response that we recorded.
What we'll have to define and ultimately
discover
is what is the appropriate measure of
response?
What is the right way of thinking about
the stimulus?
And what's the relationship between them
that's embodied by our coding model?
So here's an example of neural
representation of information.
We'd like to find some stimulus parameter,
along which the neural response varies.
So we have stimulus parameter, in
everything that
follows I'm going to call my stimulus
parameter S.
And the meaning of S is going to change
from slide to slide.
And neural response, the simplest way we
can be think about this, and the
approach that we will be taking today, is
that the neural response is some
kind of average firing rate or probability
of generating a spike.
So here are a couple of classic examples
of what we call tuning curves.
Here's data from a neuron in primary
visual cortex, V1.
Such neurons respond to oriented bars of
white, here,
passing through a sudden location in the
visual field.
As the orientation of the bar is charged,
say from almost horizontal
to almost vertical, this particular neuron
at first does not respond at all,
but then starts to fire with a higher and
higher rate.
And then less again, as the orientation
moves away from that preferred one.
If you count up the spikes in a time
bend of say, 100 milliseconds, and pluck
those number
of spikes as a function of the
orientation, you'll
get a curve like this, which looks like a
Gaussian.
Here's another example, this time from the
motor cortex.
Now, as the animal in these experiments, a
monkey,
makes an arm movement in a certain angle
relative
to his body the firing rate of this neuron
is large for some angles and small for
other.
And this time the plot of firing rate
versus movement direction is more like a
cosine curve.
It turns out the neurons in primary visual
cortex are sensetive to many input
features and
often the sensitivity to those features is
distributed
in an orderly way, across the cortical
sheet.
So what you're seeing here is an image of
a piece of cortex, a piece of visual
cortex.
The color indicates the value of a
particular feature to
which the neuron or neurons in that
location respond most strongly.
This picture of response as a function
of location in cortex is called a
funcitonal map.
This is an example of some functional maps
in cat
and bush baby, and these classic what's
called pinwheel structures.
In this case, the neurons prefer an
angle encoded by color, changes
systematically in space.
The lower two panels show a map of
preferred spatial frequency
again in cat and bush baby that is the
width of lines
in a grating, to which these neurons have
a strong response.
Again, this shows an orderly pattern
across cortex.
FMRI studies show that there are localized
regions in the temporal
lobe that are responsive to semantic
categories, for example, faces or houses.
So the notion of a stimulus that drives a
neuron maximally can get quite complex.
And the idea of being able to draw a
tuning curve
as a function of some meaningful stimulus
parameter becomes quite difficult.
As we can see here in these famous
experiments.
So this group recorded from single
neurons, in the parahypocampal area in
humans.
This is normally not something one can do,
but here the experiment
has worked with patients who are
undergoing surgery for epilepsy and had
agreed to participate in experiments.
What you're seeing here is the response of
a particular neuron to different images,
where here the number of the image is not
arranged in any particular order.
It is on the x-axis.
So, clearly there are few images that
drive this neuron particularly well.
If we now look at the images that
correspond to those large responses.
It was found that these have a common
property, they all turn out
to be images of Brad Pitt and Jennifer
Aniston.
So interestingly this neuron did not fire
to images of either of
them separately, so see for example this
response of Jennifer Aniston by herself.
Well, its true that because these
experiments were done a long time
ago it's likely that not too many people
have this neuron type anymore.
Maybe some of you, who, unlike me, refuse
to read magazine covers
in the checkout line at the supermarket
are a little bit younger,
some of you may never have had a neuron of
this type.
So here's another intriguing example.
Again, some particularly large responses
to some subset of those images.
Now when we look at which images those
were, we see
that they all have a common property, in
this case, Pamela Anderson.
But interestingly, they're not just
photographs of Pamela Anderson,
here and here, but also drawings of Pamela
Anderson.
And although
you probably can't see it on your screen,
there's also a
case of a picture of Pamela Anderson's
name written in text.
So in later experiments, this group has
also showed that
such neurons also respond to audio clips
of that person's name.
So we can think about neurons like this as
embodying a concept.
So what's emerging from all this, is a
pictgure
of brain regions having increasing
complexity of stimulus representations,
starting from more geometric and becoming
more semantic, here
down in the retina and the thalamus,
lateral geniculate nucleus.
We see very simple forms of receptive
fields.
As we go to V1, we see oriented edges.
As we move up to V4, we see conjunctions
of edges that form contours.
And higher up into the, into the brain
where we see semantic categories such as
houses and
maybe specific houses, such as The White
House.
It's tempting to think about this as a
progression of
features being agglomerated into more and
more specific and complex features.
This also leads to increasing in variance.
Higher order areas are less sensitive to
details,
such as color or location in the visual
field.
So this idea of hierarchical features
being assembled in a feed
forward way is the basis for many powerful
and important models,
including ones that are enjoying a lot of
success in Machine Vision.
What makes the true computation very
interesting
is that these regions are massively
interconnected.
For example, while the thalamus generally
is thought of as a relay
station that takes in information as
represented in sensory receptors, like the
retina, and distributes it to various
other regions of the brain, in
fact it receives a massive amount of
feedback from all of these areas.
And this suggests that these
representations also feed back to
control what information is coming through
in the first place.
So this is how semantics, the meaning or
the value of the context
of an image, can end up influencing its
initial representation in V1 or V4.
So here where there's a role for learning
and for expectation.
What you think you're looking at can shape
what you actually see.
Here's maybe a
nice example of such top down effects.
You might find it hard, at first, to
figure out what this is a picture of.
Once you've seen it, whenever you see this
picture again, which will be regularly,
if you go into the field of
visual neuroscience, you'll always see it
instantly.
And while the role of these top-down
effects is
very interesting and quite an open area
that we've
love to talk about more in this course,
I'm
primarily going to stick with these lower
level, geometric representations.
While later in the course, Roger will tell
you more about how networks
can learn to maintain memories, which
presumably
form the basis of these semantic
expectations.
In the next section, we're going to talk
about how we go about constructing these
response models.
Let's take a break and be back for the
next section.

Welcome back.
We left off about to launch into the
construction of neural response models.
In this next couple of sections, we'll be
looking at models of
increasing complexity that incorporate
more and
more features of realistic neural
responses.
The main work that we'll be doing centers
on methods for finding out what components
of
a stimulus a neural system responds to and
the response function that links stimulus
to response.
For now, we're going to take the response
to mean a single spike
produced by a chosen neuron.
Further, we'll consider how the
probability of
seeing a single spike at a certain time,
or the firing rate, which we'll denote by
r of t, depends on the stimulus.
We'll talk more later, about the model for
how the
probability of seeing that spike,
explicitly depends on this firing rate.
But, for now let's move to a description
in which we try
to determine the rate r(t) as a function
of the input s.
So, what's the simplest, possible
relationship that one could imagine?
That the response at each time, is
linearly dependent on the stimulus, at
that time.
Or perhaps, at some small time in the
past.
That's a t minus tao.
If this were the case, then if we have
some
stimulus which is varying in time like
this, our response, r
(t), is going to look just like that
stimulus that's
scaled by some factor, perhaps delayed by
a little bit and
in the appropriate units.
So what could go wrong with a picture like
this?
In general we expect that the response
depends not just on the stimulus
at some particular time in the past but on
some combination of recent inputs.
If the response depends on some weighted
sum of previous inputs like this we
can still express that as a linear system
but in this more general sense.
So what we're seeing here is that by
starting
at some time, t, we step back over a range
of time points in the past here indexed by
k.
And at each of these time points, we
weight or
multiply, just like our firing factor in
the previous slide.
We weight the stimulus at that particular
time by some factor,
but now the factor depends on how far back
you're looking.
The weight factors are given by these f
(k), which we can write as a function,
f, here defined relative to this index k,
which is stepping backward.
So we take the kth element of our stimulus
weighted by the value of f(k), multiply
the two together.
Move along to the next step.
Do the same thing.
Add them together.
We can also write this in integral form,
where now the variable tau is a dummy
variable
that acts llike the index K.
Some of you might recognize what we've
written here as a convolution.
The set of weights f are equivalent to a
linear filter on the input.
Note the effects of temporal filtering
here.
Although this is just a cartoon, the fast
variations
in S can get smoothed out in producing R.
Let's think about some examples.
What if the response is proportional to a
running average of the input?
What would f look like in that case?
In this case what we expect is that our
response at some given time has to be
weighted by some linear function and
what's that
going to look like as a function of time.
Let's say we're averaging over some window
with n time steps.
So now we'd like to take each of our
values of s at those n time steps.
And weighted by a factor of 1 over n.
That will give us an average over a window
of length n.
And so now our filter f is simply going to
look, look like that.
Where the height here is 1 over n.
See if you can figure out what a leaky
average would look like.
What if r depended on s, averaged over
time, but with a memory that fades in
time?
So time points near to time t would be
weighted with a large value.
But as one goes back from time t, those
white, those weight values would get
smaller and smaller.
So if they're exponentially
decreasing, that's something called a
leaky integrator.
Such a system sums it's inputs, but with
a strength that decays exponentially into
the past.
We'll see a physical implementation of
this later.
The neuronal cell membrane behaves in
exactly this way.
So that was a first pass at predicting how
neuronal
responses might depend on a stimulus that
changes in time.
What if the neuron is sensitive to
patterns of light in space?
We can apply the same idea.
In fact, you've already met this idea when
you
were introduced to receptive fields in the
first lecture.
You might recall from the previous lecture
that receptive fields
in the retina, have what's called a center
surround form.
And that V1 neurons
are sensitive to oriented edges.
Let's think more concretely what that
means.
So our goal here is to take what
we just learned about temporal filtering,
and apply it
to inputs s, which are now defined, rather
than being defined over time, they're
defined in space.
So we're thinking about spacial receptive
fields.
We're thinking about the response of a
neuron to some particular region of visual
space.
So lets say this is our, our visual scene,
we
have a neuron in the retina, its response
is centered
at some point, x, y, in that space.
If the response were truly linear, then,
neglecting
for now, any time variation, the response
of
a neuron who's receptive field is centered
at
that point, x, y, could be computed like
this.
We would take the stimulus which is now a
pattern of light in this two
dimensional space, and weight it at each
point in the scene by the neuron's
receptive
field f, now defined over space.
And add up all those values to get one
single number, the predicted response.
So here's our s centered around x and y.
We now vary this index, x prime and y
prime.
At each point, x prime y prime, we look up
our filter, f,
which is a function of x prime and y prime
of this relative location.
We multiply those two together and add
them up.
Now when a retinal ganglion cell is
responding to some image, its response
is determined by how similar the image is
to its receptive field, f.
What does similarity mean?
It's exactly what we just did, the
weighting of
the image by the filter defined by that
shape.
We can also write this again as a
convolution.
Now let's be a bit more precise about the
cartoon that we drew before for our
receptive field f.
We drew it here in 2D, where the shading
roughly represented the
value of f at each point in space, or in
relative space.
Here's a much better picture.
Now again defining the filter over
the relative co-ordinates, X-prime and
Y-prime,
lets draw the values or weights of f going
into this third dimension.
Now you can see that f has
large positive values here in the middle,
and negative values here in the surround.
So lets think about how a receptive field
like this interacts with an image.
This means, if there's a bright part of
the image, around zero.
So let's center this now, again, at some
particular point, x and y.
We plunk this receptive field over the
image at that point.
We take each of
the values in the image, in this
coordinate frame, x prime, y prime.
We multiply them by the values of f at
those points and we sum them together.
That means that if there's a bright part
of the image at zero, that's going
to drive the neuron positively because f
there is multiplied by that large bright
number.
That increases the response r.
But there, if there is a bright part of
the image that falls into the
surround, that's going to contribute a
large, that
large, bright number, multiplied by a
negative weight.
And that's going to decrease r.
This model for a receptive field is often
approximated as a difference of Gaussians.
A narrow positive Gaussian blob in the
middle.
That's the excitatory center, and
subtracted from it, a
broader and shallower Gaussian to capture
the, the suppressive surround.
The effect of such a differencing filter
is to detect local changes.
Such a filter will respond strongly when
there's, when
there's a bright patch near to a dark
patch.
This is not just some crazy thing that
biology does.
Such filters are ones we use all the time
in image processing.
Look at this example from the gimp
software suite.
Here's again that photo of the Taj Mahal.
And here is the same image in black and
white, when a similar filter has been
applied to it.
Every point in the image on the right is
the result of
a difference of gaussians filter applied
to the image on the left.
You should be able to see that the new
image consists only of edges because of
its differencing property.
This filter cancels out regions in the
image that have constant intensity and
has a large value only when there's a
change in light level or contrast.
So now we've seen how a linear filter can
extract
a spatial feature that explains the
responses of retinal visual neurons.
We started with cases where we just had
temporal dependance and thought about
how a time varying response might depend
on temporal variations in the stimulus.
Now let's put them together.
Because in general the sensory neurons
response will depend
both on time and on other properties of
the stimulus.
In the case of vision, the spatial
distribution
of light intensity.
So what this implies is that we need a
filter f that depends on space and on
time.
So it could be thought about as a little
3D movie.
Here, you're just seeing frames of the
receptive field f over different points in
time.
So now let's go back to the simple case of
pure time dependence.
Imagine, for example, we're driving our
retina
with a blank screen whose brightness is
varying, or that this might be an auditory
signal going into a cochlear neuron.
While the idea of linear filtering is very
powerful
and useful, it can't quite be the full
picture.
So can you see some shortcomings?
So for example, can firing rates be
negative?
Can they increase indefinitely as the
input increases?
Both of those are a possible result from a
linear filtering operation like this.
Solving these two problems can be achieved
with a clever and simple fix.
Without abandoning the power and beauty of
the
idea of linear filtering, we can ensure
that our
output does the right thing by imposing an
extra
step, a nonlinear function applied to the
filtered stimulus.
For example, the function shown above,
this guy here,
will have the effect that, I'll call this
g,
when the filtered stimulus which we
represent here as
s*f or s convolved with f is small or
negative.
Go down to here the firing rate goes to
zero.
When it's very large then the firing rate
will saturate to some fixed value.
This nonlinear function g could have other
behaviors as well.
So now that we have these basic components
of a coding model, this linear filter, and
a nonlinear, static non-linearity that
transforms the filtered
signal into a, into an estimated firing
rate.
Let's take a break and in the next
section, we'll move
onto finding the components of a model
like this from data.

Now, this parameter r is the mean firing 
rate, so what's the mean firing rate of 
our neuron that's given by its response 
curve, and that was, we called that fa of 
s,s. 
And now in this case, we're giving it, 
we're giving the neuron stimulus s, and 
so now our mean response is going to be 
given by that tuning curve value, fa of s 
times t, now k again is the number of 
spikes, ra, that we actually observed, 
times t. 
Now exponential minus and now again we 
need this parameter r. 
That's a tuning curve value. 
Gives us the average response, T divided 
by, ra. 
Now, how do we right down the full 
distribution for all of the neurons at 
the same time. 
So, we're going to represent by vector r. 
The vector of responses to all, of all 
the different neurons rn. 
So, what is the probibility of all those 
responses. 
Because we said they're independent, 
that's just going to be the product of 
all of the individual probabilities. 
Probability of ra equals 1 to n. 
All right, so when we condition that on 
the stimulus, there's a probability r 
given a stimulus. 
In a product again over all the neurons. 
So now if we write in the explicit 
expressions for the individual neurons we 
get a complete expression for our 
population response that's of this form. 
So, now that we have our expression for 
the total stochastic response, let's 
calculate the maximum likelihood solution 
to do that. 
It's useful, as we've done before, to 
work with a log likelihood, rather than 
the likelihood directly, especially when 
we look at this form of this equation. 
So let's take the log. 
So log, probability of r given s is going 
to be equal to so now taking the log of 
that product is going to give us a sum. 
Over r a t comes down from the exponent 
times the log f a s T. 
Now this is going to come out from under 
the exponential, fa s of t minus log of 
ra with t factorial. 
Sum of that overall neurons. 
So now we need to take, now we need to 
find the s, value of s for which that's 
maximized. 
So how does one do that in general, if 
one has some function like this, f as a 
functoin of s? 
And we're looking for the s prime, which 
is that, which takes, for which s takes 
its maximum value. 
And what we do is take the derivative of 
f, with respect to s, and set that equal 
to 0. 
And then solve this equation for S. 
So now I would call this the calculate 
the derivative of, of this equation, 
before we do that we should notice that 
there are a couple of terms here that are 
not going to, not going to survive. 
So, this guy doesn't depend on s, so the 
derivative with respect to s is going to 
go away. 
We should also notice this time. 
So, what do we have here, we have the 
sum, over f a s times t over all the 
neurons. 
A equals 1 to n. 
Now, if you recall we required that this 
be a constant. 
This was a condition that we on our 
tuning curves that we had good coverage 
of the parameter, parameter s across the 
entire range so that the total firing 
rate for the leverage firing rate which 
is given by this term doesn't depend on 
s. 
And so now, when we take the derivative 
with respect to s, that term's going to 
go away. 
So, what do we have left? 
We now have T times sum over a, equals 1 
to N. 
Ra, now the log of some function of s, 
take its derivative with respect to s, 
we're going to get the derivative of that 
function, f prime times T over the 
function itself. 
So now we're going to get T sum over a ra 
f, prime of s, over f of s. 
And now we will set that to 0 and solve 
that for the value of s, that solves that 
equation. 
So now, here's the expression that we'd 
like to, we'd like to solve for, for s 
star. 
So, how do we do that? 
So, f as a function of s is equal to some 
prefactor A e to the minus 1 over 2 sigma 
squared, s minus Sa that central value, 
and now f prime of s equal to A. 
And now we bring down this factor. 
There's, there's a squared that cancels 
with the half. 
There's a minus sign that cancels with 
this minus sign so we get s minus Sa over 
sigma squared. 
Multiply the same thing we had before 1 
over 2 sigma squared S minus Sa squared. 
And so now this equation is quite simple, 
so we have a equals 1 to N ra. 
What do we have left? 
The f prime and the f are going to cancel 
out these exponential factors in the A. 
And so all we're left with is S minus Sa 
over sigma squared equals 0. 
So now one can solve that for s equals s 
star, and we're going to find the 
following solution. 
So s star is equal to ra times sa divided 
by sigma a squared. 
And that's divided by the total sum of 
the firing rates, where each is again 
normalized by its variance. 
Now if all the signals are the same, 
these factors cancel above and below. 
And we get back an expression which 
should look very familiar. 
Its very similar to the population 
vector. 
So what do we gain by all this procedure? 
Well, we made a lot of assumptions to 
make that work out so nicely, and the 
Gaussian distribution is one in which 
that came out so cleanly. 
If we go back, this one step, we see 
something that we would have hoped for. 
That each neuron's contribution is 
weighted by the inverse of it's variance. 
That is if a neuron has a sharply peaked 
tuning curve. 
If it's very informative about the 
stimulus parameter, it's variance will be 
small and it's contribution will be 
large. 
So this method takes care of weighting 
the various neuronal opinions according 
to their reliability and their 
informativeness. 
So, now let's calculate the maximum a 
posteriori solution. 
So, now we have to maximize, not the log 
of the of the likelihood, but the log of 
the a posteriori solution with respect to 
s. 
So, we can write that down. 
We can use Bayes law to just write that 
in terms of the likelihood. 
And in terms of the prior and also in 
terms of, of the marginal distribution. 
And now let's just consider those terms 
that that will depend on s. 
So, as before, we can throw away or, or 
leave out for now, any terms that, that 
don't depend on s, that only depend on r. 
So, that gets rid of this term. 
So now here's our log, of the, of the, 
likelihood that we wrote down before. 
And, the log of the prior. 
And we also have gotten rid of that term 
that was a sum over, over the fine rates, 
because we're again assuming that to be 
constant. 
And won't depend on the stimulus so it 
will dissappear in the derivative. 
So now taking the derivative of seven to 
0, throwing away the sum equals constant, 
we get a new expression that we now need 
to solve for, s prime. 
And you'll see that all we've done is add 
to that the, the this expression that 
depends on, on the prior probability. 
Using again the Gaussianity of the tuning 
curves, you can write down that, that 
solution. 
I'll leave that, I'll leave that as an 
exercise for you. 
And so what you find is that we have 
these original terms that we got from 
the, the likelihood, but now they're 
shifted. 
We now get a solution for s, for s star. 
That's shifted by the prior. 
So, what prior have we assumed here? 
We've taken a Gaussian prior, where the 
mean is less prior. 
And it has some standard deviation, sigma 
prior. 
And now, the effect of such a prior, is 
to add on this additional tone that's 
going to, that's going to bias our 
estimate of s star, and now you can see 
that as one changes the standard 
deviation, the width of this prior. 
So, let's take the case that sigma 
squared is very small, so that we have a 
prior that is very peaked around s prior. 
So that means that this term, this 
additional term is going to get very 
large and now s prior is going to have a 
stronger effect in biasing s star. 
Whereas if the prior is very broad 
equivalent to saying we don't have very 
strong information about exactly, exactly 
what value s takes on in general, then 
this time it's going to be weak. 
And in the limit of, of a constant prior, 
where sigma squared goes to infinity, 
this time would disappear altogether, and 
we'll get back to the maximum likelihood 
solution. 
So what are the limitations of these 
approaches? 
One is that it stays within the confines 
of the description in terms of tuning 
curve and mean firing rate. 
So, it doesn't really incorporate rapid 
time variations. 
It also doesn't take into account 
correlations in the population. 
What was very important to have been able 
to write down a full distribution of 
responses given the stimulus, is it we're 
able to assume that neurons respond 
independently. 
The question of correlations in the 
neural code is, as we've just seen, 
really fundamental to how we go about 
deciphering the firing rates, the firing 
responses of many neurons. 
And this is a major part of the agenda of 
neuroscience for the next decade, 
including with methods such as I'm 
showing here, with calcium imaging. 
Or we can, we can record from many, many 
neurons simultaneously. 
So it's not surprising that understanding 
correlations both experimentally and 
theoretically is a major current area of, 
of investigation. 
My friend, and University of Washington 
colleague, Eric Shae-Brown, will be 
joining us as a guest lecturer in the 
course, to talk about this issue in more 
depth. 
So we're going to stop here, for this 
session and come back to finish this 
lecture with a discussion of, of 
reconstruction. 
How one recreates estimates of time 
varying conflict stimuli from neural 
activity. 

We're going to finish up today's lecture 
with a discussion of reconstruction. 
I think we all share the dream that one 
day we might be able to record brain 
activity, during our sleep, and in the 
morning play back our dreams. 
So while the dream state is still not 
well understood, how close are we from 
being able to reconstruct even awake 
sensory experience from neural activity. 
We can apply the methods that we've 
discussed in the past two lectures to 
think about how to do that. 
So in the previous parts of this lecture, 
we talked about methods to find an 
estimate of the stimulus using Bayesian 
decoding. 
So now we'd like to extend our decoding 
procedures to the case of the responses 
and the stimuli. 
At varying continuously in time. 
Let's go through a simple example of a 
decoding strategy that meshes with the 
problem set that you have been working 
on. 
Some of you, anyways. 
Let's say that we wanted to find an 
estimator, S Bayes, that gives us the 
best possible estimate of our stimulus, 
S, given that we've observed some 
response, R. 
So, how should we compute s bayes? 
So one startegy that makes sense is to 
ask for an estimator that is only average 
as close as possible, to our stimulus. 
So I'm going to intriduce some error 
function which we'll call L and then 
minimize this error. 
Averaged over all possible stiulus 
choices that are consistent with our 
response R. 
So now, we need to choose a form for this 
error function, a very natural choice. 
Is the main square error. 
We'll take L to be just the mean squared 
difference between our estimator and the 
true stimulus squared. 
Now to derive an expression for S bayes 
that solves this problem, we need to 
minimize the average error. 
So remember how we minimize a function. 
We take the derivative of that function 
which respect to the parameter that we're 
interested in. 
So here as base and we set as equal to 0. 
So lets just do that calculation. 
So now we want to take d by ds, this call 
SB integral DS. 
Now let's substitute in our expression 
for our error squared. 
Probability of s given i. 
So now we take the derivative of that 
with respect to s b. 
So that's going to be equal to interval d 
s. 
The only time that depends on SB is this 
one, so the derivative of a square is 
just S minus SB times 2 times the 
probability of S given R. 
And now we set that equal to zero. 
So hopefully you can see that the 
solution is of this form. 
So how did we get that, let's just write 
that out. 
So we have integral DS, we can separate 
these two terms out and put them on two 
sides, so and S P of S given R, is going 
to be equal to integral ds, s Bayes, 
probability of s given r. 
And now, if we integrate the probability 
of s given r, over s, sB here is just a 
constant and will come out. 
Now, the integral over this probability 
distribution, since the probability 
distribution is normalized, it's just 
going to be equal to sB. 
And so here's our solution. 
So, we have sB is equal to this 
expression, which we already, already 
have here. 
Now, I want you to take a look at that 
for a moment and see if you recognize it. 
So, what if our response is just a single 
spike? 
So what does this expression amount to? 
Well it does the spike triggered average, 
right, it's the stimulus triggered by the 
response from the spikes. 
So we're going to take all the stimuli, 
wait them by this probability that they 
occurred in response to a spike and 
average over them all. 
So how do we apply this to reconstructing 
a simple stimulus? 
So imagine that this is our spike 
triggered average. 
So now every time there's a spike, so 
that our measured spike train, we're 
going to paste in the spike trigger that 
our, our conditional average. 
So at low firing rates, this is not 
looking very good. 
But at higher firing rates, so now you 
see that we're getting closer and closer 
to a, a smoothly bearing function. 
You might've realized already that there 
are some issues with a filter or feature 
of, of this exponential form as we drew 
before. 
Which is that it can never capture a 
negative fluctuation in the input. 
This is actually an issue with the fly 
neuron data that you've looked at in the 
problem set. 
The fly has two h1 neurons. 
One that encodes leftward motion and 
another that encodes rightward motion. 
So if you've tried to construct a, 
reconstruct a velocity stimulus with only 
one of your H one neurons, you'll only 
ever be able to recover either leftward 
or rightward motions. 
In the book Spikes which is a very nice 
exposition of this kind of 
reconstruction, at considerably more 
depth than I can give here, the authors 
actually simulate the other H1 neuron by 
playing the original stimulus, but with 
the opposite sign. 
And that now gives us enough information 
to reconstruct both positive and negative 
inputs. 
So now let's see this kind of decoding in 
action. 
The movie you're about to see is based on 
the activity of multiple neurons in the 
lateral denucleate nucleus of the cat. 
This is work by Yang Dan of Berkeley done 
about 15 years ago when she was a PhD 
student. 
By convolving the spike trains from 
multiple neurons in LGN with the spatial 
temporal receptive fields of those 
neurons Activity allows a noisy, but 
comprehensible reconstruction of the 
scene. 
So in this case, it's a cat being 
recorded now from, while, while 
anesthetized. 
But the LGN neurons are giving a pretty 
good reconstruction of what the cat is 
looking at. 
Hopefully you can see Yang's advisor 
looming into view. 
That's Joe Attic whose work applying 
information theory, to understand 
receptor field structure, is coming up 
next week. 
Now let's forward a few years to 2011. 
I'm going to finish up this week lecture 
with a rather impressive example of 
decoding, that starts to get us closer to 
that mind reading fantasy. 
It also neatly brings together the idea 
we covered last week and this week. 
In this set of experiments, Jack Gallant 
also at Berkeley and his colleagues 
recorded from visual cortex of humans 
viewing movies using FMRI. 
And used the recordings to reconstruct 
one second long movie sequences. 
So here you're seeing reconstructions of 
single scenes. 
But these are in fact stills from one 
second long movies. 
Nonetheless I hope you get a sense of how 
impressive these reconstructions are. 
So how did they do this? 
So here's the basic idea. 
Going back to this model that we've used 
over and over again. 
The researchers here are trying to find a 
movie clip s that maximizes this a 
posteriori distribution. 
So they use a library of 18 million clips 
and take the prior p of s to be uniform 
across those samples. 
So what's missing is the likelihood. 
To compute the likelihood of a given clip 
from the database, they develop an 
encoding model that they fitted from a 
different training set of movies, so that 
they can evaluate the predicted response 
for an arbitrary input. 
Then they can evaluate this likelihood 
measure by computing how well the 
predicted response to a movie from the 
library matches the true response. 
Let's take a peek at the encoding model, 
as it uses several of the ideas that we 
developed last week. 
So here's the model that predict 
responses. 
As you might recall, we mentioned last 
week that fMRI relies on blood 
oxygenation or, or BOLD signals, so it 
has a slower response time than in 
neuro-, than neural activity. 
So in this model the neural response 
despite is separated from the bold signal 
separately fit how is the neural response 
model. 
Let's zoom in, its predicted as in the 
models of last week by first filtering 
the input. 
Here's a couple of different filtering 
stages to filter the input to extract 
certain features. 
Let's focus on this part in which the 
image is filtered through a pair of 
oriented filters here. 
At different phases, just as we described 
last week for complex cell responses in 
V1. 
Now the outputs of those two filters are 
squared and summed. 
This means that one gets a large response 
independent of spatial phase, as we also 
mentioned last week. 
Then the output of that filtering stage 
Is passed through a compressive 
non-linearity. 
In this case the function is taken to be 
a log function. 
And then this is temporally down sampled. 
That is, it's smoothed from a 15 hertz 
signal to a one hertz signal in order to 
reduce noise. 
That's taken to be the predicted neural 
response. 
This newer response is then passed 
through an additional filter that 
accounts for the slow response of the, of 
the blood oxygenation level. 
So now here's the full procedure. 
An encoding model, like we just saw, is 
fitted for each voxel, each volume unit 
in the brain region being image/g. 
And then that model is used to predict 
the response to the millions of images in 
the database. 
The stimuli with the highest likelihood, 
which in this case, is equivalent to 
those with the highest poterior and those 
best account for the predicted respionse. 
So here are the predicted responses, in 
this column. 
The map solution, the most likely 
solution or the highest a posteriori 
solution would be to simply read off the 
maximum value. 
Because the clips are full of highly 
specific detail, one can in this case do 
a lot better by averaging those out. 
So what they're going to do is to rank 
these images by the degree to which their 
pricted, predicted responses fit the true 
response. 
And take the top sequence of images that 
have the highest match. 
So here they're drawing the top 30 
highest posterior clips. 
So the, the 30 clips that have the 
highest degree of match to the predicted 
response. 
So one could simply take this best value 
but because of all of that, because of 
all the specific detail in these sample 
images from there prior, one doesn't look 
better by combining them. 
So now if you look at the cumulative 
average of many of these high probability 
clips, then what you see is that as one 
gets To larger and larger numbers of 
them, you're getting quite a good match. 
So remember this is one of these 
examples. 
Perhaps this one. 
So in this case, you can see the effect 
of that averaging. 
So now you no longer see a crisp a crisp 
image that you would get from a single 
choice. 
From your prior distribution. 
Instead, you average over many of them. 
But now, what that does is to remove 
specific features, and give you a general 
gestalt that's much more similar to to 
the stimulus that was presented. 
So I hope this demonstrates that we are 
within reach of that dream. 
That we will be able to look at neural 
activity, and using clever models of the 
type that I showed you just now, we'll be 
able to reconstruct naturlistic images 
from that neural activity So that brings 
us to the end of my lecture for this 
week. 
You'll also find online a special guest 
lecture by my colleague Fred Rieke, a 
world-acknowledged wizard of retinal 
processing. 
Next week we'll be moving on to a 
consideration of information: how is 
information defined? 
What exactly does it quantify, and how 
can it be useful in neuroscience? 
I hope you've enjoyed this week and that 
we'll see you back next week. 

Welcome to our first guest lecture in 
Computational Neuroscience. 
We're honored to have with us today 
Federiki, my colleague in the department 
of physiology and biophysics at the 
University of Washington. 
Fred was an undergraduate and graduate 
student at the University of California 
in Berkeley. 
His graduate work [UNKNOWN] focused on 
building theoretical studies of signal 
processing in the nervous system. 
Fred then went on to a post-doc with Eric 
Schwartz at the University of Chicago, 
working on mechanisms of synoptic 
transmission in the retina. 
He then did a second post-doc with Dennis 
Baylor at Stanford, where he worked on 
light transduction in photo-receptors. 
So, Fred is truly remarkable in having 
made a transition from truly elegant 
theoretical work. 
Which led to the publication of what's 
been a highly influential book, Spikes, 
to truly elegant experimental work. 
The creativity and excellence of Fred's 
work has lead to his recognition as an 
investigator at the Howard Hughes Medical 
Institute. 
In his work, Fred combines a mastery of 
technique with a beautiful clarity of 
thought. 
And we're delighted to give you this 
opportunity to hear from him a little bit 
about his research. 
 >> Thanks for the introduction. 
My lecture today will be about vision and 
starlight and the mechanisms that let us 
see under these conditions. 
We know, from a long history of 
behavioral measurements, that our ability 
to see under these conditions is limited 
more by the division of light into 
discreet photons. 
The physical nature of light itself, than 
it is by biological noise and 
inefficiencies. 
As we'll see in a minute, that raises 
some general computational issues. 
First, let me introduce the retina 
itself, which is where the visual process 
begins. 
This first slide is an EM picture of a 
piece of frog retina. 
The photoreceptor outer segments where 
light is transduced or converted into an 
electrical response are here on top. 
These long cylinders are the rod 
photoreceptors which are exquisitely 
sensitive to dim light. 
In between, here, here, and here are the 
cone photo receptor which mediate vision 
up at higher light levels. 
Signals produced in rod and cone outer 
segments, are processed by several layers 
of cells within the retina before they're 
passed on to the optic nerve in the 
brain. 
And we know from more than century 
behavioral measurements that the darkened 
opted visual system and hence the retina 
itself. 
Can detect the absorption of just a few 
photons, spread across a large array of 
rod photo receptors. 
So a few photons coming and being 
absorbed within a pool of several hundred 
or up 1,000 rods, generate electrical 
signals that are not only reliably 
detected by the retina. 
But they are reliably sent to the brain 
where they can give rise to perception. 
This raises a general computational 
issue. 
Which is how to detect sparse signals in 
an array of noisy detectors? 
So, the situation we're thinking about is 
a few rods out a of a pool of a thousand 
absorb photons, all of the rods are 
generating noise. 
We want to know how to pool signals 
across those rods to reliably extract 
signals from those rods that absorbed 
photons. 
Those sparse signals can reject noise 
from the remaining rods. 
This is a situation where averaging, you 
normally think about averaging as being a 
good strategy to extract weak signals. 
Under these circumstances, averaging is a 
disaster. 
That's because the signal is not 
uniformly spread across the array of 
detectors. 
It's a little bit like a situation where 
you're in a football stadium. 
There's a 1000 people yelling of you, you 
care about a few of them. 
Under those circumstances, a good 
strategy for extracting signals from 
those few people you care about would not 
be to stick a microphone at the 50 yard 
line. 
And average across everybody, all those 
sources of sound in the stadium. 
Instead you would need to go seat by seat 
and make a selection, is this likely to 
be the person who I care about, based on 
some prior information about them. 
Say the, the people you care about gotta 
be a little bit louder than average. 
So you go seat by seat, make a selection 
about which people to retain, which 
people to reject and then average those 
resulting signals. 
So, looking for something analogous here. 
That is, we're looking for some kind of 
threshold, which selectively retains 
signals from those rather likely to be 
absorbing photons. 
And rejects noise from the remaining from 
the remaining rods. 
The behavioral consequences for getting 
this right are fairly extreme, that's 
because the signal is so sparse. 
If we can reject noise from the 99.9% of 
rods that are just generating noise and 
selectively retain signals from the on 
order of .1% of the rods, that absorb 
photons. 
We stand to win considerably. 
Okay. 
So, what we're looking for then is some 
kind of thresholding non linearity, which 
retains signals from those rods that 
absorb photons and rejects noise from the 
remaining rods. 
They mention this as a general issue, 
it's one that comes up in many other 
cases in the nervous system. 
Cases in which you have convergence of 
many inputs onto a downstream cell and a 
small subset of those inputs are active 
while all of the inputs are generating 
noise. 
There are a number of conceptual and 
technical advantages for studying this 
issue in the context of photon detection 
in the retina. 
One of those is that we have access to 
the signal and noise properties of the 
rod photo receptors, so we can measure 
the responses of the rods to single 
photons. 
We can measure the noise and the rod 
responses and we can summarize those by 
constructing distributions, that capture 
the probability that the rod generates a 
given amplitude response. 
We can plot that as probability versus 
amplitude. 
For those rods in black that failed to 
absorb a photon and are just generating 
noise. 
And those rods in red that absorbed a 
photon and are generating single photon 
response. 
Those give us the basis for making 
theoretical prediction about how to make 
a selection between signal and noise. 
Particularly, we might think that this 
threshold in non-linearity should come 
in, and slice out and eliminate responses 
from those rods that are generating 
noise. 
And retain responses from those rods that 
generate single photon responses. 
It's nice because we have a theoretical 
basis for what an appropriate readout 
might be, for the rod array under these 
conditions. 
We also know a great deal from the 
anatomy about where such a thresholding 
non-linearity might be implemented. 
In particular, the rod signals traverse 
the retina through a specialized circuit. 
The first cell in that circuit is known 
as a rod bipolar cell. 
And rod bipolar cells receive input from 
multiple rods. 
So, that means that they have already 
combined signals from multiple rods. 
If they do so in a linear fashion, in 
other words they equally weight inputs 
from rods that are generating noise and 
signal. 
You've already begun to average the rod 
responses and you've begun to 
inextricably mix signal and noise. 
So, the last opportunity, where we have 
access to the responsive individual rod 
photoreceptors. 
We have the full capability of making a 
selection of those rods that are 
absorbing photon, and generating single 
proton response. 
Versus signals or noise from those rods 
that fail to absorb a photon. 
Is here at the synap between the rods and 
the rod bipolar cell. 
So, anatomically we have a good 
prediction about where such a 
thresholding non-linearity might occur. 
It should occur at the synapse between 
rods and rod bipolar cells. 
Indeed if we record from rod bipolar 
cells, we see evidence for such a 
thresholding non-linearity. 
We can now take the measured distribution 
of rod signal and noise, and ask what the 
appropriate non-linearity is to predict 
the bipolar responses. 
I summarize that here. 
So, again these, this is plotting 
probability versus the amplitude, the 
distribution of rods that are generating 
noise responses. 
And the distribution of responses from 
rods that absorbed the protons. 
On the same scale, I plotted the 
estimated non-linearity at the synapse 
between the rods and the rod bipolar 
cells. 
That's here in blue. 
I plotted gain of the non-linearity 
versus the amplitude. 
So, you think of this non-linearity as 
everything to the left get's eliminated. 
So, all this noise and a good chunk of 
the single proton response distribution 
gets eliminated. 
And only those responses that are to the 
right of this transition from the 
non-linearity between the gain of 0 and 1 
are retrained. 
So, we see evidence for non linear 
threshold between rod and rod-bipolar 
cells, it's kind of what predicted. 
Interesting thing here is, that we not 
have predicted the location of this non 
linearity. 
In particular, it is located well up into 
the single photon response distribution. 
Naively we might look at these and say, 
well I should really put a line here, 
right at the crossing point between the 
noise distribution and the signal 
distribution. 
That would be choosing a location for 
this threshold in non-linearity, which 
makes a decision based on the likelihood 
of the given amplitude response. 
If the amplitude of the response is, is 
if a given amplitude response is more 
likely to have arisen from this 
distribution of single photon responses, 
we would retain it. 
If it's more likely to have risen, arisen 
from the noise distribution, we would 
eliminate it. 
Instead this thresholding non-linearity 
seems to be pushed off to the right. 
In other words, we're eliminating many 
single photon responses, which seems like 
exactly the opposite of what we'd like to 
do, to build a system that operates on 
low light levels. 
However, this particular way of plotting 
the data, is somewhat misleading. 
An what we've not accounted for here, is 
the prior probability that the rod 
absorbs a photon. 
You can think about that as there is some 
area under this curve, the noise curve 
which represents the likelihood if the 
rod is generating noise. 
There is some area here under the single 
proton response distribution curve, which 
is the likelihood of probability the rod 
absorbs a photo. 
These are roughly equal in area the way 
I've depicted them. 
But I told you that vision is working 
under condition where 99.9% of the rods 
are generating noise and only about 0.1% 
or less are generating single photo 
responses. 
So, we really want to scale these areas 
to represent that prior probability of 
absorbing a photon. 
In the next slide I've re-plotted these 
distributions to take into account those 
prior probability. 
So, here I"m not plotting log of the 
probability versus amplitude, and then 
again, the gain of our non-linearity 
versus the amplitude. 
These are exactly the same distribution 
that you say in the previous slide. 
So, the distribution of responses from 
those rods that are generating noise. 
The distribution of responses of those 
rods that absorbed a photon and are 
generating a single photon response. 
This is at light levels which are 
producing on average about one absorbed 
photons per rod. 
So, the area under this noise 
distribution is similar to the area under 
the distribution of signal proton 
responses. 
And again the [INAUDIBLE] thing is that 
the non-linearity that we estimate is 
occurring between the rod and rod-bipolar 
synapse. 
Here in blue, is shifted far to the right 
of the crossing point of the signal and 
noise distributions. 
We seem to be throwing away many too many 
single photon responses. 
However we really want to think about 
these distributions as what would happen 
near visual threshold, when something 
like one in ten thousand rods absorb a 
photon. 
And that's what I've plotted over here. 
So, now the area underneath the noise 
distribution and the signal distribution 
had been scaled to represent this prior 
probability that something like 1 in 
10,000 robs absorbs a photon. 
So, the area under the signal 
distribution here is 10,000 times smaller 
than the area under the noise 
distribution. 
That shifts this crossing point of the 
signal and noise distribution far out to 
the right. 
And it shifts it to a point that's very 
close to the location of the transition 
of the non-linearity between a gain of 0 
and a gain of 1. 
In other words, if you're simply applying 
a rule like maximum likelihood, you get a 
given amplitude response from the rod. 
And you're going to associate that with 
noise, if that amplitude is more likely 
to have originated from the noise 
distribution. 
You're going to associate it with signal 
if that amplitude is more likely to have 
originated from the signal distribution. 
That simple rule can predict the position 
of this nonlinearity and predict in 
particular that you should throw away 
many single photon responses. 
You should do that because the cost of 
accepting those amplitudes, down in here, 
is to allow lots of noise to come through 
the system. 
Much more noise than you would like to 
allow to come through. 
So, the basic bottom line here is the 
nice example in which the prior 
probability has an important impact on 
how we think about signal detection 
theory working. 
Now we think about appropriate strategies 
for extracting sparse signals from many 
noisy inputs. 
This is one of the many challenges that 
we face in understanding how vision works 
at low light levels. 
That includes issues about the 
photo-transduction process, how rods 
themselves generate responses to single 
photons. 
I've emphasized this role in synaptic 
transmission that is what the 
opportunities are. 
And some of the general computational 
challenges are facing the retinal 
circuits that read out the rod signals. 
And there are also challenges that we 
face, in neural coding and understanding 
how the responses generated by the 
absorption of a few photons. 
Produce a reliable change in the output 
of the retina that can be interpreted by 
the brain. 
Thank you. 

[MUSIC]. 
Hello and welcome back to week four of 
computational neuroscience. 
This week we will be talking about 
information theory. 
We'll be exploring information theory as 
a way to evaluate the coding properties 
of a neural system. 
So going back to thinking about spiking 
output as binary strings, of 0s and 1s. 
How good a code do these spike trains 
represent? 
We'll explore using information theory 
and related ideas as a way to understand 
how the coding properties, of our nervous 
system might be specially structured to 
accommodate the complex structure of the 
natural environment. 
So today we'll be addressing three 
things, we're going to start by talking 
about entropy and information, defining 
our terms. 
Then we're going to talk about how to 
compute information in neural spike 
trains, and then finally we will explore 
how information can tell us about coding. 
So, let's go back to our well worn 
paradigm, a monkey choosing right from 
left. 
And suppose we're watching the output of 
a neuron while different stimuli 
appearing on the a screen. 
Here's an example, spike train, is the 
time sequence in which we're marking 
spikes, in a given time bin, with a one, 
and a silence with nothing. 
Now, here's another example. 
And another. 
So, hopefully, when these oddball symbols 
appeared, either stimulus or spike, you 
felt a tiny bit of surprise. 
So, information quantifies that degree of 
surprise. 
Let's say there was some overall probably 
p, that there's a spike in sometime bin. 
And 1 minus P but there's silence, then 
the surprise for seeing a spike is 
defined as minus log 2, so log based 2, 
of that probably, that's the information 
that we get from seeing a spike. 
And the information that we get from 
seeing silence is minus log 2 of 1 minus 
p, of the probability of seeing the 
silence. 
So why does the information have this 
form? 
Like my husband and I some of you 
probably play squash. 
And if you do you'll know that what 
you're trying to do is put the ball 
somewhere that will surprise your 
partner. 
If you're a remarkable player you can put 
the ball anywhere in the court. 
If your partner has one bit of 
information, he knows which half of the 
court the ball is in. 
There was an equal probability of being 
in either, but once he gets that bit he 
knows which half. 
Each additional bit of information cuts 
the possibilities down, by an additional 
factor of two. 
So, what we're really doing, is 
multiplying the probability, the 
probability of being in this half, is p 
equal one half. 
The probability of being in the front 
half of the court, is an additional one 
half. 
Taking the negative log base 2 turns this 
into 1 plus 1 two bits to specify being 
in the front left corner. 
So now that we have a sense of that 
information we can understand entropy. 
Entropy is simply the average information 
of a random variable. 
So entropy measures variability. 
I'll warn you right now that in the 
future, I'll usually drop this, this base 
2 on the log, and just assume it. 
Entropies are always computed in log base 
2 and their units are in bits. 
An intuitive way to think about this, is 
that the entropy counts the number of yes 
no questions, as we saw in the case of 
the squash game, that it takes to specify 
a variable. 
So here's another example, let's say I 
drive down from Seattle to Malibu, and 
park in the valet parking. 
When I come back to get my car, the car 
park attendant is not very helpful and 
won't tell me where my car is. 
He'll only grunt for yes answers. 
So the car could be in any of these, say, 
eight spots. 
How many questions will it take before I 
can find it? 
So, let's say, is it on the left? 
Grunt. 
Is it on the top? 
Grunt. 
Is it on the top left? 
Grunt. 
So, what's the entropy of this 
distribution? 
Let's, let's calculate it. 
So, remember we defined the entropy. 
I'll call H. 
As the sum over the probabilities times 
log of the probability minus. 
So what is pi? 
In this case, the probability of being in 
any one of these locations is 1 8th. 
And that's the same for every location in 
this. 
In this, car park. 
And so now H is equal to minus 1/8, sum 
from i equals 1 to 8, log base 2 of 1/8. 
Now what is that? 
Remember that 8 equals 2 to the power of 
3, so the log base 2 of 8 is 3. 
So here we have, now, sum of 1 8th times 
minus 3. 
Now we add that up over the eight 
possibilities and we get 3. 
So as we saw, it took three questions to 
specify our car, and that's exactly the 
entropy of this distribution. 
So now let's go back to our coding 
sequences, here's a few different 
examples, so which of these do you think 
has the most intrinsic capability of 
encoding? 
Encoding relies on the ability to 
generate stimulus driven variations in 
the output. 
If an output has no variation, such as in 
this case. 
We're not very optimistic about its 
ability to encode inputs. 
So these three sequences differ in their 
variability. 
Which do you think has the most inherent 
coding capacity? 
So we can use the entropy to quantify 
that variability. 
So what does having a large entropy do 
for a code? 
It gives the most possibility for 
representing inputs. 
The more intrinsic variability there is, 
the more capacity that code has for 
representation. 
So in this simple case, we can compute 
the entropy as a function of the 
probability p. 
Where, again, the other, the other 
possibility has probability 1 minus p. 
So entropy, again, is going to be given 
by a minus p log p minus 1 minus p, log 1 
minus p. 
So now when one puts that function as a 
function of P of r plus, which here would 
call p, we find that there is a maximum. 
So, what's the value of P at which H has 
a maximum. 
That's the value at which. 
P equals one half. 
In that case, in this distribution, these 
two symbols are used equally often. 
So, this is a concept we'll come back to 
at the end of this lecture. 
Let's go back to squash. 
So, we had a possibility of the ball 
being anywhere in the field. 
Generally, you're not able to put the 
ball anywhere with equal probability. 
It's exactly this reduction in 
possibility that makes it even possible 
to play. 
You could model your opponent's 
probability of x, the probability of 
placing the ball somewhere in the court, 
and you can, to some extent, predict 
where the ball is. 
The lower the entropy of your partner's p 
of x, the more easily you'll defeat him. 
So, let's come back finally to our spike 
code. 
We now appreciate that the entropy tells 
us of the intrinsic variability of our 
outputs, by obviously we really need to 
consider the stimulus, and how it's 
driving those responses. 
So here's an example. 
The stimulus can take one of two 
directions, and each is perfectly encoded 
by either a spike or no spike. 
So here's the stimulus, here's the, the 
spiking response. 
Every time there's a rightward stimulus, 
we get a spike. 
So how about this case? 
We'd probably still be comfortable to say 
that the response is encoding the 
stimulus. 
These two are perfectly correlated. 
On the other hand, there are, there are 
several other events that, that are 
misfires. 
So in this case, the stimulus occurred 
with no spike, in this case there was a 
spike with no stimulus. 
But how about this? 
At least at a glance, there seems to be 
little or no relationship between the 
responses and the stimulus. 
So just as a side bar, what if the 
problem were not so much that our code 
were noisy, but that we haven't exactly 
understood what the code is doing. 
That is, maybe there's some temporal 
sequencing S that should be more 
appropriately thought about as the true 
stimulus. 
This is really the question that we were 
addressing in week two. 
How do we know what our stimulus was? 
But let's go back to the main question. 
What we really wanted to know is; how 
much of the variability that we see here 
in R is actually used for encoding S? 
We need to incorporate the possibility 
for error. 
So, let's do that by assuming now that 
was, when a spike is generally produced 
in response to stimulus plus. 
So here, there's also some possibility 
that there will be no spike, we'll 
quantify that using the error probability 
q. 
So probability of, of correct response in 
this case is 1 minus q, and the 
probability of a incorrect response is q. 
And let's assume the same error in this 
case for a silence response. 
So, now we would like to know, how much 
of the entropy of our responses is 
accounted for by noise, by these errors. 
Because that's going to reduce the 
responses capacity to encode S. 
The way we can address that is to compute 
how much of the response entropy can be 
assigned to the noise. 
That is if we can give a stimulus plus, a 
plus stimulus, a right way stimulus and 
get a variety of responses. 
Those conditional responses for a fixed S 
have some entropy of their own. 
Similarly when we give stimulus minus. 
So we call these stimulus driven 
entropies, the noise entropy. 
So this brings us to the definition of 
the mutual information, the amount of 
information that the response carries 
about the stimulus. 
This is given by the total entropy minus 
the average noise entropy. 
That is, the amount of entropy that the 
responses r have of some fixed s, 
averaged over s, and that's drawn out 
here. 
So, here's the total entropy of the 
responses, and here's the conditional 
entropy. 
So the, the entropy of the responses 
conditioned on a particular stimulus s, 
averaged over s. 
So now let's go back to our binomial 
calculations, and see how the mutual 
information depends on the noise. 
Now fixing p. 
We're going to take p to be the one that 
maximizes the entropy, so p equals one 
half. 
Let's vary the noise probability, and 
again assume that the noise is the same 
for spike and silence. 
That is, there is one value q. 
So this should be intuitive. 
When there's no noise entropy the 
information is just the entropy of the 
response, which in this case is one bit. 
As the error rate increases, as the error 
probability grows larger and larger. 
Spiking is less and less likely to 
actually represent the stimulus S, and 
the mutual information decreases. 
When the error probability reaches a 
half, that is, responses occur at chance, 
there's no mutual information between R 
and S. 
So, let's just check that everyone's 
still on board. 
More generally, what are the limits? 
So, if the response is unrelated to the 
stimulus, what is the probability of 
argument S? 
Its simply, the probability of the 
response. 
Because there's no relationship between 
response and stimulus. 
So the noise entropy is equal to the 
total entropy, and then the difference of 
response in noise entropy is zero. 
At the opposite extreme, the response is 
perfectly predicted by the stimulus. 
So in this case the noise entropy is 
zero. 
So the mutual information will be given 
by the total entropy of the response. 
All of the response's coding capacity is 
used in encoding the stimulus. 
So let's just see how that works for 
continuous variables. 
We've talked a lot about, about binary 
choices. 
Let's think more generally about cases 
where we have some continuous r, and, 
some response variability for the 
encoding of a stimulus s by r. 
So here's an example where we've given 
several different stimuli. 
Each of these distributions is the 
probability of the response given a 
particular trice of the stimulus. 
And now that's going to be weighted by 
the probability of that stimulus. 
And when we add all of these conditional 
distributions together, we get the full 
probability, P of r. 
Now, what we're doing by computing the 
entropy is we're going to compute the 
entropy of this blue distribution. 
That's going to be the, that's going to 
give us the total entropy. 
And then we're going to compute the 
entropy of these conditional 
distributions. 
And now we're going to average them over 
the stimulus that drove them. 
So for these two cases, they differ by 
the amount of intrinsic noise that each 
response has. 
So we give a stimulus s in this case, 
there's some range of variability that 
takes out some of my range of r. 
In this case, when we give that same 
stimulus, now the degree of noise 
stretches over a much wider range of the 
response distribution. 
So much more of the variability in R is 
accounted for by variability in responses 
to specific stimuli. 
And so, I hope you can see that this kind 
of response, set of response 
distributions, is going to encode much 
more information about S, that the 
information about S and R is much larger 
in this case. 
Then it is for this case. 
Let's play a little bit with these 
distributions, because I want to 
demonstrate a couple of things that I 
think really illustrate why information 
is useful and an intuitive measure of the 
relationship between two variables. 
I'm using capital letters to denote the 
random variable, and lower case letters 
to denote a specific sample from that 
random variable. 
So, what I'd like to show you is that the 
information quantifies how far from 
independent these two random variables R 
and S are. 
To demonstrate that, I'm going to use the 
KullbackLeibler divergence; a measure of 
similarity between probability 
distributions that we introduced earlier. 
It's the mutual information, measures 
independence, then we'd like to quantify 
the difference between the joint 
distribution, of R and S, and the 
distribution, these two variables would 
have if they were independent. 
That is, that that, that joint 
distribution would simply just be the 
product of their marginal distributions. 
So, first, to refresh your memory, D KL, 
let's redefine it. 
D KL, between two different probability 
distributions, say P and Q, is equal to 
an integral. 
Over probability of x times the log of P 
of x over Q of x. 
So now let's apply that to these two 
distributions. 
So, let's compute that, we have a 
integral over ds and over dr. 
Joint distribution, times the log the 
joint distribution divided by the 
marginal distributions. 
Now we can rewrite that, using the 
conditional distribution. 
In the following form, we can rewrite 
that as the probability of r, given s 
times the probability of s, that's just 
equivalent to the joint distribution, 
divided by P of r, P of s. 
And now, you can see that P of s cancels 
out, and we can rewrite this as, now the 
difference of those two distributions. 
So we'll just expand that log. 
All right. 
Now let's concentrate on this term. 
Going to be equal to the negative ds dr, 
probability of s and r, times the log of 
P of r, plus integral ds dr, P. 
Now, let's break that up into P of s, P 
of r given s. 
Just dividing up the, the joint 
distribution again into its conditional 
and marginal. 
Times the log of P of r given s. 
Now, let's look at the terms that we've 
developed here. 
We can see that we can just integrate 
over ds. 
We can integrate the s part out of this 
joint distribution. 
And this part is just simply going to be 
the entropy of P of r. 
Whereas this one is going to be the 
entropy of P of r given s, averaged over 
s, ds P of s. 
And so what I've shown you is that this 
form, in terms of the KullbackLeibler 
divergence, gives us back the form that 
we've already seen. 
The entropy of the responses minus the 
average, minus the average over, over the 
stimuli. 
Of the noise entropy, for a given 
stimulus. 
What I hope you realize is that 
everything we've done here in terms of 
response and stimulus we could simply 
flip, response and stimulus, redo the 
same calculation, and instead end up with 
entropy of the stimulus minus an average 
over the responses, of the entropy, of 
the stimulus given the response. 
So information is completely symmetric, 
in the two variables, being computed 
between. 
Mutual information between response to 
stimulus is the same as mutual 
information, between stimulus and 
response. 
So here's our grandma's famous mutual 
information recipe. 
What we're going to do to compute this 
mutual information, is to take a 
stimulus, s, repeat it many times, and 
that will give us the probability 
responses given s. 
We're going to compute the variability 
due to the noise. 
That is, we'll compute the noise entropy, 
of, of these responses. 
So, for a given value of s, we'll compute 
its, the entropy of the responses for 
that s, we'll repeat this for all s. 
And then, average over s. 
Finally, we'll compute the, probability 
of the responses, that'll just be given 
by the average over all the stimuli that 
we presented, times the probability of 
the response given the stimulus, and that 
will give us the total entropy of the 
responses. 
So, in the next section, we'll be 
applying that idea to calculating 
information in spike trains. 
There'll two methods that, that we work 
with. 
One will be starting by calculating 
information in spike patterns. 
And then we'll be calculating information 
in single spikes. 

Hello, again. 
So now were are moving on to calculating 
information in spike trains. 
And in this section of the lecture, we're 
going to be talking about two methods one 
of which is how to compute information in 
spike patterns. 
And the other is how to compute 
information in single spikes. 
So let's go back to our, our grandma's 
information recipe. 
So remember that we're calculating the 
mutual information, which is the 
difference between the total response 
entropy and the mean noise entropy. 
So, what was the strategy, we're going 
to, test strategy. 
We're going to take a single stimulus S, 
repeat it many times to obtain the 
probability of the responses given S, in 
that response distribution, via the noise 
entropy. 
We're going to repeat that for all s, and 
then average it over s. 
Finally, we'll compute the probability of 
response, and from that the total 
response entropy. 
So now, let's go ahead and compute 
information in spike patterns. 
So far we've really only dealt with 
single spikes or firing rates, so what 
we'd like to ask here is, what 
information is carried by patterns of 
spikes? 
By these interesting sequences of 0s and 
1s that occur here in the code. 
What this allows us to do is to analyze. 
Patterns of the code and to ask how 
informative they are. 
So the way we're going to turn out our 
spike train into, into a pattern code, is 
that we're going to chop up segments of 
these responses, so we take our voltage 
train when we divided into time bins Of 
size delta t. 
If there's a spike in, in that time, then 
we'll put a one. 
If there's no spike, we'll put a zero. 
And now we'll chunk up these zeros and 
ones into words of some length, big T. 
So now that we, we've defined these 
binary words. 
With the letter size delta t and length 
of T, we can now walk through our data. 
So, so, here's a raster plot produced by 
a stimulus that was randomly chosen on 
every trial. 
And so, if one converts such a raster 
plot into sequences of zeros and ones, 
you can look through that and pull out 
many, many examples of these words, again 
of length T and type in delta t. 
So now, one can form a distribution over 
these words. 
So here, the most common word was 
silence, there was no spike in this set 
of eight consecutive time bins, the next 
most common was that one spike appeared 
and of course, we can have that one 
appearing at different locations 
throughout the word. 
These are the next most common set of 
words. 
Then one starts to get combinations of 
spikes occurring at different locations 
throughout the word. 
So now we can walk through our data and 
calculate these probabilities and then 
calculate the entropy of that word 
distribution. 
Now the information, is the difference 
between, that entropy, and the 
variability, due to noise, averaged over 
stimuli. 
So here was our total entropy. 
Here's how we're going to compute our 
noise entropy. 
So, in this case, the same stimulus was 
given every time, and now, what one sees, 
over many repetitions of that stimulus. 
Is that on the first trial, you see a 
word, zero, zero, one, zero, zero, zero, 
zero. 
On the next trial, you have the same 
word, but now you see that there are some 
times when there was no spike, and some 
times when that spike appeared in a 
different bin. 
What that's going to do is generate a 
distribution of different wads. 
Now that distribution is going to be 
considerably narrower than the total 
distribution. 
And it's exactly this reduction in the 
entropy from knowing nothing about the 
stimulus, to knowing something about the 
stimulus that information will be 
capturing. 
Alright, so let's go ahead and apply 
Grandma's recipe. 
We'll take a stimulus sequence and repeat 
it many times, by how we're sampling the, 
this probability of stimulus. 
We're going to use a bit of a trick, 
which is that instead of averaging over 
all possible stimuli, we're going to take 
a long random stimulus and average it 
over time. 
So, now, time is standing in, for the 
average over stimulus. 
So now, for each time, in the repeated 
stimulus, we're going to get a set of 
words, p of w, given stimulus at time t. 
And our noise entropy, our average noise 
entropy, is now going to be averaged over 
those different time points, i. 
So if we choose a length of repeated 
sequences long enough, that will allow us 
to sample the noise entropy adequately. 
So let's have a look at the application 
of this idea to data from the LGN in a 
classic paper by Pam Reinagel and Clay 
Reid. 
They carried out this exact procedure, so 
as you saw before they ran a random 
stimulus over many trials. 
Then they ran a fixed stimulus, call it 
frozen white noise, which has some 
structure, in fact here it is. 
It's the stimulus as a function of time 
and you can see that in response to the 
stimulus spikes appeared in a time lock 
sequence. 
And now for an averages across those 
repeats one finds a PSTH, that is a Post 
Stimulus Time Histogram, Where these 
events show these large modulations in 
the time varying fine rate produced in 
response to that stimulus. 
Now, if one zooms in on a tiny piece of 
these responses, you'll see something 
like this. 
So, at, at very Fine time scales. 
There's quite a bit of jitter in those 
responses. 
Now our goal in computing the 
information, and what the author has 
examined in this paper was ask on what 
time scale, do these responses continue 
to convey information about the stimulus? 
So one can see by looking at this picture 
that there's quite a bit of variability 
in the spike train, and so that defines 
some kind of window around which a spike 
can jitter and still signal the same 
information about the input. 
So the questions we'd like to understand 
is how finely do we have to bend our 
spike train and pay attention to the 
individual timings of spikes in order to 
extract all that the neural code has to 
tell us about the stimulus? 
So one can do that by exploring the 
information produced by the spike train 
as a function of these two parameters, as 
a function of delta t, the Binning time 
width and also of the length of the word, 
as the word gets longer our coding symbol 
is able to capture more and more of the 
correlations in the input. 
And so, to what extent does increasing L 
continue to capture more and more 
information about the stimulus. 
So here's what the authors found in the 
LGN, they varied both DT, both the 
temporal resolution of their words and 
the total word length. 
So, here drawn as a function of 1 over L. 
And have plotted here the information 
that they calculated for different 
choices of those parameters of the 
definition of the word. 
So, clearly, there's going to be a 
problem in going to this limit of very 
large word lengths. 
So, as the word gets longer an longer, 
for a finite amount of data, you're going 
to have very few samples of a word of 
that length. 
And so when one tries to estimate the 
entropy of the distribution of words of 
this length, it's very unlikely that you 
will have seen them all. 
And so not surprisingly, if you now look 
at the entropy, plotted as one over the 
word length The entropy drops off at this 
limit indicating that the information is 
not completely sampled. 
So what can be done is to compute the 
entropy for different lengths of words 
and you can see that these form almost a 
line. 
And so one can simply extrapolate the 
tendency of this line back toward 
infinite word length. 
And extract an estimated value for the 
entropy at that limit. 
That's not what was done in this figure 
this was purely the information directly 
captured. 
And so one can look over different delta 
t's and different word length to see how 
information depended on these parameters. 
So what you should notice is that there 
is some limit. 
To DT, beyond which the information 
doesn't grow anymore. 
As one looks at the woods in higher and 
higher temperol resolution. 
So one takes into account finer and finer 
details about how those spike patterns 
are generated. 
and so that's what's being quantified as 
we move down this axis. 
As the time discordization of the wood. 
These bin sizes, is getting smaller and 
smaller, that's able to capture more and 
more of the variability, in the spike 
train, that's actually signaling 
something different about the stimulus. 
But that at some point, it seems that 
that, information, stops increasing. 
So, this red, we're at about, you know, 
between 80 and 100 bits per second, is 
the information rate. 
And you see that that stops increasing 
with delta t, and of delta t of about 2 
milliseconds. 
So hopefully you'll remember from the 
jitter in the spike trance that we looked 
at, that they seem to be repeatable on a 
time scale of about a millisecond or 2 
milliseconds. 
So that time scale dt corresponds to the 
time scale in which the jitter in the 
spike train. 
Still allows one to read that off as an 
encoding of the same stimulus. 
It's going to quantify approximately 
what's the temporal with that one can 
discatize this spy train and still 
extract all the information about the 
stimulus that distinguishes it from other 
stimuli. 
So in this example we've seen one case 
where we didn't have enough data to be 
able to sample say very long words. 
In general this is always true. 
When one's trying to calculate 
information theoretic quantities, one 
needs to know the full distribution of 
responses, and the full distribution of 
stimuli. 
And there's simply never enough data to 
come up with really reliable estimates 
for information, unless one has very 
simple experimental setups. 
And so a lot of effort has been put into 
finding ways to correct the sample 
distributions for the fact that there is 
a finite amount of data. 
And there's been some very interesting 
work by a number of groups over the last 
15 years or so, that has made significant 
advances in being able to compute 
information theoretic quantities from 
finite amounts of data. 
Now we're going to turn to a different 
approach, this one proposed by [UNKNOWN] 
Brenner and [UNKNOWN]. 
How much does the observation of a single 
spike tell us about the stimulus? 
Now this is similar to the case that we 
started with at the beginning of this 
lecture, but now we're going to address 
the question that we noted then What if 
we don't know exactly what it is about 
the stimulus that triggered the spike. 
It turns out that, as in the case we just 
went through, is straightforward to 
compute information with an explicit 
knowledge of what exactly in the input is 
being encoded. 
This is because the mutual information 
allows us away to quantify the 
relationship between input and output 
without needing to make any particular 
model of that relationship relationship. 
So, the paradigm is exactly the same as 
before. 
We're going to compute the entropy of 
responses, when the stimulus is random, 
and the entropy, when given a specific 
stimulus. 
So, here, things are a little simpler, 
than in the case of Wuds/g, without 
knowing the stimulus, the probability 
that a single spike acud/g, is given by 
the average firing rate times the bin 
size. 
Similarly, the probability of no spike is 
just 1 minus that. 
Now the probability of a spike at a given 
time during the presentation of a 
stimulus r of t times the time then, when 
now r of t is the time varying rate 
caused by the changing stimulus We can 
get an estimate of that time varying rate 
by repeating the input over and over 
again. 
The variability in these responses means 
that these events show a continuous 
variation, and have some width as we saw 
before, depending on the jitter and the 
spike times. 
So let's go ahead and compute the 
entropy. 
We're going to define, for the moment, p 
equals r bar delta t and p of t to be r 
of t delta t. 
The information will simply be the 
difference between the total entropy, 
we've already computed that in the 
beginning of the lecture For, for this 
binomial case to minus p log p minus 1 
minus p log 1 minus p and we need to 
subtract from that the noise entropy. 
Now the noise entropy would take on a 
value at every time t depending on the 
time variant firing rate. 
Now again every time t represents a 
sample of stimulus S. 
And averaging over time is equivalent to 
averaging over the distribution of s. 
This ability to swap an average over the 
ensemble stimuli, for an average over 
time, is known as ergodicity. 
At different values of S are visited in 
time with the frequency that's equivalent 
to their probability. 
So now we have our expression for the 
information between response and 
stimulus, we can do some manipulations on 
it. 
So we're placing back P by R delta T. 
We can take the time average firing rate, 
to be equal, to the mean firing rate, so 
that's equivalent here to this, to the 
integral, over, the probability as a 
function of time, in the mean, going 
toward that main firing rate. 
And getting rid of some small terms, we 
have here a couple extra, extra pieces 
that turn out to be small, we end up with 
a rather neat expression for the 
information per spike. 
let's take a closer looks at this 
expression, as we've emphasized already 
This method of computing information has 
no explicit stimulus dependence. 
Meaning no need for any explicit coding 
or decoding model. 
It relies on the repeated part of the 
stimulus being a good estimate of the 
distribution of a possible stimuli. 
Note also that although we computed this 
for the arrival or not of a single spike, 
this formulism could be applied to the 
rate of any event. 
For example the occurrence of a specific 
symbol in the code. 
So this is a way to evaluate how much 
information might be conveyed by a 
particular pattern of spikes, for example 
a sudden inter spike interval. 
We can also examine what determines the 
amount of information in the spike train 
/g. 
So looking again at this expression, we 
can see that it's going to be determined 
by two things. 
One is timing precision. 
That's going to blur this function R of 
T. 
So if events are blurred so that R of T 
increases and decreases slowly, without 
reaching large values, this will reduce 
the information. 
At the extreme, let's imagine, that the 
response is barely modulated at all by 
this particular stimulas. 
In that case, r of t goes towards the 
average firing rate. 
And one gets no information. 
The more sharply and strongly modulated r 
of t is the more information it contains. 
The other factor is the main firing rate. 
If the spike rate is very low then the 
average firing rate is small and 
information is likely to be the large. 
The intuition is that the low firing rate 
signifies that the neuron response to a 
very small number of possible stimuli so 
that when it does spike its extremely 
informative about the stimulus. 
Note that this is the information per 
spike. 
The information transmitted is a function 
of time, for the information rate is 
going to be small for such a neuron. 
So let's look at some hypothetical 
examples. 
Rat hippocampal neurons have what's known 
as a place field such that when the rat 
runs through that region in space, the 
cell fires. 
Let's imagine the place cell looks like 
this. 
As the rat runs around the field, Is 
going to pass through that place field, 
and what's the firing rate going to look 
like? 
Here, as it moves through the field is 
going to go from zero, ramp up kind of 
slowly, go down again. 
Because that place field is quite large, 
the red is likely to pass through it 
farely often. 
So we're going to get some R of T of that 
form. 
Now let's imagine that the place field is 
very small. 
Now, rat runs around. 
Very, very rarely passes through that, 
that place field. 
And so, now, going to get almost no 
firing and then some blip of firing as it 
passes through that field. 
Now, what if the edges of the place fill 
the very shop? 
So now again rat runs around. 
Very, very rarely passes through that 
field, so now as the rat runs around, it 
passes through that place field very 
rarely, but when it does, the firing rate 
increases very sharply toward its 
maximum. 
So that's going to increase the 
information we get from such a receptor 
field. 
Okay, so now we're done with computing 
information in spike trains. 
Next up we'll be talking about 
information and coding efficiency. 
We'll be looking at natural stimuli. 
What are the challenges posed to our 
nervous systems by natural stimuli? 
What do information theoretic concepts 
suggest that neural systems should do 
when they encode such stimuli? 
And finally, what principles seem to be 
at work in shaping the neural code? 

In this section we'll be addressing how 
information theoretic ideas can help us 
to understand how the neural code may be 
specially adapted to the structure of 
natural signals. 
We'll briefly first look at some of the 
special properties of natural inputs. 
And then some theories of how code should 
behave. 
Finally we'll sum up with some 
suggestions from the principles that may 
be at work in shaping the neural code. 
So I'm going to show you some photos that 
we taken by one of our Post-Docs, Fred 
Sue, as he was sitting in his apartment 
on one of our typical sunny Seattle 
afternoons, looking out at the view. 
He tried to take a picture that both 
encompassed his beautifully furnished 
apartment and the grand view outside. 
You can see that he had to change his f 
stop over a wide range in order to be 
able to capture information both about 
the scene inside and about the world 
outside. 
Now this is something that our eye does 
effortlessly. 
If you were sitting here at this table, 
you would be able to see both the inside 
and the outside with perfect fidelity. 
So looking even at this familiar example, 
we can see two properties that are 
characteristic of natural inputs. 
One is that there's a huge dynamic range. 
There are variations in light level and 
contrast that range over orders of 
magnitude. 
We can see signs of another property by 
comparing these two boxes. 
Because of effects of depth and 
perspective, there's similar structure, 
similarly well defined shapes and objects 
at very different length scales. 
This is reflected in the power spectrum 
of natural images. 
If one computes the power in different 
spatial frequency components this 
function has a, this function has a power 
log form. 
That is it scales, like the frequency, to 
the power minus two. 
This reflects the lack of any 
characteristic scale. 
The similar structure are not. 
Despite these scale differences and the 
very large variations in light and 
contrast across the image, we'd like to 
be able to distinguish detail at every 
point in it. 
Unlike this camera. 
These basic issues arise for almost all 
of our senses. 
Here's an audio track of a chunk of 
speech. 
The signal is full of complex 
fluctuations that carry detailed 
information about pitch and nuance. 
However, these fast variations are 
modulated by the relatively huge 
variations in amplitude that make up the 
envelope of speech. 
We're perfectly capable of understanding 
all of these signal components regardless 
of the overall amplitude, even when there 
are multiple speakers, or they're far 
away. 
So how can a neural system, with a 
limited range of responses, manage to 
convey the relevant information about 
details in the face of these huge 
variations of scale? 
We found that the entropy, we found that 
the entropy, reached it's maximum, when 
it was of the form of these two symbols 
were used equally often. 
Now if we're thinking about maximizing 
the mutual information. 
We also have to take into account this 
noise term. 
But generally the amount of noise for a 
given stimulus may not be something 
that's easily controlled. 
While the total response entropy is 
something that's in the hands of the 
coder. 
Let's see how. 
Let's imagine that the stimulus that a 
system needs to encode Is varying in 
time, this is s of t, it has some 
distribution, p of s over here. 
Our job as an encoder is to map the 
stimulus onto the symbols that we have at 
our disposal. 
Let's imagine that we're constrained to 
use some maximal firing rate, so we have 
some limited range of possible symbols at 
our disposal, say zero to 20 hertz. 
How should we organize that mapping so 
that we end up with the most efficient 
code? 
We'll get the most information by 
maximizing our output entropy. 
That is, by using all of our symbols 
about equally often. 
So what does that imply for the shape of 
this curve? 
So what we should do is move along our 
stimulus distribution and encode equal 
shares of that distribution with each 
symbol. 
If we have 20 symbols lets count up 1 
20th of our total area under this curve, 
and assign that to symbol one. 
What this amounts to is a response curve 
that's given by the cumulative integral 
of the stimulus distribution. 
Another name for this is histogram 
equalization. 
So this implies that for a good coding 
system, its input output function, this 
function here, should be determined by 
the distribution of natural inputs. 
So here's a classic study in which this 
idea was tested directly. 
In the early 1980's, Simon Laughlin went 
out into the fields with a camera, and 
measured the typical contrasts, that is 
deviations in the light level, divided by 
the mean light level, that would be 
experienced in the natural world, for 
example, by a fly. 
So, that's this distribution here. 
If the response does indeed follow the 
distribution of natural inputs... 
Then the response curve, here, should 
look like the cumultive probability 
determined by integrating p of c. 
And in fact, that's a very good match to 
what he did actually observe in the 
response properties of the fly large 
mono-polar cells, the neurons that 
integrate signals from the fly's 
photo-receptors. 
Now, a study like this poses a challenge. 
While it makes sense that our sensory 
systems would, over evolution or 
development, set up response codes that 
are adjusted to natural input statistics. 
It seems that much more work is needed to 
handle the problems posed by this huge 
natural variation, that stimuli take as 
one moves from indoors to outdoors or 
even moves one's eyes around a room. 
The contrast distribution is varying 
widely. 
Might sensory systems rather adjust 
themselves on much shorter timescales to 
take these statistical variations into 
account. 
So let's take a patch of the image, and 
look at the, the variations in contrast 
in that image. 
Here for example, that contrast 
distribution might take, might be narrow 
like this. 
Wheras over here, it might be much 
broader. 
What our code should do is take the 
widths of these distributions into 
account in setting up a local. 
Input, output curve, that accommodates 
this structure of the, currently measured 
statistics of the input. 
So that's the question that we tested 
here, in the h1 neuron. 
In this experiment, we took a white-noise 
input, of the type that you used in the 
problem sets, so some s of t. 
Looks like that. 
And we multiplied it by some time 
varying, slowing time varying envelope. 
Call that sigma of t. 
And that's what you see here. 
So we repeated the same sigma of t. 
This is a 90 second long chunk of 
stimulus. 
Repeated the same sigma of t. 
In every trial, but we changed the 
specific white noise. 
Stimulus. 
And that allowed us to pick out spikes 
that occurred at different time points 
throughout this presentation of, of sigma 
of t, where in every trial the cell would 
have seen a different specific stimulus. 
And to calculate the input output 
function described by those spikes, in 
those different, in those different 
windows of time. 
So now one, when one analyzes spikes 
across these different windows, and pulls 
out their input output function using the 
methods that we talked about in week two, 
one finds that for example, here in this 
window, one gets a very broad input and 
output curve. 
Where, when the stimulus is varying very 
little, one finds a very sharp input and 
output curve. 
Now, it turns out that if one normalizes 
the stimulus by its standard deviation, 
or by this envelope sigma of t, all of 
these curves collapse onto the same 
curve. 
What that says is that the code has the 
freedom to stretch its input access such 
that it's accommodating these variations 
in the overall scale of the stimulus. 
And it's able to do that in real time as 
this envelope is varying. 
This is being seen in several other 
systems, including the retna and the 
auditory system. 
But here's an example from rat barrel 
cortex. 
This is somatosensory cortex of the rat. 
In particular. 
The part that encodes the vibrations of 
whiskers. 
So, from extracellular in vivo recordings 
of responses to whisker motion, whiskers 
were stimulated with a velocity signal 
again, s of t, that looked like this. 
So this is a slightly simpler experiment. 
The standard deviation was varied between 
two different values. 
And now one can pull out spikes that are 
generated in these two epochs that 
presentation. 
The high variance case and the low 
variance case. 
And one can compute input output curves 
for spikes that occurred under these two 
different conditions. 
So in the low-variance case, one sees 
this input output curve, in the 
high-variance case, one sees this input 
output curve. 
And hopefully you won't be surprised that 
if I now divide the stimulus. 
By its standard deviation, we now see a 
common curve. 
So now we see again that this input 
output curve has the freedom to stretch 
itself such that its able to encode 
stimuli in their natural dynamic range. 
So what I've shown you is that as one 
changes the characteristics of the 
stimulus. 
In this case, in the cases we've talked 
about, by changing its overall amplitude, 
changes can occur in the input output 
function. 
So here we've found that if a stimulus 
say, took on this dynamic range, it might 
be encoded with an input output curve 
like that. 
Now you should be able to see that if one 
increased the range of the stimulus and 
stayed with that same input output curve. 
Most of the time, your stimuli would be 
giving responses that were even zero or 
at saturation point. 
Similarly, if you now decrease the range 
of the stimulus you'd be hovering at the 
central part of the curve. 
So, ideally one would like to use one's 
entire dynamic brain by defined by this 
input output curve. 
And so, one would like to match it to the 
range of the stimulus. 
And that's exactly what we saw in the 
experiments. 
Now this adaptive representation of 
information is not confined to change us 
in the input output function. 
It's also been seen that changes can 
happen in the feature as the statistics 
of the inputs are changed. 
The feature that's selected by a neural 
system can also adapt to changes in the 
stimulus statistics. 
And information theory has also been used 
to explain the way in which this occurs. 
For example it's been used to explain how 
the spatial filtering properties of 
neurons in retina, and in LGN change with 
light level. 
Joe Addick and his colleagues pose the 
following question: If we consider that 
the retina imposes a linear transfer 
function, or a filter on its inputs, 
what's the shape of that filter that 
maximizes information transmission 
through the retina? 
The solution turns out to depend on two 
things. 
The powers spectrum of natural images and 
the signal-to-noise ratio. 
At high light levels, or high signal to 
noise, one would predict a filter shape 
like the one we've seen already, the 
Mexican hat shape. 
This acts like a differentiator, looking 
for edges of the stimulus, but at low 
light levels, the predicted optimal 
filter is integrating, and simply 
averages its inputs to reduce noise. 
And indeed in retinal receptive fields 
it's seen that the surround becomes 
weaker at low-light levels and the center 
braoder which qualatatively matches these 
predictions. 
We can also use information theory to 
find out what it is about a stimulus that 
drives a neuron to fire. 
We looked at this method in week two. 
In this case, this is called the, the 
method of maximally informative 
dimensions. 
One can choose a filter, so one can 
extract from the stimulus some component 
that maximizes the Colbeck-Libler 
Divergence between the spike conditional 
and the prior distributions. 
This turns out to be equivalent to 
maximizing the information that the spike 
provides about the stimulus. 
One can use this method to search for the 
optimal feature that explains the coating 
properties of a system. 
When it's being presented with stimuli of 
a particular distribution. 
Distribution. 
So for example if one initially starts 
with a Gaussian white noise distribution, 
that's a Gaussian, that's vertical 
Gaussian, in this, in this 
representation. 
One might find a particular feature. 
But now if one changes the distribution 
to say natural images, which will have 
some very different distribution. 
The filter that maximizes the, the 
information between spike and stimulus 
maybe different and that's being shown to 
be the case for cortical receptive fields 
among other systems. 
So, finish up by discussing briefly an 
influential idea that Ragesh mentioned in 
the first lecture. 
That might explain my cortical receptor 
fields have the shape that they do. 
Many years ago, Horace Barlow proposed 
that because because spikes are 
expensive, neural should be trying to 
encode systems as efficiently as 
possible. 
What does this mean for a popular of 
neurons? 
If you consider the joint distribution of 
the responses of many neurons, here lets 
just take two. 
Maximizing their entropy should imply 
that they code independently. 
That is their joint distribution should 
factor into the product of the two 
marginal distributions. 
This is a strategy that would maximize 
their entropy. 
Why is that? 
Because the entropy of a joint 
distribution is always less then or equal 
to the entropy of the distributions of 
the marginals added together. 
So this idea is known as redundancy 
reduction. 
The neural system should be optimized to 
perform as independently as possible. 
However in the past years, it's been 
realized that correlations between 
neurons can have some advantages. 
For one. 
Having many neurons that encode the same 
thing may allow for error correction and 
more robust coding. 
It's also been realized that correlations 
can actually help discrimination, and 
indeed, neurons in the retina have been 
observed to be redundant. 
That is, that their joint distribution is 
very different from the product of 
independent distribution. 
More recently, Barlow proposed a new 
idea, that neuron populations should be 
as sparse as possible. 
That is that their coding properties 
should be organized so that as few 
neurons as possible are firing at any 
time. 
This idea was developed formally by 
Olshausen and Field, and also Bell and 
Sejnowski. 
Here's the idea. 
Let's say that one can write down a set 
of basis functions, phi i, with which to 
reconstruct a natural scene. 
Then any image can be expressed as a 
weighted sum, with coefficients ai over 
these basis functions with perhaps the 
addition of some noise. 
Now this basis function should be chosen 
so that as few coefficients ai as 
possible are needed in general to 
represent an image. 
This is carried out by minimizing a 
function that includes the reconstruction 
error. 
So here, the root mean squared difference 
between the reconstructed image and the 
image itself. 
So that one gets a good match to the 
images, but that also includes a cost 
term, whose role, whose role is to count 
how many coefficients are needed, so one 
simple choice of this cost function, is 
just the absolute value of these 
coefficients. 
[INAUDIBLE]. 
The coefficient lambda, weights the 
strength of that constraint. 
The job of this term is to penalize 
solutions that require too many basis 
functions to represent an image. 
Too many coefficients ai, that are, that 
are different from zero. 
A fourier basis for instance, represents 
the images as a sum of signs and cosines. 
While the fourier basis is guaranteed to 
be able to represent any image. 
One might already be able to guess that 
coding with such a basis is not sparse. 
Because, as you probably recall, the 
power spectrum is broad, which means, 
that many coefficients are needed. 
When one runs an algorithm to find the 
best basis functions, the best values of 
phi i, for natural images, one finds, 
instead, a set of functions that look 
like this, like localized oriented 
features, like those that we see in v 
one. 
So this implies that when we view an 
image using neuronal receptive fields 
that look like this, this excites on 
average a minimal number of neurons. 
This is called a sparse code. 
So we've touched upon several different 
ideas about coding principles. 
The idea of coding efficiency, that 
neural codes should represent input 
stimuli as efficiently as possible. 
We've seen that this implies adaptation 
to stimulus statistics. 
As one changes the statistics of the 
stimulus, one should see aspects of the 
coding model changing to ensure that it 
remains efficient. 
We've also brought up the idea of 
sparseness. 
That it would be ideal if the neural code 
needed as few neurons as possible to 
represent its input. 
And this brings us to the end of our 
discussion of coding. 
I've shown you some classic and state of 
the art methods for predicting how 
stimuli are encoded in spikes. 
We've seen models for decoding stimuli 
from neural. 
Responses. 
We've discussed information theory and 
how it's used to evaluate coding schemes, 
and we've taken a very quick glance at 
how coding strategies might be shaped by 
the statistics of natural inputs. 
There's a lot that we've missed. 
In particular, let's just go through the, 
the typical cycle of behavior of an 
organism. 
Where we have invested some time is the 
idea, that we go from complex 
environments, animals extract some 
features from that environment to solve 
problems, and that's represented in 
neural activity. 
What the brain is then doing is 
extracting that information and 
synthesizing it to drive decisions. 
We talked about some examples of using 
maximum likelihood methods that might in 
fact have neural implementation. 
These decisions then generate motor 
activity which drives behavior. 
Muscles work together to perform actions 
that drive behavior output. 
And these actions effect subsequent 
sensation. 
So, we didn't really address any of this 
part of the, of the behavioral feedback 
loop. 
Next week, we'll be moving onto a new 
topic. 
Rather than handling data analysis, we'll 
be moving more into the realm of 
modeling. 
And we'll start that with a brief 
introduction to the bio-physics of 
coding. 
How do single neurons generate action 
potential. 
We'll talk about neuronal excitability. 
And we'll close up with some simplified 
models that capture neuronal firing 
before moving on to the second part of 
the course where you'll be learning about 
network modeling. 
So that's all for this week. 
Looking forward to seeing you next week. 

[MUSIC]. 
Hello, this is Adrienne Fairhall. 
Welcome back for my final week of this 
part of Computational Neuroscience. 
This week, we're going to be talking 
about computing in carbon. 
We're going to be zooming down from the 
very high level picture that we had of 
Hugh Jackman neurons, those I, those I do 
have, to the, the single neuron level. 
And thinking about how the brain, in 
fact, instantiates the kinds of 
computations that we saw in previous 
lectures. 
So we're going to be talking about the 
basics of neuroelectronics, the 
membranes, the ion channels, and the 
wiring that make our brains work. 
We'll be going from there to talk about 
single, simplified neuron models. 
And thinking there about the basic 
dynamics of neuronal excitability. 
Can we as modellers come up with simpler 
models that capture the essential 
dynamics that neurons instantiate? 
Finally, we'll be talking neuromal 
geometry. 
We see that neurons have very complex 
structures. 
So how do these matter? 
How do they affect the computational 
properties of the neuron? 
And in what way do we need to take them 
into account? 
Our goal for today is going to be to take 
this somewhat realistic picture of a 
neuron complete with a soma with 
dendrites that collect inputs from, from 
other neurons. 
And an axon that transmits action 
potentials generated in or near the soma 
to other neurons into a circuit diagram. 
We're going to start with a model of an, 
a small patch of membrane that generates 
action potentials only. 
And I hope to show you exactly what gives 
a neuron its very distinctive behavior, 
the ability to fire an action potential, 
using the classic Hodgkin-Huxley model. 
Just to note that while we've been using 
quite a variety of mathematics in this 
course so far, including linear algebra 
and probability, today's method's 
going to be based largely on differential 
equations. 
We'll be making quite heavy use of first 
order differential equations, and that 
should be enough to understand what's 
going on, which is interesting as you'll 
see. 
What I hope you'll take from today's 
topics is an understanding of ion channel 
dynamics and a general sense of how one 
can either make these models more simple 
or more complex, depending on your goals. 
So let's start with a review of simple 
circuits. 
I imagine that for some of you, it's been 
a while, if ever, so I'll go over the 
basics. 
First remember that along any line in a 
circuit diagram, that represents a wire, 
and so current can flow freely and 
there's no voltage drop along the wire. 
All voltage changes around a circuit are 
associated with circuit elements, such as 
a capacitor here, or a resistor, or a 
battery. 
A capacitor is an insulator which 
accumulates charge on either face. 
A resistor resists the flow of current, 
like a narrowing in a pipe which slows 
down the flow of water. 
A battery causes a potential drop. 
At any junction of wires, the total 
current is zero. 
This is called Kirchhoff's law. 
For example, here at this junction, let's 
imagine that we're driving the circuit 
with some input, let's say i x. 
In that case, the sum of the, the current 
flowing out here through the capacitor 
and here through the resistor must add up 
to the current that's coming in to i x. 
How does the potential change across a 
resistor? 
There's a voltage drop that currents 
times the resistance. 
This is Ohm's law. 
Often in neuroscience, we use an 
alternative measure of resistance, the 
conductance, which is just one over the 
resistance. 
So now let's look at a patch of the 
membrane that encloses our neuron. 
How are we going to model it, using such 
a circuit? 
Remember that the membrane consists of 
two layers of lipids or fats. 
These are good insulators. 
Embedded in the membrane are ion channels 
which allow ions to pass through the 
membranes selectively. 
So let's use those laws to write down the 
first pass at an equation for the neuron. 
For now we'll leave aside the ion 
channels, we'll come back to them. 
The membrane still allows some small 
amount of charge to flow through. 
So how does the membrane itself behave? 
The lipid bilayer behaves like a 
capacitor. 
As some charge can pass through the 
membrane, we also need a resistor in 
parallel. 
This gives us this very simple circuit. 
So how do we write down an equation for 
this circuit's behavior? 
We'd like to obtain an equation for the 
voltage, here v, across the membrane. 
Now we can use Kirchhoff's law to find 
it. 
All the currents have to sum up. 
So by putting some external current, pie 
X'd. 
What is IR? 
What's the current through the resistor? 
Well we can get that from Ohm's Law. 
It's just V over R. 
How about the capacitative current? 
We get that from the definition of the 
capacitance given here. 
So, the capacitance is defined to be the 
charge that can be stored across the 
capacitor divided by the voltage. 
Now, we can take the time derivative of 
this expression, so let's rewrite it as q 
equals c times b. 
And take the derivative of that. 
And dq dt equals C. 
C is just a constant dvdt. 
Now dqdt, the time derivative of the 
charge, is just a current Ic. 
So now we see that Ic is just C times 
dvdt. 
We throw these terms back into 
Kirchhoff's law and we get the following 
differential equation. 
So this differential equation is linear 
and it's first order in V, that is it's 
dependence on V is just simply linear in 
V. 
So people paying sharp attention will 
have realized that while this might be 
all well and good for a little chunk of 
membrane floating around in some 
solution, it's not really what's going on 
with our cell. 
Remember from the first lecture that the 
membrane encloses a solution in which the 
concentrations of various ions are 
different from the outside. 
So this now is more like it. 
Outside the cell is the, is the harsh 
world, basically the sea with high 
sodium, high chloride, and also high 
calcium concentrations. 
Inside, the cell has hoarded some 
potassium, which are, which is necessary 
for many of its life operations. 
It maintains these ionic gradients with 
specialized pumps that exchange sodium 
for potassium. 
Now this concentration gradient gives us 
a battery and maintain potential 
difference across the cell. 
There are two opposing forces at work. 
By osmosis, ions tend to move down their 
concentration gradient, at least until 
that osmotic force is opposed by 
electrostatic forces. 
The potential difference at which these 
forces are in equilibrium is called the 
Nernst potential. 
This is given by the ratio of the 
concentrations on the inside to the 
concentrations on the outside. 
Take the log, and now that's multiplied 
by this set of constants. 
Now kbT, this is the Boltsman constant, 
this is the temperature. 
Q is the ionic, ionic charge, and z is 
the, is the number of charges in the ion. 
Now how does our equation change in the 
presence of this battery? 
The effect is that part of the voltage 
drop V, that occurs across the resistor 
is lessened by the battery, right? 
So before, we had all of that voltage 
drop occurring across the resistor. 
Now, part of the voltage drop v is, is 
separate from the resistor. 
And so now, the part of the voltage drop 
that occurs across the resistor is v 
minus v rest. 
So that's going to reduce the current 
through the resistor. 
So now, our IR, the current through the 
resistor is going to be v minus v rest 
over R. 
So here's our new equation. 
Some of you will be familiar with the 
solutions of such a differential 
equation. 
They take the form of exponentials. 
So let's put this equation in a 
convenient form, where we can read off 
some important quantitites. 
With a little bit of algebra, we can 
rearrange this equation into this form. 
The constant, tau, here, is known as the 
time constant. 
Perhaps you can figure out, what is tau 
in terms of the original constants? 
We also have this term, v infinity. 
What's the meaning of v infinity? 
So, let's set the time derivative to 
zero. 
That is, assume that whatever changes we 
might have made in, in i x happened long 
ago. 
And we've let the system settle down. 
If you set dvdt equals zero, then v 
equals v infinity. 
So it's just the steady state. 
So imagine we're putting in a constant 
input current. 
What is v infinity in that case? 
Here's an example solution of this 
equation for a square pulse of 
[INAUDIBLE] input current. 
Initially v is zero. 
Then it rises exponentially until it 
reaches V infinity. 
When the current is switched off again, 
it falls exponentially back to zero. 
So, the solution here looks like V 
infinity 1 minus e to the minus t over 
tau. 
And in the falling phase, this looks like 
v infinity e to the minus t over 10. 
We said that the potential was given by 
the ratio of ionic concentrations inside 
and out. 
If ions could all flow in the same way 
through the membrane, we wouldn't need to 
distinguish between concentrations of 
different ionic species, we'd just 
consider the total charge. 
However, the way that ions flow through 
the solids via ion channels, and the, 
these do distinguish between different 
ions. 
What are ion channels, really? 
They are amazing little molecular 
devices, and there are many types. 
Some are voltage-dependent, some are 
neural transmitter dependent, some depend 
on calcium, some are mechano-sensitive 
and some are heat-sensitive. 
The property we'll be focusing on today 
is voltage sensitivity. 
So let's look at the current that comes 
through a single ion channel as it opens 
and closes stochastically. 
We can still apply Ohm's law to this one 
tiny channel. 
So the current is just the voltage drop 
across the, across the channel, divided 
by r, the resistance. 
Or we can also write that as I equals Vg, 
where g is the conductance of that 
channel. 
So two factors set the size of the 
current. 
The voltage drop across the membrane and 
the conductance of the channel. 
So, it turns out that each ionic species 
has its own associated equilibrium 
potential. 
Then when the membrane allows a 
particular ion type to flow through it, 
through an appropriate channel, then that 
current will tend to pull the membrane 
potential toward the equilibrium for that 
ion. 
So here are some examples of the values 
of equilibrium potentials for different 
ionic species. 
So, sodium has a positive equilibrium 
potential. 
Whereas potassium has a very negative 
equilibrium potential. 
Here's the one for, for calcium. 
Also very negative. 
And for chloride. 
6 at minus 60 millivolts at minus 60 
millivolts which is around the rest 
potential. 
We going to be concentrating today on 
these two, on sodium and potassium. 
What you should notice here, and the most 
important thing to notice about the rest 
of our discussion today, is that sodium 
and potassium have opposite tendencies. 
Sodium currents tends to depolarize the 
membrane. 
That is to move it to more positive 
potentials. 
While the potassium current tends to 
hyper polarize it. 
That is to take it toward more negative 
potentials. 
So while there are many different types 
of ion channels that allow different ion 
species to pass through the membrane, 
we're going to focus on sodium and 
potassium. 
Each ionic path through the membrane has 
its own battery potential, as we just 
learned. 
And its own conductants. 
So that the current through each branch 
of circuit is by Ohm's law, v equals i r. 
But let's use conductants, and we have to 
discount the membrane voltage drop. 
Right, we have v across the membrane. 
You have to discount the membrane voltage 
drop by the equilibrum potential for each 
ion. 
So now we can see that the current 
flowing through each of these branches 
indexed by the ionic type is the 
conductance for that ion, multiplied by V 
minus Ei where Ei is, is the equilibrium 
potential for that ion species. 
So this is all interesting. 
There are different ions; they have their 
own battery. 
But we haven't yet seen what makes the 
transformation that a neuron makes on its 
inputs qualify as a computation. 
Simply linearly transforming the current 
through the passive membrane properties 
doesn't seem to have the flavor of 
computing. 
Computing, one could say, requires doing 
something irreversible. 
Selecting something, getting rid of other 
things, qualitatively changing something 
into something. 
As we discussed in week two, a linear 
transformation simply re-represents the 
input in another coordinate basis. 
One can always invert that tranformation 
and get back what you started with. 
And the coding models that we looked at. 
The linear filters extracted a new 
representation, but the non-linearity 
threshold at that new representation and 
made the transformation non-invertible. 
Information about other components was 
irretrievably thrown away. 
So here at the single neuron level, you 
can see how this linear behavior breaks 
down. 
These traces all look comfortably linear, 
nice scale versions of the same response 
as one changes the sign and amplitude of 
this current input. 
Exactly as a linear system should do. 
But for some input currents, something 
totally different happens. 
This is called excitability. 
So let's take a break here and come back 
to figure out exactly what's going on. 

In the last section we left off with this 
picture, a neuron responding to current 
steps of different, of different 
amplitudes, and we saw that it's on the 
threshold here, of excitable behavior. 
So our goal in the remainder of this 
section is to uncover the nonlinearity 
that leads to this vital property. 
In the last section, we realized that 
each ion has its own pathway for current 
to flow, and a battery of potential 
difference that's associated with each. 
We can represent that as these different 
branches. 
So let's assemble them now, back into, 
back into our circuit. 
We have a branch for sodium. 
A branch for potassium, and we're also 
going to include one for the non-specific 
current flow of the passive membrane. 
going to call that g link. 
Now, if the ion channels had a fixed 
conductance, we'd still have a linear 
circuit, just a bit more complicated than 
our original passive, our c circuit. 
What gives this system its interesting 
behavior, is that these conductances are 
not fixed, that's what this variable 
resistor symbol stands for, they depend 
on the voltage. 
So let's take a closer look. 
So now we're zooming in down to the level 
of a single potassium channel. 
The channel is an elaborate molecular 
machine that contains a gate that 
prevents ions from entering, and a 
voltage sensor here, that controls the 
configuration of the gate channel. 
The open probability increases when the 
membrane is depolarized. 
The gate here consists of four sub-units 
that need to be in the correct 
configuration in order for ions to flow 
through. 
So the open probability of the channel is 
the product of the open probability of 
these four sub-units, so the probability 
goes as, the open probability of a single 
sub-unit raised to the power 4. 
So what is the assumption we're making 
here? 
Let's try to visualize this a bit. 
So here's the closed channel. 
Each sub-unit fluctuates, open and 
closed, at some rate. 
So let's call n the probability that this 
sub-unit, one of these sub-units is in 
the open state. 
Then the probability that it's closed, is 
1 minus n. 
So now, one of those other gates can open 
and close. 
Independently, another one flicks open. 
Sometimes we have more than one open. 
Finally when all four happen to be open 
together, the ion channel allows a 
current to flow. 
A trans, a transition between states 
occur at volt, let's say this red circle 
represents the total probability of the 
state of one of the sub-units. 
It has probability n of being in the open 
state, and probability 1 minus n of being 
closed. 
Call this open and this closed. 
So there's a rate of transitions between 
the open and the closed state. 
So the closed to open state we're 
going to call the transition probability 
between closed and open alpha, and that's 
voltage dependent. 
And there's also some rate, and there's 
also some rate of transitions between 
open and closed, which your going to call 
rate beta, that also is voltage 
dependent. 
So now the time derivative of n is then 
given by the following equation. 
So this first term, represents how much 
is added to the open state, and the 
second term, how much is lost? 
The amount that's added, is proportional 
to amount that's in the closed state, 
times the rate of going from closed to 
open alpha. 
And this is the amount that's lost, the 
amount that's actually in the open state, 
n, times the rate of moving from open to 
closed. 
So let's do what we did with the RC 
circuit, and rewrite this equation in 
terms of tau and n infinity. 
So when we do this, you can easily show 
for yourself that the rates determine the 
quantities tau and infinity in the 
following way. 
Tau is 1 over alpha plus beta. 
N is alpha over alpha plus beta. 
We'll come back and use this relationship 
shortly. 
But before we do, let's take a look at 
the sodium channel. 
So here's the sodium channel. 
It's similar to the potassium channel, 
but with one important difference. 
The sodium channel is open to the joint 
opening now of three sub-units, but, 
similar to the previous case. 
But additionally, to pass current, this 
channel requires that along with the 
activation or opening of the three 
sub-units, there's also an additional 
gating mechanism. 
A kind of ball in the socket mechanism, 
and that's required to not be in place. 
That is that there's an additional gating 
factor that must be de-inactivated. 
We can express the probability of one of 
these three sub-units being open, as m 
and the probability that the gate is not 
in place as h. 
What's interesting is that while voltage 
increases, the degree of activation or m, 
it also decreases h, the level of 
de-inactivation. 
So there's a kind of voltage window, in 
which sodium is able to flow. 
Generally this results in sodium currents 
being transient or self-limiting. 
As soon as sodium starts the flow, 
because these m gates are open, the 
voltage moves towards the sodium 
equilibrium potential, and that increases 
v and in-activates h, thus closing the 
channel again. 
This is one of the mechanisms at work in 
switching off the spike. 
So now we have these three gating 
variables, one for k, that was n, and two 
for sodium m and h. 
We can also re-express them in terms of 
tau and their steady state. 
So as we did for n we can also write 
these two equations, a tau for the h 
variable and a h infinity, and also a tau 
for m, and a m infinity. 
So now what do we do with these 
activation and activation variables? 
We want to combine them to give us these 
voltage dependent conductance's for the 
channels. 
So the probably of the potassium channel 
being open goes like n the 4th. 
You're going to multiply that by the 
total conductance of the channel, and 
that will give us this voltage dependent 
conductance. 
Similarly the probability of the sodium 
channel being open is given by m cubed of 
h. 
We multiply that by sub-maximal 
conductance for sodium, and now we get 
the voltage dependent and time dependent 
conductance for sodium. 
So now let's pull it all together. 
The voltage across the membrane changes 
as a result of changes in the, in the 
external driving current, and also 
because these opening and closing 
probabilities cause the conductance's of 
these, of these branches to change. 
And the amount of current going through 
will change will both with changes in 
voltage, and with changes in overall 
conductance. 
So we can write down that equation here. 
So we have our Capacitative current, 
that's the current coming through this 
branch. 
We have the Ionic currents, which come 
down through each of the Ionic branches 
separately with sub-scripted each of the 
ions with i. 
And, and that includes our, our leak 
which includes non-specific movement of 
ions through the, through the membrane, 
and then our external applied current. 
So, what this gives us is Hodgkin and 
Huxley's equation, here, in it's, in it's 
full glory. 
So we have our equation for the voltrage. 
And we're going to add to that, these 
three equations for the different 
activation and inactivation variables, 
that specify the conductance's for the 
different ionic types, sodium potassium. 
Now let's see how we get to use our 
understanding of the activation dynamics, 
to understand the spike. 
So, remember again, n governs the opening 
of the potassium channel, and both n and 
h must be large for the sodium channel to 
be open. 
Here's how these activation steady states 
depend on voltage. 
They all have this kind of sigmoidal 
form. 
As we see from the behavior of n 
infinity, the potassium channel will have 
a higher probability of opening for 
larger voltage. 
While the sodium channel first has an 
increase of probability of opening with 
increasing voltage, because the increase 
in m with voltage. 
But then because h is going down to 0, 
as, as, voltage increases the, the sodium 
channel will close. 
It's also very useful to look at the time 
constants, going back to our equation, 
this time constant governs how quickly n 
will approach its final steady state. 
So the time constants dictate how rapidly 
each variable responds to a change in 
volt. 
Remember the exponential solution. 
Let's say one changes v, so that's going 
to give us a new value of the steady 
state, as a function of v. 
And then we wait for everything to 
adjust. 
Each activation variable will tend toward 
the steady state for that voltage, with a 
rate given by this time constant. 
So, which of these variables here reacts 
fastest? 
The variable with the shortest time 
constant, that is m. 
So that means that the fastest response 
to a voltage change, is a change in 
sodium activation. 
The dynamics of h and n are slower, these 
time constants are larger. 
And you can see that they're on a similar 
scale. 
So let's also remind ourselves what the 
resting potentials are. 
Remember that when a potassium current 
flows, it would be tending to move the 
membrane voltage toward the potassium 
potential, down here at minus, minus 80. 
Well, sodium moves it up here. 
So let's imagine we're sitting near rest. 
Rest is about minus 60 milivolts, and 
then some input comes along that 
depolarizes the membrane, that is move it 
to larger, larger voltages. 
So because the time constant for m is the 
shortest, as we change voltage the first 
thing to adjust is going to be the m 
value, its going to approach its steady 
state value, at the new value of the 
voltage. 
That starts to open sodium channels. 
Sodium current comes in, and starts to 
move the membrane toward the sodium 
equilibrium potential. 
That's going to further increase sodium 
conductance, and that's a positive 
feedback. 
So what's going to counteract that, and 
stop the voltage from just ending off to 
this large value? 
So at a slight delay because of these 
slower dynamics for, for h and for n, two 
things are going to happen. 
One is that h goes to h infinity. 
So finally the dynamics of h catch up, 
and h is going to approach its steady 
state value. 
And you could see that as voltage 
increases, that steady state value is 
going down. 
And remember that for the sodium channel 
to be open, we need a combination of m 
cubed and h, so if h is going towards 0, 
then those channels are closing. 
Also, to help things along, the potassium 
channel also activates more. 
So now finally n will also catch up, and 
we'll see that the potassium channel 
starts to open more and more with larger 
voltages. 
Now what does that do? 
That starts to pull the voltage back down 
here, toward the equilibrium potential 
for potassium. 
So finally the membrane will come back to 
rest. 
So this is just to show the time cost of 
these events. 
Voltage increases. 
Here, there's a fast change, you see this 
very fast slope in m. 
There, there's a positive feedback in 
which this increases very rapidly, until 
its rise is truncated by the delayed 
effects of h, now going down and, and 
closing the sodium channels. 
And n's starting to increase, and allow 
that potassium current to bring the 
membrane back toward the potassium 
reversal potential. 
So, you see here that the action 
potential is this exquisitely timed 
change of molecules and charges. 
So, what's so wonderful about Hodgkin and 
Huxley's model, is that they inferred all 
of these dynamics without any knowledge 
of ion channels. 
And particularly without any knowledge of 
sub-units. 
All the dynamics here are explained by 
simple linear equation by a linear 
circuit, or a simple rate equation, 
except for two things. 
The multiplicative factors that relate 
the sub-unit behavior to the channel 
conductance's and the voltage dependence 
of the sub-unit dynamic's. 
So, from this fundamental basis there are 
two quite different directions that we 
could go in as a modeler. 
So, one can delve into the dynamics of 
ion channels, understanding how they come 
about from the microscopic level, and how 
different signaling cascades influence 
these dynamics. 
There are, of course, hundreds of 
different channel types dependent on 
calcium and chloride, and even 
combinations of multiple ions in very 
different time scales. 
And this wide range of dynamics 
influences the way in which information 
is processed by single cells, and 
dictates which neuron types carry out 
different roles in the brain. 
Furthermore, realistic neurons are not 
just patches of membrane. 
As we've looked at here, they're large 
distributed structures. 
So how does this figure into our 
understanding of computation at the 
neuronal level. 
The other direction to go in, is rather 
than to complexify, to simplify. 
Can we write down simpler models that can 
capture the essentials of these dynamics, 
but are maybe analytical tractable, so 
that we can learn something 
mathematically. 
Or at least be rapid enough to be able to 
put into large scale simulations, that 
still respect something about the 
underlying biophysics of neurons. 
So we're going to head in these two 
different directions for the rest of the 
lecture. 
In the, in the next part of today's 
lecture, we'll, we'll first deal with 
these simplified models, where some 
examples of reduced models that people 
have developed that, that are based on 
Hodgkin Huxley like neurons. 
And in the last part of today, we'll ju, 
just touch on one of these topics, that's 
geometry. 
How do we deal with Exter, such as 
dendrites, in modeling single neurons? 

Hi. 
In this part of the lecture, we're moving 
beyond Hodgkin-Huxley to think about 
simplified models. 
So, can one build simple models that 
capture the behavior of, of true neurons? 
But are either, analytically tractable, 
so that one can do some analysis on them 
and understand, maybe, how different ion 
channels contribute to their interesting 
dynamics. 
Or else, can one build a simulation, a 
large scale simulation, that has models 
that are as simple as possible that is, 
that involves little computational time 
as possible to model and yet capture the 
relevant and interesting dynamics of real 
neurons? 
So here are a few different examples of 
firing patterns from real neurons being 
driven by a noisy input. 
On the top, you see cortical neuron early 
in development and then here thalamic 
neurons that have been recorded under 
different depolarizations. 
So here, in particular, you can see a 
very characteristic bursting pattern, 
where a bunch of spikes are generated in 
a clump. 
And at a different depolarization, those 
bursts almost disappear, and you get 
single spikes, more like, more like in 
the case of the cortical neuron. 
And finally, here's a motor neuron. 
So in this case, you see very regular 
firing. 
Motor neurons tend to fire very 
regularly, and the noise leads only to 
small deviations in the regular timing of 
spikes. 
So, we see that neurons can have a wide 
range of firing patterns, which come 
about partly because of the nature of 
their dynamics, and partly because of the 
nature of their inputs. 
Let's look at some potential examples of 
firing patterns. 
Imagine that a neuron fired regularly 
like this. 
And to a second input, it also fires 
regularly, but with a different, with a 
different spiking interval. 
So one might feel comfortable thinking 
about this neuron's behavior as 
expressing a rate code. 
The spike frequency signals the input. 
What, though, if we now had this case? 
Here, the mean frequency is the same, but 
now the firing times of spikes are 
shifted slightly. 
So, we might imagine that these little 
changes in local frequency and code 
stimulus information, may be like 
frequency modulator or FM signals. 
In the next case here, the main firing 
rate might still be important. 
But there's so much variability in timing 
that, that suggests that precise spiked 
times might mean something distinct about 
the input. 
And what about this final case? 
Here now you see that there are perhaps 
two distinct symbols in the code. 
This looks like the bursting that we saw 
in this thalamic neuron, are these single 
spikes signalling something different 
than these, than these groups of spikes, 
or these bursts. 
So, neurons are capable of firing with 
these many different kinds of outputs. 
And if we're trying to come up with a 
reduced model, we'd like to aim for one 
that would allow us to represent these 
different behaviors. 
So try to keep this range of different 
behaviors in mind as we go through 
different ideas about how to make reduced 
or simplified model neurons. 
Let's start with the simplest case. 
Let's just try to write down an equation 
from V that does something like what a 
neuron does. 
So we have a differential equation that 
looks like this, and and our task is to 
find a good function f of V that makes 
[INAUDIBLE] do what we want it to. 
So as we observe, the behavior of the 
neuron can be quite close to linear as 
long as it's not near spiking. 
So how bad would it be to assume that we 
simply have a linear neuron. 
That is, an equation such as we've found 
for the passive membrane. 
So note, from now on, that I'll set the 
capacitance equal to 1, so we don't have 
to carry constants arounds. 
So I'm drawing this case above. 
So here, f of V is simply minus a minus V 
naught. 
So how did the dynamics of such a neuron 
look. 
So here's our equation for the voltage. 
Let's, for now leave aside our input. 
So f of V is minus a V minus V naught. 
We have a fixed point at dV dT equals 
zero. 
That is, that V equals V not. 
Now, how do the dynamics look above and 
below that fixed point? 
If you have a voltage, which is on this 
side of the fixed point. 
So let's add dV dT for this value of the 
voltage is positive. 
So voltage increases, and that's true 
everywhere along this part of the line. 
On this side of the fixed point however 
the voltage dV dT is negative. 
And so anything that's out here moves 
back toward V naught. 
That's what makes V naught a stable fix 
point. 
But how do we get a neuron like this to 
fire a spike. 
We need to add in a couple of things. 
So for one thing we need to say that 
there's some threshold. 
So as I move around in V, although I'm 
always being drawn back to this fixed 
point, if I happen, because of the 
addition of some input, to be pushed up 
to some threshold voltage, what I'm 
going to do is, set this equal to the 
time of a spike, and just jump myself up 
to a maximum. 
And so, if we just plot what that looks 
like in time, we have some voltage that's 
varying along. 
It hits this value of the threshold, V 
thres, and instantaneously we're going to 
set that equal to the maximum of the 
spike. 
And the next thing we're going to do is 
take that voltage and reset it. 
We're going to take it back to some V 
reset out here, and now the input will 
continue to push it around, but starting 
at that new reset value. 
And so you can see that this mimics 
pretty well what spike trains look like. 
Let's have a look at that directly. 
So this is like the passive membrane. 
It, remember the equation that we wrote 
down earlier for the passive membrane. 
This captures that linear behavior. 
It has the additional rule that when V 
reaches the threshold, a spike is fired. 
And then, it's reset. 
And V naught is just the resting 
potential of the cell. 
So here's an example of how the 
integrating fire model works in, with, in 
response to a particular input. 
It might be hard to distinguish that from 
a real, a real spike tree. 
So while the integrating fire model has a 
lot of advantages and suddenly captures 
some basic properties of neurons, one can 
come a lot closer to the true dynamics of 
neurons. 
So, in the integrating fire model, we had 
to paste on the spike to make it 
excitable. 
How can we make this model intrinsically 
excitable? 
So what we need to do is to add on some 
more stuff to our f ov V. 
What we need to do is to give f of V a 
range where the voltage can, in fact, 
increase. 
So now, what have we done here? 
We've added in another fixed point. 
So we have here, stable fixed point. 
Now, what's up with this fixed point? 
So remember, that here, the voltage heads 
toward the fixed point. 
What's going to happen as we cross this 
fixed point. 
So now with voltages larger than, than 
this value, you can see that this dV dt 
is now positive and we're going to start 
heading out to larger and larger values. 
And so in response to dynamics like this, 
what's going to happen is that as one 
crosses this effective threshold. 
So now if you have some effective input 
that takes you above this value, now the 
voltage is just going to, now the voltage 
is just going to increase. 
So that means we still need a couple of 
extra pieces as we needed for the 
integrating of firing neuron. 
We're going to add a maximal voltage, not 
a threshold, now the threshold is 
determined intrinsically by the crossing 
by this unstable fix point of f of V. 
But now we need some maximal voltage 
beyond which the spike can not continue 
to increase. 
And when we reach that voltage, we're 
going to reset again back to some reset 
value. 
One example of form of a f of V that 
works quite well, is simply a quadratic 
function. 
So another example of a choice of f of V 
that's being shown to fit cortical 
neurons very well is the exponential 
entry of fine neuron. 
Now here, we can choose f of V, so that 
has an exponential piece. 
So that part of the dynamics sub 
threshold are linear and part have this 
exponentially increasing part that mimics 
the rapid rise of the, of the spike. 
And again we have to add a maximum and 
reset. 
So this model has an important parameter, 
delta, which governs how sharply 
increasing the nonlinearity is. 
So here's a strongly related example of a 
one dimensional model that gets a lot of 
use. 
This is called the theta neuron. 
And in the theta neuron, the voltage is 
thought of as a phase, theta. 
When the phase reaches pi, here, we call 
that a spike. 
So what's neat about using a phase 
instead of a continuous variable, like 
voltage as before, is that as soon as you 
pass through pi, you wrap around to minus 
pi and that gives you a built-in reset, 
so you don't need to add that extra part 
into the dynamics. 
So the dynamics given by, by this 
equation here. 
This is being shown to actually be 
equivalent to the one dimensional voltage 
model with a quadratic nonlinearity. 
This model also has a fixed point, V 
rest, and an unstable point, V thresh, 
which acts like a threshold. 
Now, because this model fires regularly, 
even without input, so now, let's imagine 
that It is zero. 
You can see that these dynamics are still 
regularly firing. 
They'll continue to oscillate, the theta 
neuron is often used to model 
periodically firing neurons. 
So aesthetically, lets say, we're still a 
little pained by this construction of the 
maximum and the reset, or even the reset 
on the, on the phase variable. 
Is there anything else we can do to 
improve this simple model? 
How might we prevent our spike from 
increasing to infinity, apart from 
putting some maximum on it? 
So, let's try falling. 
So what does that do? 
Now, there's another fixed point, here. 
So that we still have our stable fixed 
point. 
We have an unstable fixed point, which 
acts as our threshold. 
And now, we have antother fixed point. 
Now, is it stable or unstable? 
Let's just, let's just check it. 
So here we're increasing. 
There we're decreasing. 
Here we're increasing. 
And here we're moving back toward that 
fixed point, this is a stable fixed 
point. 
Hopefully you will sort of intuitive by 
now that you can tell whether a fixed 
point in this one-dimensional 
representation is stable or unstable, by 
just looking at the slope of f of V at 
that point. 
Whenever the slope is negative, that's 
the stable fixed point. 
And if the slope is positive, it's an 
unstable fixed point. 
So now we have this fixed point. 
What's the dynamics? 
Now once we get above our threshold, we 
increase. 
And instead of increasing without bounds, 
we go to this fixed point. 
So that's great. 
However, the problem is that it stays 
there. 
The system is called bi-stable. 
In order to allow the dynamics to come 
back from that stable fixed point, let's 
remember what happened in Hodgin-Huxley. 
Actually, two separate mechanisms helped 
to restore the voltage back to rest. 
One was that, this, one was that the 
sodium, switching of the drive toward 
this sodium equilibrium potential. 
And the other was set that potassium 
channel activated pulling the voltage 
back to what the potassium equilibrium 
potential. 
So here we need to do something similar 
to pull the voltage back toward rest. 
And that is to include a second variable 
to take care of inactivation. 
So that's done here by including this 
second variable, u. 
So u here decays linearly, but it also 
has a coupling with V. 
So this function of voltage means that 
when the voltage gets large, u is also 
driven to be large. 
Then we couple inactivation variable into 
the voltage equation. 
One would want the function G(u) to be 
negative, so that a large u, pulls V down 
again. 
So this leads us to the consideration of 
models that have two dynamical variables. 
Now, instead of drawing my f of V against 
V we need a new kind of plot called a 
phase plane diagram. 
The phase plane is just the plane to find 
by a dynamical variables V and a. 
Now, our understanding of how the model 
behaves is organized not just by 
identifying the fixed points as we were 
doing so far, but looking at the entire 
line of points, where either one or the 
other variables has zero derivative. 
So, we can define these nullclines, 
here's the V nullcline, as the line in 
which In which dV dt equal zero. 
So we set this equation equal to zero. 
That's going to give us a function of u 
with respect to V, if we solve this 
equation. 
And here, here it is. 
Similarly, there's a u nullcline, at 
which du dt equals zero, and that defines 
this other curve. 
For most neural models, nullclines have 
shapes something like I've drawn here. 
In this particular case, there's one 
fixed point that is a true fixed point, 
where both dV dt equals zero and du dt 
equals zero. 
And that's here. 
This is the resting state. 
So now we can think about what happens if 
we start out at some particular value of 
V and u. 
We're going to head out in a trajectory 
that has a velocity that has a component 
in the V direction, given by dV dt, dV dt 
evaluated at V and u. 
And at component in the u direction, 
which is going to be given by du dt. 
At v and u. 
The nice thing about these nullclines is 
they give us a sense of how trajectories 
in this two-dimensional plane will work. 
So this green curve divides parts of the 
plane in which the voltage is either 
increasing, down here on this side of the 
green curve and decreasing here. 
Whereas, the red curve divides regions of 
the plane in which u is either increasing 
or decreasing on this side of the red red 
curve. 
So if we start near rest, with an input 
that now takes us out into some larger 
voltage range, the nonlinearity in 
voltage now says that we start to move 
quickly in V. 
We're now going to undergo what will look 
like a spike. 
And now that we've crossed that green 
line, now remember that the direction of 
the voltage now changes now we're 
going to come backward. 
Wrap around this direction, move that 
way, but we still need to increase in u 
in this half of the plane. 
Now we've crossed that red line. 
Now we start to decrease and we come back 
this way. 
And so we have a spiking trajectory. 
If we now plot that, so now the voltage 
as a function of time, small and rapidly 
increases. 
Now, depending on how quickly it moves 
along this part of the no claim, it will 
gradually come back again. 
So there's an enormous amount or richness 
and fun to be had by analyzing your 
[UNKNOWN] dynamics. 
Likes in the phase plane this is a very 
simple example that can be multiple fix 
points, limit cycles, all different kinds 
of bifurcations and dynamics as the input 
changes. 
Since there's no way we can do justice to 
this in this course, I'm not going to go 
into any of this, would actually be a 
great point to branch out an entire 
second course. 
Some [UNKNOWN] dynamics in phase plane 
analysis. 
So luckily for you there's a great book 
available by Eugene Izhikevich if you 
want to explore this direction. 
The reference is posted on the website. 
There's also a lot of resources online 
by, by scholars like Wulfram Gerstner, 
Bard Ermentrout, our white knight of the 
previous slide. 
And also generally perusing through 
scholarpedia. 
What I will do, however, is introduce you 
to one final model that's inspired by all 
this richness. 
And that's the so called simple model. 
Izhikevich and others have noted, that if 
you zoom in here, to the, to this part of 
the phase plane, you can pick off the 
important dynamics that generate a lot of 
the nice behavior of real neurons. 
The voltage dynamics are approximated by 
a quadratic function, that's the minimal 
nonlinearity we need in order to, to have 
excitability. 
And there's simply a linear coupling to 
you. 
The u dynamics are taken to be linear, 
the linear both in V and in u. 
So what this would do is as V gets larger 
then that's going to drive u to get 
large. 
The coupling here is now going to 
decrease the voltage as u gets large. 
So that forms the basic role of 
inactivation. 
So this reduced model is certainly not 
complete. 
So like in the cases that we've just 
left, we've thrown away the higher order 
dynamics in voltage that allow it to 
restore itself from a spike. 
So we have to go back to putting in a 
maximum and a reset. 
So these are parameters of the model. 
The u variable also needs a reset. 
So one is left with one, two, three, four 
parameters and these four parameters 
determine the decay rate of u. 
To determine the sensitivity of u to 
changes in V and they also determine the 
reset of V and u. 
So here's a range of very different kinds 
of firing patterns from very different 
kinds of neurons, sort of being generated 
by different choices of these four 
parameters. 
So these are all model fits to different 
kinds of, of real neurons and they've 
been fit using just those four 
parameters, so you can see that you can 
get a simple regular spiking dynamics 
that you can get integrating the firing 
neuron. 
You can also get neurons that do 
intrinsic bursting that have these very 
rapid sequences of spikes. 
You see bust that are punctuated by these 
large inactive periods. 
You see fast spiking, low, low threshold 
spiking. 
You see spike frequency adaptation that 
the firing of this neuron starts off 
rapid and gets slower and slower. 
This [UNKNOWN] cortical neuron that has a 
burst of spikes and then no firing. 
And here you see something nice that you 
actually can't get it off from the 
integrated firing like neuron. 
You see subthreshold resonance. 
That is the propensity of the neuron to 
oscillate in response to an input. 
This is something that can only be 
achieved with two variable systems, 
because the two variables can play off 
against each other. 
This can't be caputred by a regular 
integrated [INAUDIBLE]. 
That was a painfully brief and partial 
overview of the world of simplified 
models. 
As you can imagine this is something of a 
mathematician's playground. 
So there's lot to be found out there if 
you'd like to do more reading. 
We're going to continue in the next part 
of today's lecture to go in the other 
direction to look at the, the Gory 
reality of neurons and try to understand 
how one can model complicated dendrytic 
algebras/g. 

In this final part of the lecture, we're 
going to continue to go beyond 
Hodgkin-Huxley, but this time, in this 
direction. 
We're going to be going back to 
biophysical reality and addressing the 
issue of geometry. 
How do the complexities of neuronal shape 
and structure affect our computation? 
In the first lecture we invested heavily 
in understanding the spike generation 
process in a patch of membre here at the 
easy end. 
But its a little embarrassing to zoom out 
and look at real neurons which have a 
truly extraordinary range of beauty and 
complexity in the geometry of the 
dendritic abras. 
So for moving toward building 
biophysically realistic models of neuro 
processing. 
It would be good to know how these 
structures can contribute to the 
processing of information. 
So, here's what we learnt to model, a 
compact cell, and here's what real 
neurons really look like. 
Here's even quite a simple version. 
So, as with the complexities of ion 
channel dynamics, what is the appropriate 
level of description of a single neuron 
that's necessary to understand brain 
operation? 
Because we don't yet know the answer to 
this, and there probably is not one 
answer to this, it's important to pursue 
models with many different approaches. 
So, here I'll be introducing you to the 
techniques that one can use to handle 
dendrites, and some ideas about what they 
may contribute to computation. 
So let's start by looking to what extent 
dendrites feel what's going on in the 
soma. 
So here, this is an impulse point that's 
being put in at the soma. 
Let's see what that input looks like when 
it reaches the dendrite. 
You can see that it's both delayed, it's 
reduced in amplitude, and it's broader 
Similarly if we put an input here, add in 
the dendrite, and we look at what happens 
at the soma in response to that input, 
you also see that it's much reduced in 
size and it's broadened out. 
Furthermore, how thin the dendrite is 
affects how big a voltage change you 
could make with a given amount of current 
input. 
The thinner the dendrite the larger the 
voltage change but generally the further 
away the the more that input gets 
filtered and attenuated. 
This tells us the inputs that come along 
different parts of the dendrite can have 
very different effects and very different 
influence on firing at the soma. 
As you can image this can have a 
tremendous impact on the information that 
is integrated and representated by the 
receiving neurons The theoretical basis 
for understanding voltage propogation in 
dendrites and axons is cable theory, 
which was developed by Kelvin in quite a 
different context. 
The voltage, V, is now a function of both 
space and time, which means that we're 
now dealing with partial, rather than 
ordinary differential equations. 
So here's the setup. 
We now think about a tube of membrane 
with sides have the same properties as 
our membrane patch. 
They have both capacitance and 
resistance. 
So we see little elements that look a lot 
like our, like our previous patch model, 
but now they distributed down a cable. 
There's additionally the resistance of 
the cable interior. 
Current can flow along the cable as well 
as through it. 
So generally we're not going to worry 
about the external medium here. 
We'll just take it to be infinitely 
conducting with a resistance of zero. 
So the cable equation for a passive 
membrane, we're not going to deal with 
ion channels for now, is derived by 
considering the changes in current as a 
function of space. 
The current down the cable will be driven 
by steps in voltage as a function of x. 
So, if we have a voltage difference 
between two points in the membrane, 
that's going to drive a current down the 
membrane. 
Of course, current is also passing out of 
the membrane. 
That's the im that we modeled previously. 
Now when one puts those 2 things togteher 
dealing with the way current flows out of 
the membrane and the way that it flows 
down the membrane 1 obtains an equation 
that is actually 2nd order in space so it 
has a 2nd derivative with respect to 
space. 
So this half of the equation you 'll 
recognize that we've seen before of the 
passive membrane now we have an 
additional term that, that includes a 
spacial derivative. 
So, some of you will recognize that this 
equation is not unlike the equation that 
describes diffusion or heat propagation, 
but it has this additional term, so this 
part looks like diffusiion, has this 
additional term that's linear in v. 
You might remember that when we rewrote 
the RC second equation for the passive 
membrane to find the time constant of the 
membrane that gave us sort of the 
fundamental time scale of its dynamics. 
So there's something very similar in this 
case too we can rewrite the equation in 
this form where we bring together all the 
dimensional quantities. 
This will ask us to read off the natural 
time scale so going with this time 
derivative there's a a time. 
Constant which is our tow M and now when 
we look at the, the spacial derivative 
this has units of 1 over space squared 
and there's a space constant out the 
front lambda that carries the typical 
spacial scale from the coefficient of the 
space derivative. 
That's given by the square root of the 
ratio of the membrane resistance divided 
by the internal resistance. 
Why is that. 
So what stops a signal from propagating 
is leakage through the cable. 
The higher the membrane resistance, the 
more current stays inside the cable and 
the further the signal can propagate. 
Not surprising the ether space constant 
also depends on the inverse of the 
internal resistance. 
The less internal resistance, the further 
the signal can go and so this 
combination, and so this combination of 
resistances gives us a typical scale over 
which the current is able to propagate. 
So now let's imagine that we're a synapse 
on a dendrite, trying to get a message 
down to the soma, so our message can help 
trigger a spike and then get handed on to 
the next guy. 
Let's see how far down the dendrite our 
message can travel. 
So here we are. 
We're trying to inject a signal into our 
dendrite at position x equals zero. 
Even if we pipe in a constant amount of 
current, we just keep shouting that 
signal, the voltage change that it causes 
in that cable will only propagate about 
as far as the length constant. 
Now, you see here, it decays 
exponentially over a scale given by the 
space constant lamda. 
Now let's put in a brief pause and see 
how that behaves at T equals 0 we put in 
a spike of input at X equals 0. 
Here's what happens to it, the input gets 
broader, spreads out spatially and also 
decays in time. 
This is a lot like diffusion. 
If you spray a pulse of perfume, 
somewhere in a room, and were able to 
watch what happened to it, it would do 
something similar. 
It would spread out with the same spatial 
profile. 
But for the perfume, there's always the 
same amount of perfume. 
If you were to add up all the molecules 
of perfume in the smeared out blob, it 
would be the same as you started with. 
For the voltage, that's not the case. 
The total voltage signal is decaying away 
in time, because of that first order part 
in the differential equation. 
That is, the total voltage is decaying 
exponentially, just as it did in the RC 
circuit. 
So what are we going to see if we sit 
some distance away, and observe the 
change in voltage? 
That's what's shown in this figure. 
So these are different curves that plot 
the voltage that's observed at different 
locations. 
X equals 0, x equals .5, x equals 1. 
These are all in units of the space 
constant of the membrane. 
At different times, and so sitting at x 
equal 0, we see an exponential decay. 
As we sit a little further away we're 
going to see that. 
That, that pulse of voltage change first 
rise, then seal the decay away again. 
So you can see how rapidly the signal 
decays as a function of space. 
So at about, at one space constant you 
still see a reasonably large Deflection 
in voltage that's caused by that pulse. 
But at two space constance away, the 
signal, the size of the, of the pulse 
that we see there is down to five percent 
of the original. 
You can also track the peak of this 
potential pulse and ask how rapidly does 
this signal propagate down the cable? 
It turns out that that gives us a 
velocity of propagation of the signal 
which goes like, see if you can guess, 
so, what would you expect to be the 
propagation rate down a cable? 
Let's call it c. 
It turns out that it's equal to two times 
the space constant over the time 
constant. 
So, what's the typical spacial scale? 
It's just lamda. 
Typical timescale is tow and the 
propigation velcoity turns out to be the 
ratio of those two of those two 
quantities. 
So for some of you who like to see 
general solutions here's how the voltage 
responds to a pulse of input at time T 
equals zero and position X equals zero 
looks at position X and time T. 
We can see that this solution is made up 
of two parts. 
So here's the diffusive spread for this 
Gaussian profile. 
That's very similar to the way things 
spread diffusively. 
And mutliplying that, there's this 
pre-factor that has an exponential decay. 
So, knowing the solution, one could take 
some arbitrary pattern of inputs, 
decompose it into pulses, as we, as we 
put in here, at different central 
locations. 
Say, T prime and X prime, and add 
together a weighted sum of this solution 
form. 
We've centered up those differnt 
locations and times. 
So now we know how to find solutions for 
a very long pass of cable with a fixed 
radius. 
In fact doesn't get us very far in 
dealing with the real neuron, because of 
two things, the inter current branching 
of dendrites and the fact that many 
dendrites are not passive but are active. 
That is, they have ion channels in them. 
So it quickly becomes very difficult to 
solve anything analytically. 
The path forward is by dividing the 
dendritic arbor into what's called 
compartments. 
So here's an example, one can approximate 
the dendritic arbor as a coupled set of 
compartments. 
So these, we're going to break this 
dendrite into little sub regions In which 
the radius and the ion channel density is 
taken to be constant. 
Each compartment will then have an 
equation that only depends on the time 
derivative of the voltage, and not on x. 
The spatial dependence is incorporated by 
coupling each compartment together. 
So, so [INAUDIBLE] Rall device is a 
helpful way to approximate complex 
dendritic trees of. 
Let's consider a branch that divides into 
2 daughters. 
It turns out that if the diameters of 
these two branches have this particular 
relationship to the diameter of their 
mother so if the if the diameter of 1 of 
them raises to 3 halves plus the diameter 
of the other also raised to the 3 halves 
is equal to the diameter of the mother 
raised to the 3 halves. 
What that means is that these two 
branches are impedance matched to this 
branch, and one can simply accumulate 
them all together into one long branch of 
the same, of the same diameter as the 
mother. 
So one needs to thus compute the 
effective electrotonic length of these 
two additional branch elements. 
And extend the original cable by that 
much. 
So you can see that one can continue to 
do this iteratively. 
If the same property holds here, lennox 
can extend that branch out to an 
effective branch of this, of D2 diameter. 
And then one can agormate those two 
together. 
Until one eventually has a single, a 
single cable coming out of the soma. 
So it turns out that this condition on 
the diameters it approximately satisfied 
by real dendrites and even when it's not 
exact the resulting approximation is 
often quiet accurate. 
So the role models are very useful for 
passive membranes But it doesn't address 
the issue of ion channels which make the 
problem nonlinear. 
Furthermore ion channels densities often 
very along dendrites which can lead to a 
lot of interesting effects that one might 
like to explore. 
so here's the full approach, given the 
geometry and the ion channel density of 
the dendritic tree One can divide it 
where the properties are approximately 
constant. 
One can then write down equations for the 
membrane potential in each compartment 
individually. 
So let's say compartment 1 we represent 
here in terms of a, a similar second 
model that we saw for the passive 
membrane we're going to give that the 
voltage V1 and now right down an equation 
for V1. 
We can do the same for compartment two 
and compartment three, here. 
These equations will be similar to the 
passive membrane equations we looked at 
for Hodgkin-Huxley, but with the 
individual ion conductances, membrane 
resistance and capacitance set 
appropriately for each piece of the 
cable. 
Furthermore, there'll be also two tons 
that couple the compartment with it's 
neighbors. 
The current input from the neighboring 
compartments. 
Which depends on the voltage difference 
between the two compartments and a fixed 
coupling conductance. 
So here for example, let me write down an 
equation for for v two. 
We're going to need to include a current 
that's coming from, from compartment one 
that's going to go like g one, two 
multiply it by V1 minus v2. 
And similarly, there'll be a current that 
comes into compartment one from 
compartment two that's going to have a 
different coupling conductance and the 
opposite voltage difference. 
These fixed turns, these coupling 
conductances, depend on the area of the 
connection, and whether or not the 
compartments straddle a branching point. 
So if you notice, that in general, these 
coupling conductances are not necessarily 
symmetric, which is why there are 2 
values at each of these connections. 
So there are many models like these that 
have been built from microscopic 
reconstructions of single neurons. 
And a great many have been made publicly 
available on the ModelDB site maintained 
at Yale. 
So if you're in the mood to go explore a 
dendritic forest, there's plenty out 
there, so don't forget your adventure 
hat. 
So what do den-, so what do dendrites add 
to neuronal computation? 
There are many proposals for ways in 
which the filtering and active properties 
of dendrites can work, to shift the way 
in which incoming information is Clearly, 
where an input arrives on the tree can 
influence the effective strength of the 
input because of the passer properties. 
Interestingly, it's been found that in 
the hippocampus, neuronal dendrites have 
solved this problem. 
So that when inputs arrive at the suma/g, 
they have a very similar shape no matter 
where they come in. 
This amazing property is known as 
synaptic scaling. 
Filtering through the dendritic tree on 
the way to the sonar also determines 
whether a sequence of successive inputs 
is integrated to build up to potentially 
drive the spike or not. 
Where two different inputs enter the 
dendritic tree can also make a huge 
difference in how they interact with each 
other. 
For instance, if two inputs come in on 
separate branches. 
They contribute independently. 
While if they are on the same branch, 
they can sum either sublinearly or super 
linearly which leads to amplification. 
Another very important property is that 
thanks to their ion channels, dendrites 
can generate spikes, generally calcium 
spikes. 
[INAUDIBLE] This leads to the possibility 
that one could use coincidence of inputs, 
driving spikes in the dendrites. 
Along with back propagating spikes from 
the soma back to the dendrites to drive 
synaptic plasticity. 
This is a topic you'll be hearing much 
more about in the next lectures. 
So, let's close out by looking at two 
ideas for how dendrites might perform a 
computational role. 
The experimental evidence supporting 
these mechanisms is somewhat mixed, but 
the fundamental ideas stand as 
interesting conceptual models. 
First, here's a wonderful example where 
the propagation of signals through cables 
is thought to help out in carrying out a 
computation. 
Nuclei in the auditory brain stem are 
responsible for sound localization, the 
ability that we all have to locate where 
a sound is coming from. 
The que that's thought to be used is the 
timing difference in the arrival of a 
sound at our two years. 
The sound arise at the two ears at 
slightly different times, and the signal 
those travels through the left and right 
auditory pathways at slightly different 
times. 
Imagine that these two signals are piped 
into the population of neurons The 
thresholds are set such that they can 
only fire, when coincidence signals from 
two different inputs arrive at the same 
time. 
Each neuron receives the two inputs with 
a delay caused by traveling different 
distances along the dendrites. 
The neuron that fires the most is the one 
for which the relative timing delays, due 
to the timing difference between the two 
ears Is compensated for by the dendritic 
delay. 
This mechanism turns a tiny difference 
into a place code where by the label of 
the neuron that fires, indicates the 
timing delay, which can then be 
translated into the spatial location of 
the sound. 
Here's a final example Neurons in the 
retina show direction cell activity. 
That is, they respond to individual 
stimulus moves in one direction, and is 
suppressed when it moves in the other. 
So how might such direction cell activity 
begin constructed at the single neuron 
level? 
Imagine that inputs from different 
spatial locations are coming in to a 
dendrite at locations along the dendrite, 
arranged as in space. 
As the dendrite receives a sequence of 
activations, you can see that if they 
receive that input first, at the far end 
of the dendrite, and it begins to travel 
toward the soma, then another input comes 
in, closer to the soma. 
Then the influence of these two inputs 
can sum and build up as more and more 
inputs arrive. 
So that the net input crosses some 
threshold. 
On the other hand if the nearby location 
is stimulated first, then the next one 
and then the next one and then the first 
inputs will die away by the time the 
later ones arrived. 
And you can see how, how that, how that 
voltage signal at the soma might behave. 
So this idea was first proposed by Rall. 
While it may not fully explain direction 
selectivity and retinal ganglion cells, 
the general idea does predict that the 
firing probability of the neuron should 
be sensitive to the sequence of inputs 
along the dendrite. 
Let's say these inputs occur in some 
sequence. 
One could then scramble the order, and 
see whether the firing of the neuron can 
distinguish those inputs. 
Michael Hausser's lab carried out this 
experiment by simulating sequences of 
synaptic inputs into single neurons and 
found that indeed different input 
sequences were discriminable. 
Okay so here's where we wrap up my 
section of the course. 
I hope you've enjoyed this brief 
introduction into the electrical basis of 
neural signaling in the brain. 
Roshesh is going to take it from here, to 
guide you through the ways in which these 
basic cellular components get wired up 
together, to produce the amazing variety 
amount of behaviors that I know the 
systems are capable of, through 
experience and through learning. 
Have fun. 

[MUSIC]. 
I'm happy to introduce our next guest 
lecturer. 
That's Eric Shea-Brown of the Department 
of Applied Mathematics here at the 
University of Washington. 
So Eric did his PhD at Princeton, with 
Jonathan Cohen, and with Phil Holmes. 
On the Neurodynamics of Cognitive 
Control, and after that, he did a 
post-doc at NYU under the mentorship of 
John Rendall/g. 
Eric and his wife are avid skiers and 
hikers. 
They're great cooks and they have an 
adorable one year old son. 
Eric's work and that of his group 
concerns the relationship between neural 
dynamics and coding. 
And they're particularly interested in 
issues like decision making, chaotic 
dynamics and neural circuits and also 
correlations. 
And correlations is the topic he'll be 
talking to you about today. 
Thank you, Eric. 
 >> Thank you very much for the 
introduction. 
So I'm going to talk to you about 
representation of information in large 
Neural Populations. 
The title is Correlations and Synchrony. 
So when we think about how representation 
of a signal, say something that's in the 
sensory environment, in the spike 
responses of single cells. 
A picture like that which you see on the 
screen here, comes to mind. 
This is from famous nobel prize winning 
work, in this 60s. 
the idea is there's something in the 
sensory environment. 
Again, here, the orientation of a visual 
signal. 
You can see that changing there on the 
left. 
You record form a signal cell, here a 
cartoon of what you might see from the 
visual cortex. 
And as that feature of the sensory 
environment changes, something about the 
way the spikes you see on the right 
changes, what is that something? 
Famously, that's the rate at which the 
spikes are produced, the simplest 
statistic perhaps, that you could 
imagine. 
And there's been enormous progress in the 
field from quantifying being this change 
in rates via something called a tuning 
curve. 
So here, in the bottom you can see firing 
rates as a function of the sensory 
variable. 
itself the angle, of this particular 
visual stimulus of varying in some 
systematic way. 
Okay, so that's the first statistic, that 
matters in terms of how cells respond. 
In terms of representing information. 
We gave an example from visual 
neuroscience. 
But we see these type of tuning curves 
and the covariation of firing rates with. 
Again, something that you might imagine 
the nervous system wanting to encode 
information about. 
in a wide variety of different settings. 
One almost 100 years old in terms of 
proprioception. 
motor neuroscience and other examples 
listed at the bottom of the screen. 
So here we go, we're off. 
We're talking about statistics of 
responses of cells. 
and again, how those represent signals. 
What other statistics beyond the rate 
might we be interested in? 
Well, first, if we give some sort of 
stimulus which elicits, on average, say, 
a 10 hertz response or so, we count 
spikes in half a second. 
On one trial we might see indeed the 
average occurring 5 spikes occurring, but 
on other trials we'll see very different 
responses. 
This is not a clockwork type of system. 
Looks a lot more like popcorn or a Geiger 
Counter. 
Variability or variance as represented 
say in a pluson process. 
So, what do we have so far? 
We've got a bunch of neurons, we look at 
one of them. 
We have a tuning curve, that's the mean 
response as a sensory variable changes. 
We also have variability or variance 
around those mean, that mean. 
two statistics so far, on our journey 
towards describing population responses. 
We can do that for one cell, we can do it 
for another. 
Let's grab this blue one over here, and 
we can see that we can quantify similar 
statistics, firing rates and variance and 
function of the stimulus. 
Is that all there is to it? 
Or can we just repeat this procedure 
going one cell at a time, describing 
possibly different properties of mean and 
variance in their responses. 
Or is there something more there, in the 
neural population, beyond that which we 
could deign by looking at one cell at a 
time. 
Well the simplest way to get at that 
question is to look at pairs of cells at 
once and ask whether again, in these 
paired responses. 
There's more there than you see in just 
one cell at a time. 
Quantify it as follows, put down some 
time window of length T, measure from a 
couple of cells, upper cell here. 
Another cell, grab it, produce the 
response, down there. 
Close that window. 
and simultaneously measure how many 
spikes these two cells produce, right? 
So here in our window of time the first 
cell produced two spikes, second cell 
three. 
Slide that window of time along see what 
happened, okay? 
Label these spike counts n1 and n2 for 
the two cells and ask, well, again, is 
there more there in those two cells 
responses than I could have seen one cell 
at a time? 
How do I do that? 
Measure something called the correlation 
coefficient or the Pearson's correlation 
coefficient. 
That's just that covariance of these two 
spike counts, divided by their variance. 
And you ask, well, are these cells 
covarying or not? 
What is this number, is it zero or 
something nonzero? 
By now we have many studies which 
indicate that this correlation 
coefficient is significantly nonzero. 
Now, there are some interesting cases in 
which these correlation coefficients do 
seem to be 0. 
but by and large, again, there are a 
large number of examples all the way from 
the input and of the nervous system to 
the output. 
Where we see significant departures from 
independents of the cells, again, 
quantified by non 0 correlation 
coefficients row. 
Okay, so that it looks like we need to 
keep going in our effort to describe what 
neural populations do. 
We can't just look at one cell at a time, 
there's more in the joint or co-bearing 
activity of these two cells. 
That could be discovered by looking at 
one at a time. 
But what we haven't yet established is 
whether that's just some factoid about 
the way cells fire. 
Fine, they happen to go at the same time, 
spike at the same time, with a little bit 
more prevalence than you might expect by 
chance. 
but that does that actually matter for 
the way that they encode information? 
Encode, for example, the type of simple 
sensory variables that we have been 
looking at so far. 
That's the question, who cares, at the 
bottom of the screen. 
So, there, this has been, studied. 
And also, reviewed, you see a review 
paper here. 
Averbeck et al, Nature Reviews 
Neuroscience '06. 
and studied and reviewed in the context 
of, of neuroscience by a large number of 
authors. 
And I want to give you a sense of, what 
the type of results that have been 
established. 
seem to be pointing to, so, let's look 
again at the responses of two of our 
cells, our friends, the blue and green 
neuron from before. 
in response to a particular sensory 
variable, so a drifting grading, say, 
with an orientation that you see is is 
diagonal. 
As indicated by my lollipop in the bottom 
of the screen. 
Okay, so let's talk about the mean 
responses, that these two cells produce. 
they're both firing at some reasonable 
rate. 
the blue and the green cell together. 
And if you would make some sort of plot 
where on one axis we have the spike count 
coming out of cell one. 
The other the spike count coming out of 
two cells cell two, we would get some 
sort of a point on average in this. 
in this two-dimensional space for the 
mean responses of these two cells. 
Now if we on top of that, were to make, 
we know it's probably wrong 
But we were to make the assumption that 
these cells are statistically independent 
of one another. 
Then their variability would be spread 
around that mean in some some roughly 
circular way, okay? 
Fine, so this would be a picture of the 
cloud of responses that I get out of 
these two cells. 
Under the assumption that they are 
independent of one another, not 
correlated, okay? 
Now, I present another stimulus, so my 
lollipop has moved over by a little bit. 
And my stimulus is now a little bit more 
horizontal. 
What's going on both of these cells 
respond with a slightly higher firing 
rate right? 
So, the mean of the distribution has 
moved up in this two dimensional space. 
But we're still assuming in an in a 
roughly independent way so the response 
distributioner cloud is still roughly 
circular, okay? 
So those are my two clouds of responses 
elicited by stimulus one and stimulus two 
in my pair of neurons. 
Under the assumption that these spike in 
an independent way. 
Hm, now, what if these cells were not 
independent of one another and they 
tended to be correlated in a positive 
way? 
In other words, their responses tended to 
covary. 
They're still variable, this cloud is 
extended, okay? 
But what happens to occur, is that both 
cells tend to fluctuate or tend to do 
about the same thing. 
They tend to have similar correlated 
noise, or correlated variability. 
That means that these responses cluster 
towards the diagonal. 
Again, where cell 1 and cell 2 are doing 
approximately the same thing, okay? 
So, under that correlation assumption, 
right, my response distribution has gone 
from a European football to an American 
football. 
It is more concentrated, more elongated. 
My response distribution is going to look 
something like this for stimulus 1, and 
for stimulus 2, same thing, right? 
The mean, once again, shifted up. 
R axis in both directions, but we 
maintain our correlation, so that the 
response distribution again is expanded 
or elongated like an American football. 
Fine, so that's my picture, do I care? 
Well, let's take the organism's 
perspective, as the saying goes in the 
research literature and think about 
trying to look at the responses of these 
two neurons. 
And determine or decode which sensory 
stimulus was given. 
Was it the more diagonal one or the 
flatter one? 
Well you can certainly tell that that 
task is going to be much more difficult 
in the presence of these correlations. 
Because these two response distributions 
overlap more. 
The conclusion? 
Correlations can degrade the encoding of 
neural signals, okay? 
So, we saw this result for two cells. 
Now it turns out, this is not just a 
finding auh, that holds for pairs of 
neurons. 
If I look at large groups of cells, say, 
M cells with identical tuning curves, 
there's a famous argument. 
over a paper of Zohary, Shadlen, and 
Nethor, Newsome that makes the following 
point. 
Let's compute the signal-to-noise ratio 
of the output of all M cells at once. 
What's that? 
That's just a mean response divided by 
the variance of that response. 
Okay, so this signal-to-noise ratio is 
going to grow with M as I include a more 
and more cells into the population. 
Let's be careful there. 
should I be the mean, divided by the 
variance or the mean divided by the 
standard deviation 
That will grow with M if we have M 
independent cells, then the mean will 
grow with M, and the variance will also 
grow with m. 
So this is going to be something which 
grows in time, is going to be the mean. 
That's where it grows with the number of 
neurons you include in the population 
will be the will be the mean divided by 
the standard deviation. 
So there's a typo on the slide. 
Okay, anyway, we have some measure of the 
signal-to-noise ratio. 
This is growing with am, include more 
cells in the population that are signal 
noise ratio. 
Does this make sense? 
absolutely this makes sense. 
it's just like doing an experiment over 
and over again, or flipping a coin even, 
over and over again. 
The more times you do this, if you take a 
look at the aggregate response, it will 
have a smaller ratio of the size of the 
fluctuations. 
As opposed to the, again, the aggragate 
response, the mean response. 
Repeat an experiment many times, 
aggregate the data. 
You get a more accurate result, okay? 
So, this is the type of thing you see if 
all of the cells are statistically 
independent of one another. 
Do more, include more, get more 
information out. 
But what do you see, as you include 
correlation among these variables? 
So, here's our friend the correlation 
coefficient row again, before it was 
zero, all these cells were independent of 
one another. 
Now we increase this correlation 
coefficient, it goes from 0 up to 0.1. 
And you see something quite interesting 
happening to this signal noise ratio. 
Looks like it saturates even with a 
relatively wimpy correlation coefficient 
of one part in ten. 
So this is the same picture. 
This is code fluctuation or commonality 
in the response. 
in the responses of these cells, giving 
us a noise term that cannot be averaged 
away as I include more and more cells in 
the population. 
The consequence of this is a limitation 
on the signal, a noise ratio. 
A reinforcement of our overall point that 
we already saw in these perhaps easier to 
understand bubble pictures up at the top. 
Positive correlation giving you more 
overlapping responses, giving you less 
information, a bad news story. 
Now, some in the audience probably 
already thinking about this option. 
Is this bad new story the only one we can 
ever read? 
And the answer is no. 
What if I have my friends the blue and 
the green cells arranged as follows. 
Still the same two stimuli are presented. 
But now these cells have less similar 
tuning curves. 
So, that, notice please, when you go from 
stimulus one to stimulus cell, the green 
cell displays a lower firing rate. 
But when you go from stimulus one to 
stimulus two, the blue cell displays a 
higher firing rate. 
Well, What are my clouds of response 
distributions going to look like? 
Well, in this case, one of the cells has 
a higher firing rate, but the other cell 
has a lower firing rate as I go from one 
stimulus to the next. 
And my two response distributions will be 
arranged across the main diagonal like 
this. 
Now, you can guess what's going to happen 
when you introduce positive noise 
correlations. 
There we go, these two responses become 
more elliptical, exactly as before. 
But in becoming more elliptical they now 
become less overlapping or easier to 
discriminate. 
The conclusion's in the box here. 
Correlation can have a good news effect, 
as well. 
So if we sum up what we learned here, 
right? 
These are the two examples. 
and when we were trying to answer the 
question of, who cares? 
about the fact that I see positive 
correlation or nonzero correlation, I 
should say, in many places in the nervous 
system. 
We saw that there were a number of 
different options. 
There was this bad news story, right, as 
highlighted by this famous paper in the 
l, talking about large group of m cells. 
Or in our simple lips picture here, a 
decrease in information when cells tend 
to be more homogeneous or have similar 
response properties in their means. 
A good news story where if the cells are 
sufficiently heterogeneous with respect 
to one another. 
The presence of these correlations could 
increase the detectability of the two 
different signals, the discriminability 
of the two different signals. 
Now, it's also there's another possible 
angle on this good news story. 
And that's that the neuropopulations 
could be correlated in a way that 
covaries with the stimulus. 
Say stimulus A gives you uncorrelated 
response, stimulus B gives you correlated 
responses in an extreme limit. 
With nothing else about the single cell 
responses changing at all. 
Then the presence of the correlations, or 
the synchrony, could be actually carrying 
information. 
That's another channel, you can say, in a 
very informal way by which a signal could 
be carrying. 
It's an attractive idea that's been put 
forward in more sophisticated ways than I 
offered in the literature. 
And there could be a no news story as 
well, right? 
With these varied impacts of correlations 
on encoded information, and you can 
imagine multiple of these different 
impacts being present in different 
In different competing ways at once so 
that there's not much of an impact. 
or perhaps the effects themselves do to 
the correlation coefficients being less 
extreme than I've demonstrated in my 
ellipse pictures. 
Or due to the population sizes being 
small the effects in the cells being 
smaller rather insignificant. 
So the full range of different marquee 
headlines can occur when we think about 
what the possible roles are of 
corelations. 
at the paralyze level here in terms of 
representing, representation of 
information around populations. 
And this is still an area in active 
study, and under active debate. 
Okay that's the debate in blue. 
Now I'd just like to close by, mentioning 
an area in which many parts of the field 
the research field are moving now. 
and that's best done by summing upward 
come so far. 
So, we've seen for many cases, not all of 
them, many cases in the literature, we 
can't describe responses of a neural 
ensemble here. 
It's just an ensemble of two cells. 
By thinking about the two cells 
independently, we have to think about 
these two cells as a unit, as a possibly 
correlated unit. 
Well hang on, that's just, thinking about 
2 cells at once, but what happens when I 
think about 3 cells at once. 
It's something that's really different 
there? 
Is there an analog of the American 
football being different from the 
European, or the International, football. 
that occurs when you go from three two 
cells to three. 
And how about from three cells to four? 
And when is this story ever going to 
stop? 
Now, this is a question that's been 
around in neuroscience for a long time. 
Here are some of the references, and the 
ideas go back, not surprisingly, even 
before that. 
But these type of questions have really 
come to the floor even more strongly with 
an increase in this scope and scale of 
array type of recordings. 
Here's a famous, pe, paper from the 
research group of E.J Chichilnisky. 
at the Salk Institute in which a ballpark 
of 100 or so cells are recorded 
simultaneously. 
We really have to think about the 
statistical scale, at which we would 
describe those cell populations, when 
we're faced with these type of data. 
An exciting question. 
How are you going to do it? 
How are you going to describe the 
response of this entire ensemble. 
How are you going to build up the 
probability distribution and not just 
over n1 and n2, as we had in my blue mare 
example before. 
But over that which contains all n cells. 
And this is more than just an academic 
question as these authors at the bottom 
of the page for example have emphasized. 
when we think about simply the practical 
process of doing this and trying to build 
up this probability distribution over n 
cells. 
Think about it, how many different firing 
rates are first order statistics? 
How many different tuning curvers would I 
have to describe? 
Well, that's going to be n, right? 
Because, we've got n cells. 
But, how many different pairwise 
combinations are there n squared? 
Again, these are really the arguments of 
these authors down here, Schneidman and 
Shlens, they're colleagues. 
How many triplet interactions are there? 
Well, n cubed, quartuplet, on and on and 
on. 
And if N is set at reasonably large, this 
is the appropriate time to make some sort 
of a galactic metaphor, but you get the 
picture. 
We need a intelligent way of doing this, 
of thinking about these population-wide 
statistics which is not a brute force 
enumeration of all the joint statistics. 
There are too many of them to write down 
let alone, 
Other problems that come about with 
thinking about such a complicated 
probability distribution. 
How are you going to do this? 
There is one, there are many, different 
approaches, I should say. 
There's just one I want to close with. 
This is my very last slide. 
It's this, here's an approach. 
Let's talk about this full probability 
distribution over n cells is what 
actually happens. 
What if I tried to based on this complete 
description of all of these cells. 
Build up my best possible estimate of 
what happened in all of those n cells by 
pretending that I could only observe 
pairs of cells at once, right? 
So I'm saying look, I went through that 
whole first part of that tog. 
I got what this guy was saying about 
these paralyze correlations and these 
ellipses. 
Let's just try to extend this type of 
description to the population as a whole. 
What I would get is some model which 
we'll call P2. 
Again, the best possible description 
based on just looking at pairs of cells 
at a time. 
Okay? 
So, again, if I look at just, at most, 
two cells at once all I know is how 
quickly they fire and how correlated all 
of those individual pairs are. 
Mm, okay? 
And then I minimize any further 
assumptions about the way those cells are 
interacting with one another. 
That's equivalent to something called 
maximize the entropy, this is absolutely 
not my idea. 
This goes back to Jaynes and perhaps even 
further. 
And has been advanced in the neuroscience 
literature by all of these other authors 
who you see listed at the bottom of the 
page as well as many others. 
But one idea is, again, to build up this 
P2 under that assumption. 
Under this minimal assumptions model. 
That leads to a particular probability 
distribution across the whole ensemble. 
A P2 that looks like this, it has the 
following special form. 
We're obviously not going to derive that. 
The references are here, it's a 
reasonably doable, but also a more 
advanced topic. 
but the bottom line is there's something 
concrete to compare with in answering the 
question, is there more there, than what 
is present at the level of pairs? 
The answers in research community are 
mixed. 
and interesting, this is a contemporary 
area on the frontier and I'm looking 
forward to seeing what we all learn, as 
the field moves in these and other 
complementary directions in the future. 
We'll stop there. 
